<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Discrete notes</title>
    <description>Blog</description>
    <link>https://discrete-notes.github.io/</link>
    <atom:link href="https://discrete-notes.github.io//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Bayesian mechanism design and Yao's principle</title>
        <description>&lt;p&gt;I attended a talk yesterday about 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithmic_game_theory&quot;&gt;algorithmic game theory&lt;/a&gt; by 
&lt;a href=&quot;https://www.or.tum.de/en/group/alexandrostsigonias/&quot;&gt;Alexandros Tsigonias-Dimitriadis&lt;/a&gt;
as part of Santiago’s &lt;a href=&quot;http://www.dii.uchile.cl/acgo/seminar-acgo/&quot;&gt;AGCO seminar&lt;/a&gt;.
Here are a few elements from this talk.&lt;/p&gt;

&lt;h2 id=&quot;basic-bayesian-auction&quot;&gt;Basic Bayesian auction&lt;/h2&gt;

&lt;p&gt;The very basic setting we are looking at is the following: a client (the bidder)
comes to a seller (the auctioneer) with the goal of buying a particular item. 
The auctioneer has to set a price for the item. 
The bidder knows the maximum price at which she will buy the object (the value 
of the item for her).
If the auctioneer’s price is higher than the bidder’s value, the bidder does not 
buy the item, and if the price is lower then the bidder buys it, but the 
auctioneer “looses” the difference.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian-optimal_mechanism&quot;&gt;Bayesian setting&lt;/a&gt;, 
the bidder’s value is taken following a probability 
distribution, and the auctioneer knows this distribution. 
Then, the expected revenue of the auctioneer is the price she announces, 
multiplied by the probability that this price is lower than the value taken by 
the bidder. In other words, if $F$ is the cumulative probability distribution 
of the value of the bidder, 
then the expected revenue for a price $p$ is $p \cdot (1-F(p))$.
Then the auctioneer chooses a price $p$ that maximizes this quantity.&lt;/p&gt;

&lt;h2 id=&quot;generalizations&quot;&gt;Generalizations&lt;/h2&gt;

&lt;p&gt;One can generalize this to work with several bidders. This is called 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian-optimal_mechanism#The_Myerson_mechanism&quot;&gt;Myerson mechanism&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The paper of Alexandros and his co-authors 
(&lt;a href=&quot;https://arxiv.org/abs/1907.04220&quot;&gt;Robust Revenue Maximization Under Minimal Statistical Information&lt;/a&gt;),
explores the setting where the auctioneer does not know the full distributions 
of the bidders, but only the means and the standard deviations. (That’s why it 
is called &lt;em&gt;robust&lt;/em&gt; revenue maximization.)&lt;/p&gt;

&lt;p&gt;An important element of their proof is a generalization of Yao’s minimax principle.&lt;/p&gt;

&lt;h2 id=&quot;yaos-minimax-principle&quot;&gt; Yao’s minimax principle&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Yao%27s_principle&quot;&gt;Yao’s minimax principle&lt;/a&gt; 
is a general theorem for randomized algorithms. 
Basically it is saying that the best randomized algorithm on its worst instance 
will get the same performance as the best deterministic algorithm on the worst 
distribution of instances. (This needs a precise statement, to say which thing is 
optimized first etc.).&lt;/p&gt;

&lt;p&gt;In the context of Alexandros, the instances are distributions (of which we know
only the mean and the standard variation) thus a distribution of instances is 
a distribution of distributions. This can be made precise, by considering a
mixture of distributions.&lt;/p&gt;

</description>
        <pubDate>Thu, 21 Nov 2019 00:00:00 -0300</pubDate>
        <link>https://discrete-notes.github.io///bayesian-auctions</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///bayesian-auctions</guid>
      </item>
    
      <item>
        <title>More November notes</title>
        <description>&lt;p&gt;Some more notes for November 2019.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/nueva-constitucion.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;network-creation-game&quot;&gt;Network creation game&lt;/h2&gt;

&lt;p&gt;Network generation models are mechanisms to create networks. 
In a classic setting the nodes arrive one after the other and are linked 
to nodes already in 
the network following some rules. 
In another setting, called &lt;em&gt;network creation game&lt;/em&gt; the nodes are players, 
and they play a game in which they can choose to pay to be linked to 
other nodes. 
The outcome of the game is a network. 
The cost that a player pays is $\alpha$ for every node it decides to be 
linked to, plus 
the sum of the distances from this node to all the other nodes. 
In other words, a node wants to have short distance to every node, but 
cannot add a link to every node, because it would be too expensive.&lt;/p&gt;

&lt;p&gt;For this game one can study the usual objects of 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithmic_game_theory&quot;&gt;algorithmic game theory&lt;/a&gt;:
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Nash_equilibrium&quot;&gt;Nash equilibrium&lt;/a&gt; 
and the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Price_of_anarchy&quot;&gt;price of anarchy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is conjectured that the price of anarchy is constant for 
any value $\alpha$, and &lt;a href=&quot;https://arxiv.org/abs/1909.09799&quot;&gt;this recent preprint&lt;/a&gt; 
makes progress on the conjecture.&lt;/p&gt;

&lt;h2 id=&quot;lempel-ziv-compression-algorithms&quot;&gt;Lempel-Ziv compression algorithms&lt;/h2&gt;

&lt;p&gt;Lempel-Ziv algorithm is a classic compression algorithm (or more 
precisely a classic family of algorithms, are there are several versions). 
A &lt;a href=&quot;https://semidoc.github.io/lagarde-catastrophe&quot;&gt;blog post on Semidoc&lt;/a&gt; 
describes the algorithm and gives an overview of 
&lt;a href=&quot;https://arxiv.org/abs/1707.04312&quot;&gt;this paper&lt;/a&gt; which studies how the compression
rate can change when the original text is changed by one bit.&lt;/p&gt;

&lt;p&gt;Two recent papers on arxiv deal with Lempel-Ziv:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1910.00941.pdf&quot;&gt;The first one&lt;/a&gt; gives a new analysis of 
the fact that Lempel-Ziv is optimal for some models of random text (hidden 
Markov sources)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.10347&quot;&gt;The second one&lt;/a&gt; improves the complexity of 
the algorithm decompressing the text.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-armed-bandit&quot;&gt;Multi-armed bandit&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Multi-armed bandit&lt;/em&gt; is an expression that appears here and there in 
TCS conference, and very often in theoretical machine learning. It is a type of 
problem where one has to make decisions one after the other, to 
maximize some pay-off. Basically, at each round, it has the choice 
between several options called the “arms” of the bandit (like the levers 
of different slot-machines).&lt;/p&gt;

&lt;p&gt;A basic version is the following framework:&lt;/p&gt;

&lt;p&gt;Given: $k$  arms, $T$ rounds.&lt;/p&gt;

&lt;p&gt;In each round $t\in[T]$:
1. Algorithm picks arm $a_t$.
2. Algorithm observes reward $r_t\in [0,1]$ for the chosen arm.&lt;/p&gt;

&lt;p&gt;Pay-off: the sum of the rewards&lt;/p&gt;

&lt;p&gt;The reward for an arm comes from an unknown distribution, but if the 
algorithm chooses an arm repeatedly, it somehow learns this distribution. 
There is already a lot to say on this simple case, and there are a flurry
of papers about this, these days.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.07272.pdf&quot;&gt;Here&lt;/a&gt; is a recent introduction 
to multi-armed bandit. Also if you are in Rennes, France, on Wednesday
there is a 
&lt;a href=&quot;https://perso.crans.org/besson/phd/defense/&quot;&gt;PhD defense on this topic&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;delaunay-triangulations-have-perfect-matchings&quot;&gt;Delaunay triangulations have perfect matchings&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Delaunay_triangulation&quot;&gt;Delaunay triangulations&lt;/a&gt; 
are triangulations of point sets in the plane. I recentely learnt that
the graphs that are Delaunay triangulations, always have a perfect 
matching (that is a matching of size $n/2$ if $n$ is even, and $(n-1)/2$
is $n$ is odd).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/delaunay.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
A point set, its Delaunay triangulation, and the associated graph with a perfect matching.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;A short proof of this appeared on arxiv recently, 
&lt;a href=&quot;https://arxiv.org/pdf/1907.01617.pdf&quot;&gt;here&lt;/a&gt;. (Actually it is a stronger
result that is proved, about the so-called “toughness” of Delaunay 
triangulations.)&lt;/p&gt;

&lt;h2 id=&quot;learning-augmented-algorithms&quot;&gt;Learning-augmented algorithms&lt;/h2&gt;

&lt;p&gt;Learning-augmented algorithms are algorithms that can use informtation
coming from some machine learning source. 
Here is an example.&lt;/p&gt;

&lt;p&gt;Binary search takes $O(\log n)$ in the worst-case. 
Now if you have some neural network (NN) telling you that the element you’re 
looking for is around position $i$, how do you modify your search? 
Well you can begin by testing position $i$. Then, if the NN is not perfect, 
this might not be the right value, but maybe it’s close. Say the value 
you’re looking for is larger. Then you can try to find a position 
that has larger value than your element, for example by doing exponential guesses. 
Once you have both upper and lower bound, you can run the usual binary 
search.&lt;/p&gt;

&lt;p&gt;If the error (that is, the number of positions between your element
and the prediction of the NN) is $\mu$, then your algorithm runs in 
$O(\log \mu)$. This is good: if the prediction is good then you speed up 
the search, and if it’s bad, then you do not loose much.&lt;/p&gt;

&lt;p&gt;In more general terms, one looks for two properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;consistency: the better the prediction, the better the algorithm&lt;/li&gt;
  &lt;li&gt;robustness: if the predition is bad, then the algorithm does not get 
much worse.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that for real application, one might also be interested in the running 
time of the NN, and a lot of other things.&lt;/p&gt;

&lt;p&gt;Some material on this topic:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a &lt;a href=&quot;https://www.mit.edu/~andoni/algoS19/scribes/scribe24.pdf&quot;&gt;lecture note&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theory.stanford.edu/~sergei/slides/HALG-slides.pdf&quot;&gt;the slides&lt;/a&gt; 
of a talk at &lt;a href=&quot;http://2019.highlightsofalgorithms.org/&quot;&gt;HALG 2019&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mit.edu/~vakilian/ttic-workshop.html&quot;&gt;a workshop&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 18 Nov 2019 00:00:00 -0300</pubDate>
        <link>https://discrete-notes.github.io///more-november-2019</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///more-november-2019</guid>
      </item>
    
      <item>
        <title>Mid-November (non-technical) notes</title>
        <description>&lt;p&gt;Some notes about a new conference, a new research organization etc.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/comida-india.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;conference-model-survey&quot;&gt;Conference model survey&lt;/h2&gt;

&lt;p&gt;An online survey has been created &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfOmbqTTfQfUYEXADCLLqat-OAl7XUh8gFceg27uDfpr_NaaQ/viewform&quot;&gt;here&lt;/a&gt; to gather the opinions of the distributed 
computing community about its model of conferences. Topics include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;changing from one deadline to three deadlines a year. Basically it’s like 
having the reviewing process made three times (with a smaller number of papers), 
and then having a conference with the papers accepted to any of the three phases.&lt;/li&gt;
  &lt;li&gt;collocating the two main conferences of the domain.&lt;/li&gt;
  &lt;li&gt;changing the format (more keynotes, or shorter talks etc.)&lt;/li&gt;
  &lt;li&gt;transitioning to a journal model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details can be found in the survey.&lt;/p&gt;

&lt;h2 id=&quot;paper-presentation-videos&quot;&gt;Paper presentation videos&lt;/h2&gt;

&lt;p&gt;One of the side topics of the survey is the possibility for the 
authors to upload a video presenting their papers. It’s something I have thought
of doing for my papers, but finally didn’t try.&lt;/p&gt;

&lt;p&gt;Till Miltzow is a researcher in graph theory and he records a talk for every 
paper he has, see &lt;a href=&quot;https://sites.google.com/view/miltzow/publications&quot;&gt;here&lt;/a&gt;. 
I think it’s good!&lt;/p&gt;

&lt;h2 id=&quot;the-polytcs-project&quot;&gt;The PolyTCS project&lt;/h2&gt;

&lt;p&gt;You may know the &lt;a href=&quot;https://en.wikipedia.org/wiki/Polymath_Project&quot;&gt;Polymath Project&lt;/a&gt;,
which is a project from the math community. It consists in choosing an open problem, 
and then working on this problem in a massively collaborative way: everything is
public, and everyone can help. This project has been quite successful, with great
collaborations, and great papers.&lt;/p&gt;

&lt;p&gt;A similar project has been launched in TCS: 
&lt;a href=&quot;https://polytcs.wordpress.com/&quot;&gt;The PolyTCS project&lt;/a&gt;.
The first problem is about boolean functions, see 
&lt;a href=&quot;https://polytcs.wordpress.com/2019/11/01/the-entropy-influence-conjecture/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;forc-conference&quot;&gt;FORC conference&lt;/h2&gt;

&lt;p&gt;A new conference in TCS: &lt;a href=&quot;https://responsiblecomputing.org/forc-2020-call-for-paper/&quot;&gt;Symposium on the Foundations of Responsible Computing (FORC)&lt;/a&gt;. Topics include privacy, fairness and electoral processes.&lt;/p&gt;

&lt;h2 id=&quot;symbolic-computations-in-python&quot;&gt;Symbolic computations in python&lt;/h2&gt;

&lt;p&gt;I recently had to do a lot of small computations on toy examples. To check the 
computations, I wanted to have some symbolic math software, but the usual
ones (e.g. Maple) are big machines, and often not open-source. If you are in the 
same situation, python with &lt;a href=&quot;https://www.sympy.org/en/index.html&quot;&gt;sympy&lt;/a&gt; is a 
good choice.&lt;/p&gt;

&lt;h2 id=&quot;pull-requests-for-this-blog&quot;&gt;Pull requests for this blog&lt;/h2&gt;

&lt;p&gt;This blog is a github page, that is, it works as a git repository. 
I recently got some pull requests for it, from my friend 
&lt;a href=&quot;https://perso.crans.org/besson/me/index.fr.html&quot;&gt;Lilian Besson&lt;/a&gt;. 
Pull requests is a way to suggest changes, e.g. correct a typo. It is very 
convenient (on my side): I just have to pull these commits. 
If you want to know more and correct one of the many typos of this blog, you can 
take a look at &lt;a href=&quot;https://www.freecodecamp.org/news/how-to-make-your-first-pull-request-on-github/&quot;&gt;this page&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 12 Nov 2019 00:00:00 -0300</pubDate>
        <link>https://discrete-notes.github.io///mid-november-2019-non-technical</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///mid-november-2019-non-technical</guid>
      </item>
    
      <item>
        <title>October 2019 notes&amp;#58; distances and moving stuff</title>
        <description>&lt;p&gt;Some notes for October 2019, related to distances and to moving stuff.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/farmacia.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;600px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
A lot is going on in Chile. Fortunately there are some good recipes.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;matching-graphs-via-gromov-hausdorff-distance&quot;&gt;Matching graphs via Gromov-Hausdorff distance&lt;/h2&gt;

&lt;p&gt;There are many contexts where one would like to compare two graphs, to 
measure if they are close or not. 
One way of doing this is to decide whether one is included in the other
(the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Subgraph_isomorphism_problem&quot;&gt;subgraph isomorphism problem&lt;/a&gt;) 
or more generally if they have a 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_common_edge_subgraph&quot;&gt;large isomorphic subgraph&lt;/a&gt;.
Another way is to measure the number of edits one has to make to go from 
one graph to the other (for example the number of edges to add and 
remove). This is what is done in 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Property_testing&quot;&gt;property testing&lt;/a&gt;, where having a 
distance on the objects is essential.&lt;/p&gt;

&lt;p&gt;More generally, there exists a distance for compact metric spaces called 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Gromov%E2%80%93Hausdorff_convergence#Gromov%E2%80%93Hausdorff_distance&quot;&gt;Gromov-Hausdorff distance&lt;/a&gt;, 
that applies to graphs.&lt;/p&gt;

&lt;p&gt;No surprise, this distance is a complicated notion 
(an inf of a max of a sup of an inf),
and computing it is NP-hard. The reduction is to the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Quadratic_bottleneck_assignment_problem&quot;&gt;quadratic bottleneck assignment problem&lt;/a&gt;, 
a &lt;a href=&quot;https://en.wikipedia.org/wiki/Facility_location_problem&quot;&gt;facility location problem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://arxiv.org/pdf/1909.09772.pdf&quot;&gt;fairly recent preprint&lt;/a&gt; studies how to 
estimate a modification of this distance in polynomial time.&lt;/p&gt;

&lt;h2 id=&quot;hardness-of-moving-earth&quot;&gt;Hardness of moving earth&lt;/h2&gt;

&lt;p&gt;Given two point sets of equal size in an Euclidian space, $A$ and $B$, and a 
bijection $f: A \rightarrow B$ , the cost of transporting $A$ to $B$ 
through $f$ is the sum over the elements of $a$ of the distances 
from $a$ to $f(a)$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/earth-moving.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now given two point sets, the earth mover distance is the minimum such 
distance over all bijection:
$
EMD(A,B)=\min_f \sum_{a\in A} d(a,f(a))
$&lt;/p&gt;

&lt;p&gt;It is the discrete analogue of 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Transportation_theory_(mathematics)&quot;&gt;Monge-Kantorovich metric&lt;/a&gt;
in probability theory, and is used in machine learning.&lt;/p&gt;

&lt;p&gt;Algorithms to compute the EMD are quadratic in $n$, and faster algorithms 
are known for approximation but only in small dimensions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.11068&quot;&gt;A recent preprint&lt;/a&gt; proves conditional 
hardness results that explain this situation.&lt;/p&gt;

&lt;h2 id=&quot;complexity-of-sandpiles&quot;&gt;Complexity of sandpiles&lt;/h2&gt;

&lt;p&gt;Sandpile models are models close to cellular automata, based on a grid 
in some dimension $d$, where every cell has a number (the number of 
grains of sand there is at this location), and there is a local rule to 
decide where the grains move at each step.&lt;/p&gt;

&lt;p&gt;There is &lt;a href=&quot;https://arxiv.org/pdf/1909.12150.pdf&quot;&gt;a recent survey&lt;/a&gt; about 
the complexity of predicting the final shape of sandpiles. 
There are some very nice things going on. For example the complexity of 
the prediction is related to the computational power of the sandpile itself:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If the prediction is P-hard, then the sandpile has the computational 
power of a Turing machine.&lt;/li&gt;
  &lt;li&gt;If the prediction is easier then it is not the case.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A classic sandpile model is called the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Abelian_sandpile_model&quot;&gt;abelian sandpile model&lt;/a&gt;
(or Bak–Tang–Wiesenfeld model) and is the following. 
On a grid $Z^d$, the rule is: every node that has $2d$ grains or more 
gives one grain to each neighboring cell.&lt;/p&gt;

&lt;p&gt;Here is an example with $d=1$, and a starting configuration where one 
cell has 5 grains. The cells that have 2 or more grains are “unstable” 
and give one grain to their left neighbor and one grain to their right 
neighbor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/sandpile.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;180px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the dynamic is not exactly what you would expect from a real 
sandpile, but it is very simple and has very nice properties.&lt;/p&gt;

&lt;p&gt;For this model, the following hold:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In dimension 1, prediction is in &lt;a href=&quot;https://en.wikipedia.org/wiki/NC_(complexity)&quot;&gt;NC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In dimension 3 or more the prediction is P-hard.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The survey gives a lot results, sketches and conjectures.&lt;/p&gt;

&lt;h2 id=&quot;k-opt-ratio-for-tsp&quot;&gt;k-OPT ratio for TSP&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/2-opt&quot;&gt;2-OPT&lt;/a&gt; and its generalization 
k-OPT, are popular heuristics for 
the (metric) &lt;a href=&quot;https://en.wikipedia.org/wiki/Travelling_salesman_problem&quot;&gt;traveling salesman problem&lt;/a&gt;.
They consist in iteratively looking for 2 (respectively $k$ edges) to modify to improve the 
cost of the tour. Here is an example for 2-OPT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/2opt.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This heuristic performs poorly in the worst-case: exponential time, and 
approximation ratio in $\theta(\sqrt n)$. But it performs well in practice.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://arxiv.org/pdf/1909.12025.pdf&quot;&gt;recent preprint&lt;/a&gt; computes the 
precise approximation ratio for 2-OPT, which is $\sqrt(n)/2$.&lt;/p&gt;

&lt;p&gt;Other papers from the literature prove that : 
* The approximation ratio is in $O(\sqrt n)$ for random edge weights 
(which are not metric in general), see &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S016763770900011X?via%3Dihub&quot;&gt;here&lt;/a&gt;.
* For Euclidian instances, the worst ratio is in $O(\log n)$.
* The &lt;a href=&quot;https://en.wikipedia.org/wiki/Smoothed_analysis&quot;&gt;smoothed analysis&lt;/a&gt;
of the problem has been studied, eg &lt;a href=&quot;https://wwwhome.ewi.utwente.nl/~mantheyb/full/MantheyVeenstra_TwoOpt.pdf&quot;&gt;here&lt;/a&gt;.
* The fine-grain analysis of $k$-OPT has been done &lt;a href=&quot;http://drops.dagstuhl.de/opus/volltexte/2016/6277/pdf/LIPIcs-ICALP-2016-5.pdf&quot;&gt;here&lt;/a&gt;
with improved complexity for $k\geq 4$.&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Nov 2019 00:00:00 -0300</pubDate>
        <link>https://discrete-notes.github.io///october-2019-notes-distances</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///october-2019-notes-distances</guid>
      </item>
    
      <item>
        <title>Graph classes and forbidden patterns&amp;#58; Introduction</title>
        <description>&lt;p&gt;Michel Habib and I have quite recentely finished the paper 
&lt;a href=&quot;https://arxiv.org/abs/1812.05913&quot;&gt;Graph classes and forbidden patterns on three vertices&lt;/a&gt;. 
This blog post is the first of a small series that will present the paper.
It follows the structure of a talk I gave yesterday at the University 
of Chile.&lt;/p&gt;

&lt;h2 id=&quot;why-studying-graph-classes&quot;&gt;Why studying graph classes?&lt;/h2&gt;

&lt;p&gt;This paper is about graph classes, and it seems that it is a topic 
that is not popular with everybody, so let’s start with a couple of 
reasons to study graph classes.&lt;/p&gt;

&lt;h4 id=&quot;graphs-are-fundamental-objects&quot;&gt;Graphs are fundamental objects&lt;/h4&gt;
&lt;p&gt;Just like sets and languages, graphs are basic objects, thus gathering 
knowledge about them is useful. For example, the pumping lemma
for regular languages appears in distributed computing, because 
languages are basic objects, and thus they pop up everywhere.&lt;/p&gt;

&lt;h4 id=&quot;to-use-algorithms-in-the-real-world&quot;&gt;To use algorithms in the “real world”&lt;/h4&gt;
&lt;p&gt;If one wants to use algorithms for real-world problems, it is likely 
that looking at results for general graphs is not the most useful 
approach. There is a good proportion of the real-world instances that
belong to very special classes, and if (1) you can recognize such 
special classes fast, and (2) you have a fast algorithm for your problem 
on this class, then you are happy. In other words,
your transportation problem may be hard on expanders, but road networks 
are not expanders, so let’s look at what road networks look like.&lt;/p&gt;

&lt;h2 id=&quot;why-not-studying-graph-classes&quot;&gt;Why &lt;em&gt;not&lt;/em&gt; studying graph classes?&lt;/h2&gt;

&lt;p&gt;To be fair, there is a problem with graph classes: 
there are way too many of them! The website 
&lt;a href=&quot;http://graphclasses.org&quot;&gt;graphclasses.org&lt;/a&gt; lists 1600 classes, and it’s 
difficult to navigate in this jungle, to know what is an interesting
class, to know whether the class you’re working on is known etc.
(although the website is very useful for that).&lt;/p&gt;

&lt;h2 id=&quot;this-paper&quot;&gt;This paper&lt;/h2&gt;
&lt;p&gt;This paper brings a new point of view on some very classic graph classes, 
putting them all in the same framework, with algorithmic consequences.
(Thus it goes in the right direction!).&lt;/p&gt;

&lt;p&gt;Actually “new” should be used with a lot of quotation marks: this point of view 
has been known for a long time for several classes. I cannot cite all 
the papers here, so I’ll cite none of them, but keep in mind that most of 
what I’ll write about is not new. Take a look at the paper if you want 
to see the 70 citations.&lt;/p&gt;

&lt;p&gt;Now, let’s get started with two examples.&lt;/p&gt;

&lt;h2 id=&quot;example-1--forests&quot;&gt;Example 1 : forests&lt;/h2&gt;

&lt;p&gt;Here is a complicated characterization of the forests (e.g. the acyclic 
graphs).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Theorem:&lt;/em&gt; A graph $G=(V,E)$ is a forest, if and only if, there exists an ordering 
of the nodes, such that: there does &lt;em&gt;not&lt;/em&gt; exist three nodes $a&amp;lt;b&amp;lt;c$, with 
 $(a,b)\in E$ and $(a,c)\in E$.&lt;/p&gt;

&lt;p&gt;That is, we forbid this pattern:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/tree.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;100px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof sketch&lt;/em&gt;
$(\Rightarrow)$ : Consider a tree of the forest. Put the leaves 
first in the ordering and remove them from the tree; then repeat 
with the remaining nodes of the tree, until the tree is empty. 
Then move on to the next tree, do the same etc. 
It is easy to check that the pattern will not appear.&lt;/p&gt;

&lt;p&gt;$(\Leftarrow)$ : If the graph is not a forest, then there is a cycle. 
Now consider any ordering of the graph. The node of the cycle that is 
the left-most in the ordering must have both its cycle neighbors on 
its right, thus the pattern appears.&lt;/p&gt;

&lt;h2 id=&quot;example-2--interval-graphs&quot;&gt;Example 2 : interval graphs&lt;/h2&gt;

&lt;p&gt;A more interesting example is interval graphs. Given a set of intervals
$S$, one can define the intersection graph of it, $I(S)$, by creating one 
node for each interval and putting an edge between two nodes if the two 
corresponding intervals intersect.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/interval-1.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;150px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Definition:&lt;/em&gt; A graph $G$ is an interval graph if $G=I(S)$ for some set 
of intervals $S$.&lt;/p&gt;

&lt;p&gt;From the picture above, we know that a path on four vertices is an 
interval graph. A graph that is not an interval graph is the cycle on 
four vertices. Consider the following picture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/interval-2.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;150px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the red node and the 
blue node are not adjacent, they must correspond to two disjoint 
intervals, and without loss of generality, the red interval comes first. 
The black interval should intersect both the red and the blue intervals. 
Thus it uses the space between these two.
The same holds for the green interval, but the black and the green 
intervals should not intersect, thus the construction is impossible.&lt;/p&gt;

&lt;p&gt;Here is a characterization of interval graphs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Theorem:&lt;/em&gt; A graph $G=(V,E)$ is an interval graph, if and only if, 
there exists an ordering of the nodes, such that: there does not exist 
three nodes $a&amp;lt;b&amp;lt;c$, with $(a,b)\notin E$ and $(a,c)\in E$.&lt;/p&gt;

&lt;p&gt;In other words, the following pattern is forbidden:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/interval-pattern.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;100px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof sketch:&lt;/em&gt; 
The key idea for the proof is to consider the ordering of the left 
endpoints of the intervals. First, given a set of intervals 
it is easy to see that the ordering given by these left endpoints avoids 
the pattern. Second, to build a set of intervals, given the ordering, 
one just as to put the left endpoint of the intervals in the right order
and then to make put the right endpoint at the spot where the right most 
neighbor starts. (The pattern insures that this construction is correct.)&lt;/p&gt;

&lt;h2 id=&quot;similar-characterizations&quot;&gt;Similar characterizations&lt;/h2&gt;

&lt;p&gt;Thus these two classes can be characterized in a 
similar way: the existence of a vertex ordering that 
avoids some small pattern. Our paper is about understanding what are the
classes that can be defined this way. In the next post, we will define 
this more formally.&lt;/p&gt;

</description>
        <pubDate>Thu, 17 Oct 2019 00:00:00 -0300</pubDate>
        <link>https://discrete-notes.github.io///patterns-1</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///patterns-1</guid>
      </item>
    
      <item>
        <title>Summer-winter notes</title>
        <description>&lt;p&gt;Some notes for summer 2019, or actually winter 2019, as I moved to Chile.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/azotea.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
The Andes.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;queue-number-again-and-applications&quot;&gt;Queue number again, and applications&lt;/h2&gt;

&lt;p&gt;A recent result about the so-called &lt;em&gt;queue number&lt;/em&gt; was mentioned on this blog in March 
(see &lt;a href=&quot;./stoc-socg-picks&quot;&gt;here&lt;/a&gt;). The result says that this graph parameter is bounded 
on planar graph that have bounded degree.
The result has now been generalized to any planar graph! 
The &lt;a href=&quot;https://arxiv.org/pdf/1904.04791.pdf&quot;&gt;paper is on arxiv&lt;/a&gt; and will be presented 
at FOCS 2019.
I haven’t looked into the paper, but I know from 
&lt;a href=&quot;https://11011110.github.io/blog/2019/08/10/report-from-cccg.html&quot;&gt;David Epptein’s blog&lt;/a&gt; 
that it is a byproduct of showing that “every planar graph is a subgraph of a strong 
product of a path graph and a bounded-treewidth graph”.&lt;/p&gt;

&lt;p&gt;Another &lt;a href=&quot;https://arxiv.org/abs/1908.03341&quot;&gt;recent paper on the arxiv&lt;/a&gt; 
uses this result to design shorter labeling schemes for planar graphs.
As the abstract says: 
“In graph-theoretical terms, this implies an explicit construction 
of a graph on $n^{4/3}+o(1)$ vertices that contains all planar graphs 
on n vertices as induced subgraphs”.
(Thanks to Cyril Gavoille for pointing out this paper.)&lt;/p&gt;

&lt;h2 id=&quot;unfolding-polytopes-durers-conjecture&quot;&gt;Unfolding polytopes: Durer’s Conjecture&lt;/h2&gt;

&lt;p&gt;Given a polyhedra, one can cut along the edges to unfold it on the plane as in the following drawing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/durer-1.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But sometimes, if you cut along some edges, you may have an overlap in the unfolding, as the
second unfolding of the 
following  example (borrowed from &lt;a href=&quot;http://mathworld.wolfram.com/Unfolding.html&quot;&gt;here&lt;/a&gt;) shows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/durer-2.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.openproblemgarden.org/op/d_urers_conjecture&quot;&gt;Durer’s conjecture&lt;/a&gt; states that 
“Every convex polytope has a non-overlapping edge unfolding”.
A non-overlapping edge unfolding is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Net_(polyhedron)&quot;&gt;a net&lt;/a&gt;, 
so the conjecture can be reformulated as: “Every convex polytope has a net”.
It is surprising that such a cute and simple result is still open.&lt;/p&gt;

&lt;p&gt;I learned about this conjecture in 
&lt;a href=&quot;https://11011110.github.io/blog/2019/08/10/report-from-cccg.html&quot;&gt;Eppstein’s report on CCCG&lt;/a&gt;. 
More information can be found in 
&lt;a href=&quot;https://www.mi.fu-berlin.de/math/groups/discgeom/ziegler/Preprintfiles/127PREPRINT.pdf&quot;&gt;Ten Problems in Geometry&lt;/a&gt;
by Moritz W. Schmitt and Günter M. Ziegler.&lt;/p&gt;

&lt;h2 id=&quot;more-on-graphs-inspired-by-objects-and-nature&quot;&gt;More on graphs inspired by objects and nature&lt;/h2&gt;

&lt;p&gt;We had a &lt;a href=&quot;./april-may-2019-notes&quot;&gt;post in april&lt;/a&gt; on graphs that have some meaning 
in terms of real-world objects.&lt;/p&gt;

&lt;p&gt;Two additional pointers on this topic:
* a &lt;a href=&quot;https://11011110.github.io/blog/2013/12/07/kinematic-chains-and.html&quot;&gt;blog post&lt;/a&gt; about graph that have one degree of freedom when you want to move them.
* the &lt;a href=&quot;https://www.ics.uci.edu/~eppstein/pubs/Epp-WADS-19.pdf&quot;&gt;slides&lt;/a&gt; of a talk about graphs inspired by nature, for example by crystals and by bubbles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/cristaux-bulles.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
The red circles are the nodes on the right drawing, but on the left ones they are part of the procedure to draw the crystals. See the slides for more details.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;automata-size-to-separate-two-words&quot;&gt;Automata size to separate two words&lt;/h2&gt;

&lt;p&gt;Yet another very simple and still open problem: consider two strings of length $n$, what is the size of the smallest deterministic automaton than accepts one and rejects the other? 
Or more precisely, what is the maximum over all such pairs of this size? 
The upper bound is polynomial in $n$ (something like $\sqrt(n)$) and the lower bound is $\Omega(\log n)$. An exponential gap!&lt;/p&gt;

&lt;p&gt;For more on this see these two posts by Lipton: &lt;a href=&quot;https://rjlipton.wordpress.com/2019/09/08/separating-words-by-automata/&quot;&gt;here&lt;/a&gt; and
&lt;a href=&quot;https://rjlipton.wordpress.com/2019/09/16/separating-words-decoding-a-paper/&quot;&gt;there&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The paper about the matching lower bound that I mentioned here several times won the best paper award at FOCS! I think it’s a remarkable research story: identifying a fundamental problem, telling everybody about it, learning more and more, and finally solving it, and getting an award for it!&lt;/li&gt;
  &lt;li&gt;Some day, I’ll learn about spanners, I hope. &lt;a href=&quot;https://arxiv.org/pdf/1909.03152.pdf&quot;&gt;Here&lt;/a&gt; is a recent survey that may help.&lt;/li&gt;
  &lt;li&gt;A &lt;a href=&quot;https://arxiv.org/abs/1907.05257&quot;&gt;paper appeared on the arxiv&lt;/a&gt; on the recognition of intersection graphs of objects touching a diagonal line, related to &lt;a href=&quot;https://www.dii.uchile.cl/~feuilloley/publications/rectangles_DCG15.html&quot;&gt;this paper of mine&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 17 Sep 2019 00:00:00 -0300</pubDate>
        <link>https://discrete-notes.github.io///summer-winter-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///summer-winter-2019-notes</guid>
      </item>
    
      <item>
        <title>June notes</title>
        <description>&lt;p&gt;Some notes for June.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/saint-laurent.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;600px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
&lt;a href=&quot;http://www.st-laurent-de-la-cabrerisse.com/eng/&quot;&gt;Saint Laurent de la Cabrerisse&lt;/a&gt;
where the &lt;a href=&quot;https://www.irit.fr/algotel2019/index.html&quot;&gt;Algotel conference&lt;/a&gt; 
took place.&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;recorded-talks-and-collaborating-virtually&quot;&gt;Recorded talks and collaborating virtually&lt;/h2&gt;

&lt;p&gt;Continuing on the topic of video conferences (e.g. to avoid plane travel),
an important question is: can virtual conferences, and more generally 
communication via video-conference, be really efficient?&lt;/p&gt;

&lt;p&gt;I have to admit that, although I really like the idea of video conference, 
for talks or for work, I have 
difficulties following recorded talks, and I’m not very comfortable with 
video calls. 
Here are the reasons I could find for why video talks and calls are not as good 
as their physical analogues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;in a recorded talk, you can escape, take a coffee, answer an email, that is 
you can “escape the room”, thus you are less focused.&lt;/li&gt;
  &lt;li&gt;videos are less immersive: the sound is bad, what you watch 
occupies a very small part of your visual field, and in a talk you don’t feel 
that you are part of a crowd participating in something.&lt;/li&gt;
  &lt;li&gt;A consequence of the previous bullet is that the experience is socially less 
comfortable: in a real meeting, there can be a long silence and it’s fine, but 
during a video call, it’s often awkward.&lt;/li&gt;
  &lt;li&gt;Also video conferences are very static. This is especially true for talks: the 
main part of the screen is taken by the slide, which moves once every minute on 
average, and even if you see the speaker, you just see a face, no body movements.&lt;/li&gt;
  &lt;li&gt;Related to the previous point: I don’t know good ways to have a shared 
whiteboard when working through video, which reduces a lot the efficiency of the 
conversation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of these problems are kind of solved by some frameworks. For example 
&lt;a href=&quot;https://sites.google.com/site/plustcs/&quot;&gt;TCS+&lt;/a&gt; is an online seminar where the 
speaker does not record a video alone: it is more like a large video conference
with a dozen of labs connected, and one person giving the talk. (The 
talks are actually recorded so you can watch them offline, but it is not the 
main goal.) 
This means that there is a crowd, and that you cannot escape. Also if you have a 
good video/sound system, then it might be quite immersive. 
I never had the opportunity to test this so I can’t tell.&lt;/p&gt;

&lt;p&gt;Note that some people think the exact opposite of me: they actually prefer 
watching a recorded talk, because it’s offline: you can stop, rethink what was 
said, check a reference, jump to the next section etc. 
Also note that attending a live talk can also be boring, so it also depends a 
lot on the speaker.&lt;/p&gt;

&lt;p&gt;If you know good alternatives to skype (ideally open-source, with a possibility of 
drawing, and multi-users), send me an email. This will be useful not only to me 
but also to other people: I timidly signed to be part of a pool of people 
thinking about such technical topics, in the 
&lt;a href=&quot;https://labos1point5.org/en/home/&quot;&gt;Labo 1.5 collective&lt;/a&gt; I talked about last 
month.&lt;/p&gt;

&lt;h2 id=&quot;terence-tao-on-the-radius-of-the-earth&quot;&gt;Terence Tao on the radius of the Earth&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Terence_Tao&quot;&gt;Terence Tao&lt;/a&gt; has a blog, 
&lt;a href=&quot;https://terrytao.wordpress.com/&quot;&gt;What’s new&lt;/a&gt;. Most of the content is way too 
complicated for me, but once in a while he writes excellent posts about simpler
things. 
A very recent one is 
&lt;a href=&quot;https://terrytao.wordpress.com/2019/05/25/the-spherical-cayley-menger-determinant-and-the-radius-of-the-earth/&quot;&gt;about the radius of the Earth and determinants&lt;/a&gt;. An older one is about 
&lt;a href=&quot;https://terrytao.wordpress.com/2009/03/23/sailing-into-the-wind-or-faster-than-the-wind/&quot;&gt;sailing&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;basic-and-advanced-tools-in-combinatorics&quot;&gt;Basic and advanced tools in combinatorics&lt;/h2&gt;

&lt;p&gt;I have heard that some people tend to despise combinatorics as being “very basic”.
To some extent, my research is very basic: most of the time I consider very 
simple objects, a few edges and a few nodes in a graph. I almost never use 
complicated tools which have been improved over the centuries and which 
need time to master. Sometimes it is a bit unsettling: one can feel like playing
child games, compared to what one was taught at the university.&lt;/p&gt;

&lt;p&gt;On the other hand of course, it is very annoying when people boast about using
complicated stuff, and it is always very nice to find a basic proof of a theorem.&lt;/p&gt;

&lt;p&gt;Something I like is when you have a basic proof that is a bit of a mess, with
many cases for example, and using the point of view of a more general 
theory you suddenly understand things better. I had this experience with some 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Matroid&quot;&gt;matroid&lt;/a&gt; structure in
&lt;a href=&quot;https://pages.lip6.fr/Laurent.Feuilloley/publications/error_sensitive.html&quot;&gt;this paper&lt;/a&gt;
(although it does not appear at all in the final write-up).&lt;/p&gt;

&lt;p&gt;I was reminded of this by 
&lt;a href=&quot;https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html&quot;&gt;this post&lt;/a&gt;
on David Eppstein’s blog, where he describes a new point of view he has on a 
matching problem thanks to a more advanced notion of matroid (yes matroids, again).&lt;/p&gt;

&lt;h2 id=&quot;john-ellipsoid-and-the-s&quot;&gt;John ellipsoid and the “‘s”&lt;/h2&gt;

&lt;p&gt;This &lt;a href=&quot;https://arxiv.org/pdf/1905.11580.pdf&quot;&gt;arxiv preprint&lt;/a&gt; made me discover 
&lt;a href=&quot;https://en.wikipedia.org/wiki/John_ellipsoid&quot;&gt;John ellipsoid&lt;/a&gt;. Given a polytope
in $n$ dimensions, John ellipsoid is the largest-volume ellipsoid included in 
the polytope. On the picture below it is the red ellipse (more or less).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/john-ellipsoid.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An interesting property is that if you take the ellipsoid and dilate it by a 
factor $n$, then this new ellipsoid contains the polytope (more or less the blue 
ellipsoid on the picture). At first I thought that dimension 3 should not require 
more dilatation than dimension 2, but if you think about an equilateral triangle 
in the plane, and a tetrahedron in a 3D space, you see that the inner ball of the 
tetrahedron has to be smaller than the inner disk of the equilateral triangle, 
and that the outer ball has to be larger.&lt;/p&gt;

&lt;p&gt;The arxiv preprint presents an algorithm to approximate John ellipsoid.&lt;/p&gt;

&lt;p&gt;By the way, why is it “John ellipsoid” and, say, “Dijktra’s algorithm”, why is 
there a “‘s” sometimes but not always?&lt;/p&gt;

&lt;h2 id=&quot;eppsteins-socg-report-and-robust-statistics&quot;&gt;Eppstein’s SOCG report and robust statistics&lt;/h2&gt;

&lt;p&gt;Eppstein wrote a nice
&lt;a href=&quot;https://11011110.github.io/blog/2019/06/21/report-from-socg.html&quot;&gt;report of SOCG&lt;/a&gt;,
with a selection of talks that caught his interest.&lt;/p&gt;

&lt;p&gt;He refers to &lt;a href=&quot;https://en.wikipedia.org/wiki/Centerpoint_(geometry)&quot;&gt;Tukey depth&lt;/a&gt;, 
which is related to Tukey median, which is a 
generalization of the median in higher dimension. I discovered this notion in a great 
talk by &lt;a href=&quot;https://people.csail.mit.edu/moitra/&quot;&gt;Ankur Moitra&lt;/a&gt; about the 
computational aspects of
&lt;a href=&quot;https://en.wikipedia.org/wiki/Robust_statistics&quot;&gt;robust statistics&lt;/a&gt;, at 
&lt;a href=&quot;http://2018.highlightsofalgorithms.org/&quot;&gt;HALG 2018&lt;/a&gt;. 
The basic story is the following. Suppose you have data, and you want to 
summarize it by one point. You want to use the average, but because your data 
has some adversarial noise, the average of the noisy data might be far from the 
average of the “real data”. If you are in 1D then you can take the median, which 
is robust to such noise. But if you are in $d$ dimension, then there are several 
generalizations of the median, 
and either they are not robust to noise or they are NP-hard to compute, and then 
you have to think.&lt;/p&gt;

&lt;p&gt;Two references on robust statistics : 
&lt;a href=&quot;http://people.csail.mit.edu/moitra/docs/robust2.pdf&quot;&gt;here&lt;/a&gt;
and 
&lt;a href=&quot;https://www.csun.edu/~ctoth/Handbook/chap58.pdf&quot;&gt;there&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper I talked about in a 
&lt;a href=&quot;https://discrete-notes.github.io/simulation-1&quot;&gt;series of posts in February&lt;/a&gt;, by 
the Aalto distributed computing people has been accepted to 
&lt;a href=&quot;http://focs2019.cs.jhu.edu/&quot;&gt;FOCS 2019&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;a href=&quot;http://eatcs.org/images/bulletin/beatcs128.pdf&quot;&gt;128th bulletin of the EATCS&lt;/a&gt; 
is out, with a chapter on matching with preference lists, and a chapter on 
leader election in directed dynamic graphs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 02 Jul 2019 00:00:00 -0400</pubDate>
        <link>https://discrete-notes.github.io///june-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///june-2019-notes</guid>
      </item>
    
      <item>
        <title>Material on distributed graph algorithms</title>
        <description>&lt;p&gt;This post is a list of pointers to books and surveys about distributed graph 
algorithms (LOCAL model, CONGEST model, and friends).
I probably missed some references, as typing “local model” in a search engine 
is not very helpful if you are not looking for top model agencies. 
Other references are most welcome!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/local.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;distributed-computing-a-locality-sensitive-approach-by-peleg-and-other-references&quot;&gt;Distributed computing: a locality-sensitive approach, by Peleg, and other references&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://epubs.siam.org/doi/book/10.1137/1.9780898719772&quot;&gt;Distributed computing: a locality-sensitive approach&lt;/a&gt;
by &lt;a href=&quot;http://www.weizmann.ac.il/math/peleg/&quot;&gt;David Peleg&lt;/a&gt; is the classic book 
about the local model. It’s from 2000, so it’s getting a bit outdated in terms 
of results.&lt;/p&gt;

&lt;h2 id=&quot;distributed-graph-coloring-by-barenboim-and-elkin&quot;&gt;Distributed graph coloring, by Barenboim and Elkin&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.bgu.ac.il/~elkinm/book.pdf&quot;&gt;Distributed graph coloring&lt;/a&gt;
by 
&lt;a href=&quot;https://www.openu.ac.il/personal_sites/leonid-barenboim/&quot;&gt;Leonid Bareboim&lt;/a&gt;
and &lt;a href=&quot;https://www.cs.bgu.ac.il/~elkinm/&quot;&gt;Michael Elkin&lt;/a&gt;, is a more recent book, 
with a focus on coloring. It explains quite a few techniques, and has a list of 
open problems (some of these problems have been solved already).&lt;/p&gt;

&lt;h2 id=&quot;survey-of-local-algorithms-by-suomela&quot;&gt;Survey of local algorithms, by Suomela&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://users.ics.aalto.fi/suomela/doc/local-survey.pdf&quot;&gt;survey of local algorithms&lt;/a&gt;
by &lt;a href=&quot;https://users.ics.aalto.fi/suomela/&quot;&gt;Jukka Suomela&lt;/a&gt; is the reference for 
results about constant-time computations in the local model.&lt;/p&gt;

&lt;h2 id=&quot;distributed-algorithms-by-suomela&quot;&gt;Distributed algorithms, by Suomela&lt;/h2&gt;

&lt;p&gt;A neat online textbook is
&lt;a href=&quot;https://users.ics.aalto.fi/suomela/da/da-print.pdf&quot;&gt;Distributed algorithm&lt;/a&gt;
by Jukka Suomela.
In addition to the classic topics such as coloring, that are contained in most 
references listed here, it has a focus on showing the similarities and differences 
between different models (such as port numbers and unique identifiers), and on the 
graph theory tools that can be used (such as covering maps and Ramsey theory).&lt;/p&gt;

&lt;h2 id=&quot;the-swiss-german-lecture-notes&quot;&gt;The Swiss-German lecture notes&lt;/h2&gt;

&lt;p&gt;There are several courses related to the local model that are taught in 
Switzerland and South Germany. They are rather close one from another, as they 
stem from the same Zurich source (but then they evolved on their own).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The current version of the original source is 
&lt;a href=&quot;https://disco.ethz.ch/courses/podc/&quot;&gt;Principles of Distributed Computing&lt;/a&gt;, 
by &lt;a href=&quot;https://disco.ethz.ch/members/wroger&quot;&gt;Roger Wattenhofer&lt;/a&gt; (and more recently
&lt;a href=&quot;https://people.csail.mit.edu/ghaffari/&quot;&gt;Mohsen Ghaffari&lt;/a&gt; but his part is 
covered below). In addition to the classic topics of synchronous computing, it 
covers some topics at the boundary with asynchronous computing (such as 
synchronizers), or fully asynchronous computing (such as shared objects).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A set of lecture notes by &lt;a href=&quot;http://ac.informatik.uni-freiburg.de/kuhn/&quot;&gt;Fabian Kuhn&lt;/a&gt;
is available chapter by chapter on
&lt;a href=&quot;http://ac.informatik.uni-freiburg.de/teaching/ss_18/network-algorithms.php&quot;&gt;the course webpage&lt;/a&gt;.
The topics are very close from the ones of the bullet above. Two topics that 
appear only here: dynamic networks and
&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_network_coding&quot;&gt;network coding&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another set of lecture notes is 
&lt;a href=&quot;https://www.mpi-inf.mpg.de/departments/algorithms-complexity/teaching/winter18/tods/&quot;&gt;Theory of Distributed Systems&lt;/a&gt;, 
by &lt;a href=&quot;http://people.mpi-inf.mpg.de/~clenzen/&quot;&gt;Christoph Lenzen&lt;/a&gt;. It covers 
(in addition to the classic material) self-stabilization and 
routing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Probably the most recent course is
&lt;a href=&quot;https://disco.ethz.ch/courses/podc/lecturenotes/LOCAL.pdf&quot;&gt;Distributed graph algorithms&lt;/a&gt;
by 
&lt;a href=&quot;https://people.csail.mit.edu/ghaffari/&quot;&gt;Mohsen Ghaffari&lt;/a&gt;. 
It is a shorter text, with a focus on algorithmic techniques for the local model, 
e.g. network decomposition.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;material-covering-only-the-basics-or-only-specialized-content&quot;&gt;Material covering only the basics, or only specialized content&lt;/h2&gt;

&lt;p&gt;A standard book is 
&lt;a href=&quot;https://www.elsevier.com/books/distributed-algorithms/lynch/978-1-55860-348-6&quot;&gt;Distributed Algorithms&lt;/a&gt;
by &lt;a href=&quot;http://people.csail.mit.edu/lynch/&quot;&gt;Nancy Lynch&lt;/a&gt;, but most of the book is 
off-topic for this post, because it deals with asynchronous systems.&lt;/p&gt;

&lt;p&gt;Yet another reference (with little material on the LOCAL model) is the online 
textbook 
&lt;a href=&quot;http://www.cs.yale.edu/homes/aspnes/classes/465/notes.pdf&quot;&gt;Notes on Theory of Distributed Systems&lt;/a&gt;
by &lt;a href=&quot;http://www.cs.yale.edu/homes/aspnes/&quot;&gt;James Aspnes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you look for &lt;a href=&quot;https://en.wikipedia.org/wiki/Self-stabilization&quot;&gt;self-stabilizing algorithms&lt;/a&gt;
then a reference from 2000 is 
&lt;a href=&quot;https://mitpress.mit.edu/books/self-stabilization&quot;&gt;Self-Stabilization &lt;/a&gt; 
by
&lt;a href=&quot;https://in.bgu.ac.il/en/natural_science/cs/dolev//Pages/default.aspx&quot;&gt;Shlomi Dolev&lt;/a&gt;, 
and a very recent one is 
&lt;a href=&quot;https://www.morganclaypool.com/doi/abs/10.2200/S00908ED1V01Y201903DCT015&quot;&gt;Introduction to Distributed Self-Stabilizing Algorithms&lt;/a&gt;
by
&lt;a href=&quot;http://www-verimag.imag.fr/Karine-Altisen,102.html?lang=en&quot;&gt;Karine Altisen&lt;/a&gt;,
&lt;a href=&quot;http://www-verimag.imag.fr/~devismes/WWW/introduction.html&quot;&gt;Stéphane Devismes&lt;/a&gt;,
&lt;a href=&quot;https://pages.lip6.fr/Swan.Dubois/&quot;&gt;Swan Dubois&lt;/a&gt;,
and 
&lt;a href=&quot;https://pages.lip6.fr/Franck.Petit/&quot;&gt;Franck Petit&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;in-french&quot;&gt;In French&lt;/h3&gt;
&lt;p&gt;Also, if you read French, you may be interested in 
&lt;a href=&quot;https://www.irif.fr/_media/users/pierref/notes_algo_distribue.pdf&quot;&gt;Algorithmique distribuée pour les réseaux&lt;/a&gt;
by &lt;a href=&quot;https://www.irif.fr/users/pierref/index&quot;&gt;Pierre Fraigniaud&lt;/a&gt;, and 
&lt;a href=&quot;http://dept-info.labri.fr/~gavoille/UE-AD/cours.pdf&quot;&gt;Algorithmes distribués&lt;/a&gt; 
by &lt;a href=&quot;http://dept-info.labri.fr/~gavoille/&quot;&gt;Cyril Gavoille&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Thanks to Jukka Suomela for pointing out some references.&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Jun 2019 00:00:00 -0400</pubDate>
        <link>https://discrete-notes.github.io///local-model</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///local-model</guid>
      </item>
    
      <item>
        <title>April-May notes</title>
        <description>&lt;p&gt;Some deadlines disturbed the schedule of the monthly notes ; this post is for 
both April and May.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/table.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;cuckoo-cycles&quot;&gt;Cuckoo cycles&lt;/h2&gt;

&lt;p&gt;A 
&lt;a href=&quot;https://blog.computationalcomplexity.org/2019/04/cuckoo-cycles.html&quot;&gt;guest post&lt;/a&gt; 
by John Tromp, on the Computational Complexity blog, is about the cuckoo cycle 
problem.&lt;/p&gt;

&lt;p&gt;Basically there are some crypto-currencies, where the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Proof-of-work_system&quot;&gt;proof-of-work&lt;/a&gt; consists in 
finding cycles in a huge graph. 
This graph is the cuckoo graph, whose edges are defined by the hash function of 
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cuckoo_hashing&quot;&gt;cuckoo hashing&lt;/a&gt;. 
The post describes briefly the method used by the miners for this problem.&lt;/p&gt;

&lt;p&gt;Interesting to see a theoretical problem tackled in very practical way.&lt;/p&gt;

&lt;h2 id=&quot;pcp-videos&quot;&gt;PCP videos&lt;/h2&gt;

&lt;p&gt;At the end of 2018, a workshop took place in Tel Aviv, about the
&lt;a href=&quot;https://en.wikipedia.org/wiki/PCP_theorem&quot;&gt;PCP theorem&lt;/a&gt; and related topics.
Great news: the talks have been recorded, and the videos are available on a
&lt;a href=&quot;https://www.youtube.com/playlist?list=PLGRBwz8taWHiBHlgnX98zrbnWQCeDtFQ_&quot;&gt;youtube channel&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I watched &lt;a href=&quot;https://en.wikipedia.org/wiki/Ronitt_Rubinfeld&quot;&gt;Ronitt Rubinfield&lt;/a&gt;’s 
talk, and it’s very good. She tells the story of checkers and testers. 
Just a bit to arouse your interest: like many people, I know interactive proofs 
for difficult problems 
(isomorphism, satisfiability etc.), but actually testing and interactive proofs 
have started with things as simple as GCD.&lt;/p&gt;

&lt;h2 id=&quot;graphs-defined-bymatchsticks-pennies-and-hinges&quot;&gt;Graphs defined by matchsticks, pennies and hinges.&lt;/h2&gt;

&lt;p&gt;Here are three types of graphs that have a description in terms of real-world 
objects.&lt;/p&gt;

&lt;p&gt;First, &lt;a href=&quot;https://en.wikipedia.org/wiki/Matchstick_graph&quot;&gt;matchstick graphs&lt;/a&gt; are 
graphs that you can draw on a table with matchsticks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/alumettes.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In more precise terms, these are planar
&lt;a href=&quot;https://en.wikipedia.org/wiki/Unit_distance_graph&quot;&gt;unit-distance graphs&lt;/a&gt;, that 
is graphs that you can draw on the plane with only (straight) edges of length 1.&lt;/p&gt;

&lt;p&gt;Second, &lt;a href=&quot;https://en.wikipedia.org/wiki/Penny_graph&quot;&gt;penny graphs&lt;/a&gt; are graphs 
that can be represented by pennies on a table. More precisely, a graph is a 
penny graph, if it is possible to represent each vertex by a unit disk, and to 
place the disk in such a way that they touch only at the boundaries, and that 
each contact represents an edge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/pennies.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Penny graphs are matchsticks graphs but the reverse is not true.&lt;/p&gt;

&lt;p&gt;Finally consider the following experiment. 
Take a graph, replace edges by rods and nodes by hinges.
Now, if when you push on some side, the graph changes shape, then the graph is 
flexible, otherwise it is 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_rigidity&quot;&gt;rigid&lt;/a&gt;.
For example a cube is flexible, and a tetrahedron is rigid.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/rigide.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;breaking-symmetries-in-cells&quot;&gt;Breaking symmetries, in cells&lt;/h2&gt;

&lt;p&gt;A common challenge in distributed algorithms is breaking symmetry: 
if two processors have exactly the same view, they have to take the same step. 
For example, on a cycle, if the nodes do not have a way to break 
symmetry (for example unique identifiers), then they all have the same view, 
thus they take the same steps, and will output the same thing.&lt;/p&gt;

&lt;p&gt;Let us now take a look at biology. We know that an essential mechanism in living 
stuff is &lt;a href=&quot;https://en.wikipedia.org/wiki/Cell_division&quot;&gt;cell division&lt;/a&gt;, 
where a cell is basically replaced by two copies of itself. 
At the very beginning of a new organism, there is one cell, and then it divides 
again and again, and at the end you get a fully developed living being.&lt;/p&gt;

&lt;p&gt;From a distributed computing perspective, int his context,
it would be natural to have the exact same cell everywhere, with 
the same development. 
It is not the case: brain cells are not muscle cells. 
Ok, so a first reason it that there is the environment: two identical cells 
could evolve differently because the things around them are different. 
In some sense the environment breaks symmetry.
But if you think about an embryo, the environment is probably very similar on 
one side or the other, so at least at the beginning it should be just a heap of 
identical cells, and it is not the case. So, what’s going on?&lt;/p&gt;

&lt;p&gt;A quick look at wikipedia provides the answer: cell division does not produce 
identical cells. In particular, stem cells
&lt;a href=&quot;https://en.wikipedia.org/wiki/Asymmetric_cell_division&quot;&gt;divide asymmetrically&lt;/a&gt;.
The wikipedia article goes quickly into specific things, but from what I 
understand, that fact that the sperm cell fertilizes the egg cell “in one 
direction” gives a special 
direction, that influences the way the small stuff in the cell is distributed.
Thus what is inside the cell is not uniformly distributed. 
This asymmetry is then propagated at the level of the cells. 
This is because the division of a cell happens in such a way that the two cells 
have very different small stuff inside (because some proteins make this happen 
this way).&lt;/p&gt;

&lt;h2 id=&quot;discrete-analysis-and-light-bulbs&quot;&gt;Discrete analysis and light bulbs&lt;/h2&gt;

&lt;p&gt;Once in while I remember that the 
&lt;em&gt;&lt;a href=&quot;https://discreteanalysisjournal.com/&quot;&gt;Discrete Analysis&lt;/a&gt;&lt;/em&gt;
journal exists. 
It is one of these few journals that are really open access, 
in the sense of the &lt;a href=&quot;https://freejournals.org/&quot;&gt;Free journal network&lt;/a&gt;.
One can check the list of free journals 
&lt;a href=&quot;https://freejournals.org/current-member-journals/&quot;&gt;here&lt;/a&gt;.
The relevant journals for complexity/algorithms/graphs are
&lt;a href=&quot;https://dmtcs.episciences.org/&quot;&gt;DMTCS&lt;/a&gt;, 
&lt;a href=&quot;https://journals.carleton.ca/jocg/index.php/jocg&quot;&gt;JoCG&lt;/a&gt;, 
&lt;a href=&quot;http://jgaa.info/&quot;&gt;JGAA&lt;/a&gt;, 
&lt;a href=&quot;https://lmcs.episciences.org/&quot;&gt;LMCS&lt;/a&gt;, 
&lt;a href=&quot;https://digitalcommons.georgiasouthern.edu/tag/&quot;&gt;TAG&lt;/a&gt;, 
and 
&lt;a href=&quot;https://theoryofcomputing.org/introduction.html&quot;&gt;ToC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Discrete analysis&lt;/em&gt; is mainly about analytic methods in combinatorics, thus not 
exactly my cup of tea, but I know it from the blog of 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Timothy_Gowers&quot;&gt;Timothy Gowers&lt;/a&gt;. 
And once in a while, there is a paper that arouses my curiosity.&lt;/p&gt;

&lt;p&gt;The website is especially well-done for readers like me. 
Unlike usual journals with ugly websites, and just a list of papers, 
Discrete Analysis has a nice picture for each paper, a small page describing it,
with an introduction written by a member of the editorial board. All this is 
very nice, although at first sight it may look a bit too flashy.&lt;/p&gt;

&lt;p&gt;A paper that “makes the cover” of the website these days is 
&lt;a href=&quot;https://discreteanalysisjournal.com/article/2730-fixed-energy-harmonic-functions&quot;&gt;Fixed-energy harmonic functions&lt;/a&gt;.
In addition to the introduction, there is the video of a talk about the paper. 
It goes quickly into maths that I can’t understand without effort, but the first 
five minutes explain the problem in a fun fashion. 
Consider an electric circuit with light bulbs. 
The circuit is represented by a 
graph and every edge has a weight: the resistance of the light bulb of this 
section of the circuit. 
Now you can plug the wires of a battery at two arbitrary nodes of the graph and 
ask for the energy dissipated on each bulb, that is for the brightness of each 
bulb. This is a classic exercise in physics, and on reasonable circuits one 
just has to use simple laws of electricity and a lot of time. 
For example the video features this circuit, where all the resistances are equal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/ampoules.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The paper studies the reverse problem: given a graph, and two nodes to plug the 
battery, how to choose the resistance of the light bulbs such that they all 
shine with the same brightness?&lt;/p&gt;

&lt;h2 id=&quot;pie-rule&quot;&gt;Pie rule&lt;/h2&gt;

&lt;p&gt;In the &lt;a href=&quot;./march-2019-notes-1&quot;&gt;March notes&lt;/a&gt;, I mentioned some criteria for fair 
item assignment. One is well-known: the first player defines two bundles of items, 
the second player choose the bundle it prefers. This way the first player will 
design the bundles in the most balanced way. 
A more concrete setting is with a pie (hance the name of “pie rule”): 
the first player cuts the cake into two pieces, and 
the second player chooses the slice.&lt;/p&gt;

&lt;p&gt;I discovered that something similar exists for games such as chess, go, etc. 
In some of these two-players games, the first player has a great advantage. 
To mitigate this, one can use the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Pie_rule&quot;&gt;pie rule&lt;/a&gt;. Let say that we consider 
chess, and that A plays white, and B plays black. Then the pie rule 
says that after A has played the first move, B has the choice between:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;playing the normal way until the end&lt;/li&gt;
  &lt;li&gt;changing the color, that is taking the white side, and letting A do the first 
move for the black, and then playing the normal way until the end (with A playing 
black, and B playing white).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This way, A has an incentive to make a not-too-good move to start, which 
mitigates its first-player advantage.&lt;/p&gt;

&lt;h2 id=&quot;multiplicative-weight-update&quot;&gt;Multiplicative weight update&lt;/h2&gt;

&lt;p&gt;The 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Multiplicative_weight_update_method&quot;&gt;multiplicative weight update method&lt;/a&gt; 
is a general algorithmic method, that is not known enough, and that has been 
discovered several times in several contexts. 
The basic description is the following. Suppose you have several experts, that 
give you advice for a series of decisions that you take one by one. 
After each decision, you redesign the way you aggregate the advice to take a 
decision. Typically you want to give more weight to experts that have given good
advice.
The multiplicative weight update method basically states that updating the weight
in multiplicative manner is a good idea.
It seems pretty specific but it is actually very general.&lt;/p&gt;

&lt;p&gt;In addition to the wikipedia page and the references therein (including a survey 
by Arora, Hazan and Kale), you may want to take a look at a series of post that 
Luca Trevisan is currently writing 
on this topic. 
Actually he is mostly interested in the developments of this 
method that have been used in a variety of papers in complexity recently. 
The series of posts starts 
&lt;a href=&quot;https://lucatrevisan.wordpress.com/2019/04/17/online-optimization-for-complexity-theorists/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jeremy Kun also has a 
&lt;a href=&quot;https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/&quot;&gt;nice post&lt;/a&gt; 
about the method.&lt;/p&gt;

&lt;h2 id=&quot;maximum-matchings-that-are-not-perfect&quot;&gt;Maximum matchings that are not perfect&lt;/h2&gt;

&lt;p&gt;I recently had to think about graphs that do not have a perfect matching. 
The first example is of course graphs with an odd number of vertices. 
Next, you can find things like the one below (with a maximum non-perfect 
matching).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage-max.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Intuitively, in each triangle, one node must be matched with a node outside the 
triangle, but the three triangles are looking for the same node in the middle, 
and as only one can be served, the other triangles must have an unmatched 
node. 
Actually, even a star with three leaves is a good example.&lt;/p&gt;

&lt;p&gt;Remember &lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte_theorem&quot;&gt;Tutte theorem&lt;/a&gt;: 
A graph, $G = (V, E)$, has a perfect matching if and only if for every subset $U$ 
of $V$, the subgraph induced by $V − U$ has at most $|U|$ connected components with 
an odd number of vertices (definition from wikipedia). 
In the drawing above, take the middle node as $U$, you get three connected component 
with an odd number of vertices. As $3&amp;gt;1$, this graph doesn’t have perfect matching.&lt;/p&gt;

&lt;p&gt;Tutte theorem is actually a special case of the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte%E2%80%93Berge_formula&quot;&gt;Tutte–Berge formula&lt;/a&gt;, 
which states that the size of the maximum matching is: 
$\frac{1}{2} \min_{U\subseteq V}  \left(|U|-\operatorname{odd}(G-U)+|V|\right)$&lt;/p&gt;

&lt;p&gt;where $\operatorname{odd}$ counts the number of odd connected components.&lt;/p&gt;

&lt;h2 id=&quot;irreproducibility-and-unpublished-failures&quot;&gt;Irreproducibility and unpublished failures&lt;/h2&gt;

&lt;p&gt;A
&lt;a href=&quot;https://www.nature.com/articles/d41586-019-01307-2?utm_source=twt_nnc&amp;amp;utm_medium=social&amp;amp;utm_campaign=naturenews&amp;amp;sf211598052=1&quot;&gt;paper in Nature&lt;/a&gt; 
identifies the “four horsemen of irreproducibility” that harm science. 
One of them is the publication bias: studies that do not 
conclude that there is a relation between two things are not publishable. 
Two problems with this are: (1) researchers have an incentive to find
correlations, when they should have an incentive to find the truth, and (2) if 
because of statistics a false correlation can appear in 5% of the experiments, 
then only the wrong conclusion will be made public (after 19 teams on average 
have tried, and failed to find a correlation).&lt;/p&gt;

&lt;p&gt;There are efforts to fight this problem. One is the notion of
 &lt;a href=&quot;https://cos.io/rr/&quot;&gt;registered report&lt;/a&gt;. This consists in having the journal to
review an experiment (the topic, the hypothesis tested, the method), before the 
experiment is made. If this set-up is accepted, then the experiment is made, the 
paper written and it goes through a second review.
The paper is provisionally accepted after the first review.&lt;/p&gt;

&lt;p&gt;In TCS and maths in general, this publication bias is not a central problem, as 
one can check the proofs, and hopefully be convinced that the papers published 
are correct. Nevertheless, we probably loose a lot of time trying the same 
approaches on the same problems, without making our failures public.&lt;/p&gt;

&lt;h2 id=&quot;planes-etc&quot;&gt;Planes etc.&lt;/h2&gt;

&lt;p&gt;As a follow-up to the 
&lt;a href=&quot;https://discrete-notes.github.io/march-2019-notes-2&quot;&gt;note of last month&lt;/a&gt; about 
conference travel and its environmental impact, here are a few more elements.&lt;/p&gt;

&lt;p&gt;An &lt;a href=&quot;https://theconversation.com/chercheurs-donnez-lexemple-prenez-moins-lavion-110613&quot;&gt;article (in French)&lt;/a&gt; 
about flights of researchers, mentions several alternatives. One is to have 
purely virtual conferences (such as 
&lt;a href=&quot;https://ehc.english.ucsb.edu/?page_id=12687&quot;&gt;this one&lt;/a&gt;).
Another one is to have the conference distributed among multiple sites, for 
example one in each continent. Probably worth testing…
By the way, remember that we can already enjoy a virtual TCS seminar: 
&lt;a href=&quot;https://sites.google.com/site/plustcs/&quot;&gt;TCS+&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, it’s difficult to make efforts when you are almost alone to make them and 
that it even hurts you career. Maybe a charter or a code of conduct, for 
researchers of an institution or of a community would help.
For example an important Danish newpaper now has such a charter (no domestic 
flights (remember it’s Danemark), less flying in general, less advertising for 
far away destination etc.). I couldn’t find good press coverage of this in English, 
but &lt;a href=&quot;https://www.lemonde.fr/climat/article/2019/01/16/climat-plus-de-vols-interieurs-pour-les-journalistes-de-politiken_5409874_1652612.html?xtmc=politiken&amp;amp;xtcr=5&quot;&gt;here is an article&lt;/a&gt; in the main French newspaper.&lt;/p&gt;

&lt;p&gt;EDIT(July 2019): there actually exists some rules in some universities, see 
&lt;a href=&quot;https://www.wired.com/story/climate-scientists-take-the-train/&quot;&gt;this Wired article&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 23 May 2019 00:00:00 -0400</pubDate>
        <link>https://discrete-notes.github.io///april-may-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///april-may-2019-notes</guid>
      </item>
    
      <item>
        <title>Algorithms and natural history</title>
        <description>&lt;p&gt;Until recently there was in Paris an exhibition about some techniques used for 
research in natural history.
It raises some algorithmic questions and comments about bones, diamonds and 
polytopes.
(More details about this great exhibition at the end of the post.)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/grenouille.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;Researchers measure the teeth of the tadpoles this frog 
species, and learn stuff.&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;computing-skull-volumes&quot;&gt;Computing (skull) volumes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/crane.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When you get a skull, something you want to measure is how much brain you can 
fit in, that is, what is its inside volume.
This used to be done by filling the skull with some kind of grains, and then 
measuring the volume of grain. 
It is now done by with sensor measurements and computations. 
But if I give you some triangulation of an object, how do you compute the volume?
If you assume the object to be convex, then one can surely do some kind of discrete 
integrals, suming some basic polytope volumes. 
&lt;a href=&quot;https://www.ams.org/journals/mcom/1991-57-195/S0025-5718-1991-1079024-2/S0025-5718-1991-1079024-2.pdf&quot;&gt;This paper&lt;/a&gt; 
(or more precisely its introduction) seems to validate this intuition. 
It also raises the question about how the object to measure is given.&lt;/p&gt;

&lt;p&gt;This made me remember that some price was awarded for something related to 
volume computation. 
I dug out the reference: 1991 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Fulkerson_Prize&quot;&gt;Fulkerson prize&lt;/a&gt; was awarded to 
Dyer, Frieze and Kannan for works of this flavour.
Their model is black-box: you ask for a point and you are answered whether it is 
inside or outside. 
They provide a randomized PTAS&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; for approximating
the volume of high-dimensional polytopes using 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&quot;&gt;Markov chain Monte Carlo&lt;/a&gt; 
methods. 
As many counting problems can be rephrased in terms of polytopes, this is much 
more useful than just computing high-dimensional skull volumes.
For a bit more on this, see 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_volume_approximation&quot;&gt;this wikipedia page&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;inclusions-ofdiamonds-and-polytopes&quot;&gt;Inclusions of diamonds and polytopes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/diamant-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The diamond above is the &lt;a href=&quot;Tavernier Blue&quot;&gt;Blue Diamond of the French Crown&lt;/a&gt; 
that Louis XIV bought at some point of the 17th century. 
Or actually, it’s what it used to look like, because this diamond disappeared. 
It was suspected that it has been recut and is now the diamond known as the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Hope_Diamond&quot;&gt;Hope diamond&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This was presented in the exhibition because the museum in Paris has a lead cast 
of the original diamond, and used it to validated the link between the two. 
Basically the two diamonds fit so well one in the other, that is very likely 
that they are the same with only a light recut.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/diamant-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But how can you decide if two polytopes could be included one in the other?
And what’s the complexity? (Note that contrary to skulls, diamonds are pretty 
much polytopes.)&lt;/p&gt;

&lt;p&gt;First for the decision problem: the two polytopes are somehow aligned and you 
just want to check the inclusion of one into the other.
If both polytopes are given by sets of vertices, then some methods similar to
&lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_hull_algorithms&quot;&gt;convex hull computations&lt;/a&gt; 
should do the job.&lt;/p&gt;

&lt;p&gt;Now how to find this alignment? For our diamonds, the spikes and planar 
symmetries surely help. But what if your polytopes are say, 
very close to spheres, but with random perturbations? Doesn’t seem very easy. 
This looks similar to some robotics problem, such as the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Motion_planning&quot;&gt;piano mover’s problem&lt;/a&gt;, but it’s 
not quite the same.&lt;/p&gt;

&lt;h2 id=&quot;symmetrizing-old-bones&quot;&gt;Symmetrizing old bones&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/dinosaures.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;350px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yet another problems that one can transfer to polytopes. 
Here the paleontologists find skulls like the one on the left (I didn’t take 
notes of the name of this fellow), that has been deformed by non-uniform forces 
in the ground.
Then they try to reconstruct the correct shape, like the one on the right.&lt;/p&gt;

&lt;p&gt;Again if you are a human, if you know a lot about such creatures, it is possible 
to find a plausible shape by adjusting the parameters of some transformation.
In particular, you try to have a left-right symmetry.
But what if I give you an arbitrary triangulation, and tell you: find the best 
way to modify it in order to have a symmetry?&lt;/p&gt;

&lt;p&gt;Could be related to 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;principal component analysis&lt;/a&gt;, 
that gives you some notion of “this object is somehow aligned with this 
direction”, but again if you have a random near-spherical objects, this does not 
help you (but in this case, you may be happy with any plane, as an approximation).&lt;/p&gt;

&lt;h3 id=&quot;about-the-exhibition&quot;&gt;About the exhibition&lt;/h3&gt;
&lt;p&gt;The name of the exhibition in French was 
&lt;em&gt;&lt;a href=&quot;https://www.mnhn.fr/en/node/5277&quot;&gt;Secrets dévoilés : voir l’imperceptible&lt;/a&gt;&lt;/em&gt;, 
which basically means &lt;em&gt;Unveiling secrets : seeing the imperciptible&lt;/em&gt;.
It was created by the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/National_Museum_of_Natural_History,_France&quot;&gt;National Museum of Natural History of Paris&lt;/a&gt;
and consisted in panels, hung in the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Jardin_des_plantes&quot;&gt;Jardin des Plantes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some of the pictures can be seen on the 
&lt;a href=&quot;https://marieducom.com/portfolio/exhibition-secrets-devoiles-voir-limperceptible/?lang=fr&quot;&gt;website of the illustrator&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;For PTAS and related acronyms, see &lt;a href=&quot;https://discrete-notes.github.io/october-batch-forgotten&quot;&gt;this post&lt;/a&gt;.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 09 May 2019 00:00:00 -0400</pubDate>
        <link>https://discrete-notes.github.io///natural-history</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///natural-history</guid>
      </item>
    
  </channel>
</rss>
