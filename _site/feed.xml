<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Discrete notes</title>
    <description>Blog</description>
    <link>https://discrete-notes.github.io/</link>
    <atom:link href="https://discrete-notes.github.io//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Network decomposition 1&amp;#58; local algorithms</title>
        <description>&lt;p&gt;This the first real post of a series on distributed network decomposition. 
The introductory post of this series is 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-0&quot;&gt;here&lt;/a&gt;. 
This post is a quick introduction to local algorithms, with the example of the
maximal independent set problem. If you have heard about the local model before, 
you probably know everything in this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-2.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-problem58-computing-a-maximal-independent-set&quot;&gt;A problem: computing a maximal independent set&lt;/h2&gt;
&lt;p&gt;A typical problem of network distributed computing is computing a 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximal_independent_set&quot;&gt;maximal independent set&lt;/a&gt;
(MIS). An MIS is a set $S$ of nodes of the graph such that not two nodes of $S$
are adjacent, and for every node not in $S$, there is neighbor in $S$.&lt;/p&gt;

&lt;p&gt;The two pictures below &lt;em&gt;do not&lt;/em&gt; represent an MIS: the first one 
because of two adjacent selected nodes, and the second because of an “isolated node”.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-arete.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-noeud.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The two following pictures represent MISs. Note that one in &lt;em&gt;maximum&lt;/em&gt; (it has 3 
nodes, and no MIS on 4 nodes exists), but the other is just maximal, and it’s 
also fine.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-maxi.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt; &lt;img src=&quot;assets/MIS-pas-maxi.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;easy-to-solve-in-a-centralized-manner&quot;&gt;Easy to solve in a centralized manner&lt;/h2&gt;

&lt;p&gt;It’s very easy to solve this problem in a centralized manner. 
An algorithm is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Label all nodes as &lt;em&gt;active&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;As long as it is possible:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Take an arbitrary active node&lt;/li&gt;
  &lt;li&gt;Put it in the MIS&lt;/li&gt;
  &lt;li&gt;Label this node and all its neighbors as &lt;em&gt;inactive&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-seq-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-seq-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-seq-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A first problem for us, with this algorithm, is that it is not distributed: you
need an external entity to chose the “arbitrary active node”. This is some kind 
of scheduler, who decides which node is “doing something” at any step.&lt;/p&gt;

&lt;h2 id=&quot;using-identifiers-to-simulate-a-centralized-scheduler&quot;&gt;Using identifiers to simulate a centralized scheduler&lt;/h2&gt;

&lt;p&gt;In our model, we will assume that every node has a unique identity. This 
identifier is a number in $[1,n^2]$, where $n$ is the size of the netork 
(in general we take a large enough 
polynomial for the upper bound, but for concreteness let’s say $n^2$). 
For a node $v$, let $ID(v)$ be its identifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/MIS-ID.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;65%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using these identifier we will simulate the centralized scheduler.&lt;/p&gt;

&lt;p&gt;All nodes start at the same time, and follow time steps (time-step= 1, 2, 3 etc. ).
They start with a status that is &lt;em&gt;active&lt;/em&gt;.
The following algorithm is run at all nodes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If time-step = $ID(v)$ and status = &lt;em&gt;active&lt;/em&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Change status to &lt;em&gt;selected&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Send message “selected” to all neighbors&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;If the time-step $\neq ID(v)$ and status = &lt;em&gt;active&lt;/em&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Wait for a message “selected” from neighbors&lt;/li&gt;
  &lt;li&gt;If one arrives, change status to &lt;em&gt;not selected&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One can check that on our example, with the identifier given above, the run of 
the algorithm simulates the run of the centralized algorithm. Another identifier 
assignment would correspond to another centralized scheduler, and would give 
another MIS.&lt;/p&gt;

&lt;p&gt;Note that the algorithm is correct because the identifiers are all distinct. 
Indeed, if two neighbors had the same identifier, they would be selected at the 
same time, and the outcome would not be an MIS.&lt;/p&gt;

&lt;p&gt;Now to evaluate the performance of a local algorithm, we measure the number of 
time steps before the solution is completed. Here it is $n^2$ in general, as 
one would have to wait for the node with the largest identifier. 
This is a very poor complexity. Indeed we finish this post with a proof that any
problem can be solved in $O(n)$ time steps.&lt;/p&gt;

&lt;h2 id=&quot;general-algorithm-in-on-steps&quot;&gt;General algorithm in $O(n)$ steps&lt;/h2&gt;

&lt;p&gt;Consider the following algorithm (that is described partially by the pictures 
below).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Send identifier to all neighbors&lt;/li&gt;
  &lt;li&gt;Receive the identifier of all neighbors, and build the list of the adjacent 
edges, e.g. $(ID(v),ID(w))$ for a node $v$ receiving a message from a neighbor 
$w$.&lt;/li&gt;
  &lt;li&gt;Send these edges to all neighbors.&lt;/li&gt;
  &lt;li&gt;For $n$ time steps: send the set of all the edges received so far.&lt;/li&gt;
  &lt;li&gt;Then build a local copy of the graph, solve the problem on this copy, and 
output the part of the solution that correspond to the node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following set of pictures shows how the information about the existence of
the egde (2,5) is built and then broadcasted to the whole graph.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/flooding-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt; &lt;/td&gt;
      &lt;td&gt; &lt;img src=&quot;assets/flooding-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/flooding-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This algorithm is correct because after $n$ steps of flooding, all nodes know 
about all the edges, thus the local copy of the graph that each node has is 
correct, and then the output of the algorithm is also correct.&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-1-local-algorithms</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-1-local-algorithms</guid>
      </item>
    
      <item>
        <title>Network decomposition&amp;#58; Introduction</title>
        <description>&lt;p&gt;This post is an introduction (and a table of contents) for a series of posts on
distributed network decomposition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-1.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the important papers of these recent years in distributed graph 
algorithms will appear at STOC this summer:&lt;/p&gt;

&lt;p&gt;“Polylogarithmic-Time Deterministic Network Decomposition and Distributed 
Derandomization” by &lt;a href=&quot;https://n.ethz.ch/~rozhonv/&quot;&gt;Václav Rozhoň&lt;/a&gt; and 
&lt;a href=&quot;https://people.inf.ethz.ch/gmohsen/&quot;&gt;Mohsen Ghaffari&lt;/a&gt; from ETH Zurich.&lt;/p&gt;

&lt;p&gt;It is basically the description of a distributed algorithm that performs what is 
called a &lt;em&gt;network decomposition&lt;/em&gt;, faster than before. This decomposition can 
then be used to do many other things fast, and the paper solves several important 
open problems of the field.&lt;/p&gt;

&lt;p&gt;I quickly looked at the paper when it appeared on arxiv, but I want to 
understand it better, and writing a series of posts about it is good way to do 
this.&lt;/p&gt;

&lt;p&gt;Here is the my current plan for this series:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-1&quot;&gt;Local algorithms&lt;/a&gt;&lt;/em&gt;, 
an introduction to local algorithm, with the example of 
the maximal independent set problem.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Impact for distributed algorithms&lt;/em&gt;, that is, why is a network decomposition so 
useful.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Centralized construction&lt;/em&gt;, in particular, why such decomposition with good 
parameters exist.&lt;/li&gt;
  &lt;li&gt;(and more) TBA&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 08 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-0</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-0</guid>
      </item>
    
      <item>
        <title>Notes from pre-COVID19 times</title>
        <description>&lt;p&gt;Hi there! After abruptly coming back to France because of the virus, I resume
blogging (instead of traveling, basically). Here is a set of notes that I wrote 
before the quarantine and everything.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/grafitti-donut.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;380px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;shapley-value&quot;&gt;Shapley value&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Shapley_value&quot;&gt;Shapley value&lt;/a&gt; 
is a concept in game theory. It basically describes how a 
group of players that have played collaboratively, should share the pay-off of 
the game, taking into account that some players have contributed more than others. 
In other words, it is a way to measure how important is each player for the team. 
For some settings it is proved to be the optimal way to share the gain, with 
respect to some objective of fairness.&lt;/p&gt;

&lt;p&gt;The setting is the following. There are $N$ players, $1,2, …, N$, and a 
function $f$ from the subsets of players to the set of possible pay-offs (eg $R$). 
For a subset $S$ of players, $f(S)$ is the pay-off that the players of $S$ would 
get as a whole if they were to collaborate together.&lt;/p&gt;

&lt;p&gt;Now, assume that all players collaborate, and get some pay-off $P$. How should 
we distribute $P$, using $f$? The idea is the following. First chose an 
arbitrary order on the players, for simplcity, let say the natural order $1, 2, 
…,N$. Then player $1$, gets $f(${$1$}$)$, player $2$ gets $f(${$1,2$}$) - 
f(${$1$}$)$ etc. 
That seems pretty reasonable: every player gets the marginal value it adds
to the pay-off. Now this is not always fair, because of the ordering that we 
chose arbitrarily. Therefore one normalizes this by taking the average over all 
permutations of the individual pay-offs. This is the Shapley value.&lt;/p&gt;

&lt;p&gt;[Thanks to &lt;a href=&quot;https://perso.univ-st-etienne.fr/remila/&quot;&gt;Eric Remila&lt;/a&gt; for telling 
me about this.]&lt;/p&gt;

&lt;h2 id=&quot;stochastic-dominance&quot;&gt;Stochastic dominance&lt;/h2&gt;

&lt;p&gt;Stochastic dominance is a concept of probability that is useful in (algorithmic) 
game theoretical settings, such as 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Secretary_problem&quot;&gt;secretary-type problems&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let $A$ and $B$ be two real random variables. Then $A$ 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_dominance&quot;&gt;stochastically dominates&lt;/a&gt;
$B$ if for every $x$, $P(A\geq x)\geq P(B\geq x)$, and for some x, the 
inequality is strict. In terms of cumulative distribution function, this means 
$F_A(x)\leq F_B(x)$ for all $x$, and with strict inequality for some $x$.&lt;/p&gt;

&lt;p&gt;This notion is useful in the randomized online setting: you want to show that 
your online algorithm obtains, in expectation, at least half the payoff of a 
“prophet” that would play knowing all the game in advance. Then 
it is sometimes useful to show that your algorithm stochastically dominates 
“half the prophet”.&lt;/p&gt;

&lt;p&gt;[I heard about this by working in the group of José Correa in Santiago de Chile.]&lt;/p&gt;

&lt;h2 id=&quot;polygonization-a-problem-not-known-to-be-easy-or-hard&quot;&gt;Polygonization, a problem not known to be easy or hard&lt;/h2&gt;

&lt;p&gt;There are not so many reasonable problems for which we have neither a 
polynomial-time algorithm nor a proof of NP-hardness. Some well-known are 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_isomorphism_problem&quot;&gt;graph isomorphism&lt;/a&gt; and 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_factorization&quot;&gt;integer factorization&lt;/a&gt;.
Wikipedia has a list in its 
&lt;a href=&quot;https://en.wikipedia.org/wiki/NP-intermediate&quot;&gt;NP-intermediate article&lt;/a&gt;.
Here is a counting problem that falls into this category.&lt;/p&gt;

&lt;p&gt;A polygonization of a set of points is a simple polygon that visits all the 
points. In the following picture, the points are black and a polygonization is 
drawn in blue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/polygonization.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The algorithmic problem is: given the point set, count the number of 
polygonizations it has.&lt;/p&gt;

&lt;p&gt;See the two posts by Eppstein on special aspects of this problem, 
&lt;a href=&quot;https://11011110.github.io/blog/2020/01/12/counting-grid-polygonalizations.html&quot;&gt;here&lt;/a&gt; 
and 
&lt;a href=&quot;https://11011110.github.io/blog/2020/01/29/unflippable-polygon.html&quot;&gt;there&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;liquid-rope-coiling-and-discrete-model&quot;&gt;Liquid rope coiling and discrete model&lt;/h2&gt;

&lt;p&gt;When you pour some honey on a pancake, you can see that the honey behaves a bit 
like a rope and can get some rotational movement when touching the pancake. 
Something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/miel.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[See also &lt;a href=&quot;https://www.youtube.com/watch?v=lZbOV8BIOt8&quot;&gt;this video&lt;/a&gt; by Jearl 
Walker.]&lt;/p&gt;

&lt;p&gt;Something strange about this is that there is no reaon a priori for the movement
to be rotational: it’s just something (the honey) falling on something else 
(the pancake). I was curious whether this could appear in a very simple discrete 
model. Here is a naive model, which probably makes little sense, but shows
some rotation, without explicitely refering to a rotation.&lt;/p&gt;

&lt;p&gt;Start with an hexagonal grid representing the pancake (because it’s easier than 
a square grid). The yellow cell is where the honey is currently arriving, and 
the orange circle is the projection of the origin of the honey (e.g. the spoon) 
on the pancake.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/miel1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now there are two forces. One is the “speed” of the yellow patch, that simulates
the fact that the honey tends to go where there is less honey, and the very last 
cell visited has more, so is repulsive. It is represented by the yellow 
arrow. The other one is the attraction of the orange circle, that simulates the 
fact that the honey is still “tied” to the spoon. It is represented by an 
orange arrow. Now the average of the two forces is the red arrow, that is 
pointing to the next cell.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/miel2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The proccess is repeated, and the yellow patch is circulating around the 
cell with a orange circle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/miel3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Well it’s not a very fancy model, in particular it doesn’t make a difference 
between honey on a pancake, and a planet around the sun, but maybe it can be 
improved..!&lt;/p&gt;

&lt;p&gt;Anyway, physicists prefer continuous models, and a paper about the modelization 
of liquid rope coiling is available 
&lt;a href=&quot;https://www.annualreviews.org/doi/10.1146/annurev-fluid-120710-101244&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[I learned about research on liquid rope coiling in 
&lt;a href=&quot;https://www.pourlascience.fr/sd/physique/les-acrobaties-des-filaments-liquides-8027.php&quot;&gt;this article&lt;/a&gt;
in the French popularization magazine “Pour la science”.]&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A latex commands that some of my coauthors didn’t know: 
&lt;code&gt;\scalebox{x}{picture}&lt;/code&gt; allows to scale a picture by a factor x.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After trying various techniques and packages to write comments in latex files, 
I ended up simply using text in color with 
&lt;code&gt;\textcolor{color name}{text}&lt;/code&gt;, but sometimes (eg for the revision of
a journal paper), one needs to color a large patch of text, and 
&lt;code&gt;textcolor&lt;/code&gt; does not allow this. For example you cannot color two 
sections. Then I rediscovered, that one can simply change the text color at any 
point with &lt;code&gt;\color{color name}&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Eppstein has a 
&lt;a href=&quot;https://11011110.github.io/blog/2020/02/22/applications-maximum-matching.html&quot;&gt;blog post&lt;/a&gt; 
about applications of maximum matching algorithms, including kidney exchange, on 
which I have worked recentely. 
(As &lt;a href=&quot;./march-2019-notes-1&quot;&gt;already mentionned&lt;/a&gt; on this blog, 
&lt;a href=&quot;https://cstheory.stackexchange.com/questions/19759/core-algorithms-deployed&quot;&gt;here&lt;/a&gt; 
is a stack exchange thread on “core algorithms deployed”.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 01 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///notes-pre-COVID19</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///notes-pre-COVID19</guid>
      </item>
    
      <item>
        <title>First 2020 notes</title>
        <description>&lt;p&gt;The first set of notes of 2020.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/pinguinos.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
Magellanic penguins.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;conference-model-outcome-of-the-podc-survey-and-remote-attendance&quot;&gt;Conference model: outcome of the PODC survey and remote attendance&lt;/h2&gt;

&lt;p&gt;As &lt;a href=&quot;./mid-november-2019-non-technical&quot;&gt;reported here&lt;/a&gt; in November, the
community of the theory of distributed computing is in the process of
changing some aspects of its conference model. A survey has been proposed
that has received many answers. The email summarizing these answers
is &lt;a href=&quot;https://listserv.acm.org/SCRIPTS/WA-ACMLPX.CGI?A2=PODC;3b2ab6fa.1911&amp;amp;S=&quot;&gt;here&lt;/a&gt;.
It is fairly short, and I don’t have much to say, so I’ll just list 
the topics discussed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;changing to several deadlines a year, to avoid a large gap between
important deadlines,&lt;/li&gt;
  &lt;li&gt;a transition to a system with a journal,&lt;/li&gt;
  &lt;li&gt;collocation of the two main conferences.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A topic mentioned in a previous mail was that this kind of survey is 
more useful than business meetings: in a business meeting, most people are 
afraid to talk, some people are very vocal and do not let others talk, 
and more importantly, it’s late, and everybody wants to leave the room.&lt;/p&gt;

&lt;p&gt;Another related text is by Moshe Vardi in the Communication of the ACM, 
see 
&lt;a href=&quot;https://cacm.acm.org/magazines/2020/1/241717-publish-and-perish/fulltext&quot;&gt;here&lt;/a&gt;.
He basically says that conferences have a big environmental impact, and 
that we should allow people to participate via video. 
The usual answer to this is that you would loose a lot of informal 
interaction between participants. He says he thinks it’s not as bad as 
it looks.&lt;/p&gt;

&lt;h2 id=&quot;selfish-routing-andtraffic-lights&quot;&gt;Selfish routing and traffic lights&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&quot;https://arxiv.org/pdf/1912.06513.pdf&quot;&gt;recent paper&lt;/a&gt; on the arxiv, 
consider the classic model of routing but with traffic lights. 
As discussed recently on this blog (see &lt;a href=&quot;./price-anarchy-flows&quot;&gt;here&lt;/a&gt;)
a classic problem in algorithmic game theory is to evaluate how good is 
the traffic on a network, if you allow each car to chose selfishly the 
best route. 
A surprising phenomenon in this model is that sometimes opening a new 
street can slow down the traffic 
(&lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;Braess’s paradox&lt;/a&gt;). 
The authors show that this does not 
happen when the network is equipped with some traffic lights.&lt;/p&gt;

&lt;h2 id=&quot;sunset-geometry&quot;&gt;Sunset geometry&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Which of the two pictures below looks more like a sunset over a very 
calm lake?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/sunset-geometric.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If the Earth is flat then it’s the one on the left. If it’s spheric, it’s 
the one on the right, and you can even compute the radius of the planet
from the picture!&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://vanderbei.princeton.edu/tex/sunset/ms.pdf&quot;&gt;this&lt;/a&gt; for an 
explanation using trigonometry, and 
&lt;a href=&quot;https://www.shapeoperator.com/2016/12/12/sunset-geometry/&quot;&gt;that&lt;/a&gt; for 
one using geometric algebra.&lt;/p&gt;

&lt;p&gt;[I learned about this on &lt;a href=&quot;https://11011110.github.io/blog/&quot;&gt;Eppstein’s blog&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A bit more of geometry if you read French or Spanish: the former flag 
of Chile has a very nice geometric construction. See 
&lt;a href=&quot;http://images.math.cnrs.fr/Un-drapeau-en-or-perdu-dans-l-histoire.html?lang=fr&quot;&gt;here&lt;/a&gt;
for the article in French, and 
&lt;a href=&quot;http://images.math.cnrs.fr/Una-bandera-aurea-perdida-en-la-historia.html&quot;&gt;here&lt;/a&gt;
in Spanish.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://dblp.uni-trier.de/&quot;&gt;DBLP&lt;/a&gt; now keeps track of the citations of 
the papers. You can access them by clicking on the page symbol close to 
the colored square. Not all the citations are present because they use 
an open access database, and of course, not everything is open.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I used to use &lt;a href=&quot;http://latexdraw.sourceforge.net/index.html&quot;&gt;latexdraw&lt;/a&gt;
to generate pictures that can be included in latex. That is you draw, 
like on a software like Paint, and it generates the code of this picture. 
 Unfortunately the output is only pstricks code, and one would sometimes 
like a tikz code. 
I discovered yesterday &lt;a href=&quot;https://www.mathcha.io/&quot;&gt;mathcha.io&lt;/a&gt; which is an 
online latex editor, and does the same but generates tikz code.
[Thanks to 
&lt;a href=&quot;https://ingenieria.uai.cl/profesor/pedro-montealegre/&quot;&gt;Pedro Montealegre&lt;/a&gt;
for showing me this.]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We mentioned robust statistics 
&lt;a href=&quot;https://discrete-notes.github.io/june-2019-notes&quot;&gt;on this blog&lt;/a&gt; some 
time ago; if you’re interested, there is a survey on the recent advances 
on this topic &lt;a href=&quot;https://arxiv.org/pdf/1911.05911.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 22 Jan 2020 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///first-2020-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///first-2020-notes</guid>
      </item>
    
      <item>
        <title>Smoothed analysis in distributed computing</title>
        <description>&lt;p&gt;Happy 2020! A short post about smoothed analysis in distributed 
computing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/arbre-patagonie.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Smoothed_analysis&quot;&gt;Smoothed analysis&lt;/a&gt; is 
about a complexity measure in between the complexity on random instances 
and the complexity on worst-case instances. 
It basically asks about the complexity on the worst instance,
but with a small random permutation.
The goal of this measure is to better capture the complexity observed in
practice.&lt;/p&gt;

&lt;p&gt;For example, the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Simplex_algorithm&quot;&gt;simplex algorithm&lt;/a&gt;
performs very well in practice
but is proved to be exponential in the worst-case. This means
that worst-case complexity is not the right tool to evaluate this algorithm.
But if you consider a bit of noise, then the simplex algorithm becomes 
polynomial! More precisely the complexity is bounded 
by a polynomial in $n$ and $1/\sigma$, where $\sigma$ is the standard 
deviation of the gaussian noise. This somehow explains why the simplex 
is so good in practice: the examples where it is exponential are very 
special constructions, that are eliminated by small perturbations.&lt;/p&gt;

&lt;p&gt;I was recentely asked whether there exists some smoothed analysis 
in network distributed computing. There is actually a 
&lt;a href=&quot;https://arxiv.org/pdf/1911.02628.pdf&quot;&gt;recent paper&lt;/a&gt; that started that. 
It does a smoothed analysis of distributed minimum 
spanning tree.&lt;/p&gt;

&lt;p&gt;A problem with smoothed analysis, or a feature maybe, is that it can be 
made in different ways, in particular a question is: What kind of noise 
do you consider? When there are 
numerical inputs, you can modify these inputs with a gaussian noise. But 
in the case of minimum spanning tree, small perturbations of the weights
do not change much.
More precisely, given a difficult instance, one 
would have to add a noise of the same order of magnitude as the 
weights to break the lower bound. 
Instead, the authors of the paper above consider a model 
where each node is allowed to ask for a new adjacent random edge at 
each round. These new edges have infinite weight thus they are not 
useful for the MST, only for the communication. 
This seems a rather strange model, but I can imagine that a lot of more
natural variants do not make sense.&lt;/p&gt;

&lt;p&gt;More generally, a problem I can see with smoothed 
analysis for the LOCAL model for example, is that it is based on the 
idea that random instances are easy. 
For graphs, a natural choice of random instances, is 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;&gt;Erdos-Renyi graphs&lt;/a&gt;,
but these are not always easy for distributed algorithms: they are 
sometimes used as lower bound instances (or more precisely they are 
expanders,  and have logarithmic girth, which are two properties that 
often pop up in lower bounds). 
Some graphs that are somehow easy are grids, but I don’t know what kind of 
random transformation you can apply to a graph to make it more grid-like.&lt;/p&gt;

&lt;p&gt;One could also play with the identifiers. For example a random identifier 
assignment often boils down to a randomized algorithms
(see &lt;a href=&quot;https://arxiv.org/abs/1704.05739&quot;&gt;this&lt;/a&gt;), and maybe some slightly 
random assignement could make sense.&lt;/p&gt;
</description>
        <pubDate>Mon, 13 Jan 2020 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///smoothed-analysis</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///smoothed-analysis</guid>
      </item>
    
      <item>
        <title>A cute &quot;open&quot; problem on polygons</title>
        <description>&lt;p&gt;Here is a cute problem that appeared on a chat channel of University of 
Chile a few months ago. To my knowledge this is still open, but as it 
seems rather natural, the answer is probably known somewhere.&lt;/p&gt;

&lt;p&gt;Consider the following transformation: take a polygon in the plane and 
then dilate it along the $x$-axis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/polygon-expansion.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In other words, the $x$ coordinates of the vertices
are multiplied by some expansion factor $\alpha$.
On the picture the expansion factor is $\alpha=2$.&lt;/p&gt;

&lt;p&gt;Now the question is: is it always true that the original polygon fits in 
the dilated polygon? Note that you are allowed to rotate and translate 
these as you like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/polygon-inclusion.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We have thought a bit about this at some point, and we have a proof for 
triangles, but not much more…&lt;/p&gt;

</description>
        <pubDate>Wed, 11 Dec 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///polygon problem</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///polygon problem</guid>
      </item>
    
      <item>
        <title>Price of anarchy for dynamic flows</title>
        <description>&lt;p&gt;This post is about flows from a game theory perspective. 
It originates from a recent talk of 
&lt;a href=&quot;https://sites.google.com/view/timoosterwijk/home&quot;&gt;Tim Oosterwijk&lt;/a&gt; at 
the University of Chile about 
&lt;a href=&quot;https://drive.google.com/file/d/1u-NUQLppaTDdNUUh-BvsXYJ2QH9RM1K9/view&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-model-for-dynamic-flows&quot;&gt;A model for dynamic flows&lt;/h2&gt;

&lt;p&gt;We use a model useful to study settings like urban traffic.
The flow is dynamic (that is we do not focus on 
some stationary state) and there 
can be congestion. It is called the &lt;em&gt;fluid queueing 
model&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The network is a graph, and there is one source node where the flow 
enters and one target node where the flow exits. Each edge of the 
network has a delay and a capacity per time unit. 
If more flows enters the edge than the capacity, a (FIFO) queue will 
form inside this edge.&lt;/p&gt;

&lt;p&gt;Maybe a good example is a network where on every edge there is a 
toll. If there is no queue, going through a toll $t$ takes some $s_t$ seconds,
and at most some $c_t$ cars can go through the toll at each second. 
If too many cars arrive, then a queue is forming, that will disappear 
later if not so many cars arrive.&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective&lt;/h2&gt;

&lt;p&gt;We consider a setting where a constant flow $u_0$ 
enters the network at each unit of time.&lt;/p&gt;

&lt;p&gt;Now at each intersection of degree $d$ a particule of flow can go in 
either of the $d-1$ directions. Given the choice of each particule at 
each intersection, the flow is completely defined, and you can measure 
how fast it is. For example routing every particule through the same edge
of small capacity would in general form a huge queue in this edge and 
make the flow very slow.&lt;/p&gt;

&lt;p&gt;We can consider at least two notions of efficiency: (1) for a given 
time, how much flow exits the network, and (2) for a given amount of flow 
to start with, when does the last particule exits the network. 
We we will consider the second one, called the &lt;em&gt;makespan&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;price-of-anarchy&quot;&gt;Price of anarchy&lt;/h2&gt;

&lt;p&gt;We study the makespan of different strategies for routing the flow.
As often in 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithmic_game_theory&quot;&gt;algorithmic game theory&lt;/a&gt;,
one is interested in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Price_of_anarchy&quot;&gt;price of anarchy&lt;/a&gt;.
On a given instance this price is the ratio of the makespan of the best 
routing strategy divided by the makespan of the strategy where every 
particule optimizes its own travel time. 
In other words, the ratio between the strategy where an oracle decides 
optimally a route for every car, and the strategy where every driver 
optimizes its own travel time. Note that the setting where we let the 
particules (or drivers) decide is a kind of game, and we look at the 
equilibrium of this game. This might be called the &lt;em&gt;selfish&lt;/em&gt; solution.&lt;/p&gt;

&lt;p&gt;The price of anarchy of the problem is the largest 
(the supremum to be precise) price of anarchy among every instance of 
the problem.&lt;/p&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;Tim and his co-authors show (among other results) an upper bound of 
$e/(e-1)$ on the price of anarchy for this problem, under some conditions.&lt;/p&gt;

&lt;h2 id=&quot;a-key-open-problem-the-monotonicity-conjecture&quot;&gt;A key open problem: the monotonicity conjecture&lt;/h2&gt;

&lt;p&gt;There is a very neat and puzzling conjecture that, if true, would imply 
that the upper bound above always hold.&lt;/p&gt;

&lt;p&gt;The so-called &lt;em&gt;monotonicity conjecture&lt;/em&gt; states that: if one reduces the 
flow that enters in the network by unit of time (but keeping the total 
amount to push in the network),  then the makespan of the selfish solution 
increases.&lt;/p&gt;

&lt;p&gt;This seems very natural, but it is still open, and sometimes unexpected
things happen in the such games (like in 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;Braess’s paradox&lt;/a&gt;)&lt;/p&gt;

</description>
        <pubDate>Thu, 05 Dec 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///price-anarchy-flows</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///price-anarchy-flows</guid>
      </item>
    
      <item>
        <title>Bayesian mechanism design and Yao's principle</title>
        <description>&lt;p&gt;I attended a talk yesterday about 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithmic_game_theory&quot;&gt;algorithmic game theory&lt;/a&gt; by 
&lt;a href=&quot;https://www.or.tum.de/en/group/alexandrostsigonias/&quot;&gt;Alexandros Tsigonias-Dimitriadis&lt;/a&gt;
as part of Santiago’s &lt;a href=&quot;http://www.dii.uchile.cl/acgo/seminar-acgo/&quot;&gt;AGCO seminar&lt;/a&gt;.
Here are a few elements from this talk.&lt;/p&gt;

&lt;h2 id=&quot;basic-bayesian-auction&quot;&gt;Basic Bayesian auction&lt;/h2&gt;

&lt;p&gt;The very basic setting we are looking at is the following: a client (the bidder)
comes to a seller (the auctioneer) with the goal of buying a particular item. 
The auctioneer has to set a price for the item. 
The bidder knows the maximum price at which she will buy the object (the value 
of the item for her).
If the auctioneer’s price is higher than the bidder’s value, the bidder does not 
buy the item, and if the price is lower then the bidder buys it, but the 
auctioneer “loses” the difference.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian-optimal_mechanism&quot;&gt;Bayesian setting&lt;/a&gt;, 
the bidder’s value is taken following a probability 
distribution, and the auctioneer knows this distribution. 
Then, the expected revenue of the auctioneer is the price she announces, 
multiplied by the probability that this price is lower than the value taken by 
the bidder. In other words, if $F$ is the cumulative probability distribution 
of the value of the bidder, 
then the expected revenue for a price $p$ is $p \cdot (1-F(p))$.
Then the auctioneer chooses a price $p$ that maximizes this quantity.&lt;/p&gt;

&lt;h2 id=&quot;generalizations&quot;&gt;Generalizations&lt;/h2&gt;

&lt;p&gt;One can generalize this to work with several bidders. This is called 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian-optimal_mechanism#The_Myerson_mechanism&quot;&gt;Myerson mechanism&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The paper of Alexandros and his co-authors 
(&lt;a href=&quot;https://arxiv.org/abs/1907.04220&quot;&gt;Robust Revenue Maximization Under Minimal Statistical Information&lt;/a&gt;)
goes into another direction and explores the setting where the 
auctioneer does not know the full distribution of the bidder, but 
only the mean and an upper bound on the standard deviation. (That’s why it 
is called &lt;em&gt;robust&lt;/em&gt; revenue maximization.) Later they extend it to one 
bidder buying (or not) several items.&lt;/p&gt;

&lt;p&gt;They show matching upper and lower bounds, and their lower bound
is based on an argument similar to Yao’s minimax principle.&lt;/p&gt;

&lt;h2 id=&quot;minimax-principle&quot;&gt;Minimax principle&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Yao%27s_principle&quot;&gt;Yao’s minimax principle&lt;/a&gt; 
is a general theorem for randomized algorithms. 
Basically it is saying that the best randomized algorithm on its worst instance 
will get the same performance as the best deterministic algorithm on the worst 
distribution of instances. (This needs a precise statement, to say which thing is 
optimized first etc.).&lt;/p&gt;

&lt;p&gt;In the context of Alexandros, the instances are distributions (of which we know
only the mean and an upper bound on the standard deviation) thus a 
distribution of instances is a distribution of distributions. 
This can be made precise, by considering a mixture of distributions.&lt;/p&gt;

</description>
        <pubDate>Thu, 21 Nov 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///bayesian-auctions</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///bayesian-auctions</guid>
      </item>
    
      <item>
        <title>More November notes</title>
        <description>&lt;p&gt;Some more notes for November 2019.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/nueva-constitucion.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;network-creation-game&quot;&gt;Network creation game&lt;/h2&gt;

&lt;p&gt;Network generation models are mechanisms to create networks. 
In a classic setting the nodes arrive one after the other and are linked 
to nodes already in 
the network following some rules. 
In another setting, called &lt;em&gt;network creation game&lt;/em&gt; the nodes are players, 
and they play a game in which they can choose to pay to be linked to 
other nodes. 
The outcome of the game is a network. 
The cost that a player pays is $\alpha$ for every node it decides to be 
linked to, plus 
the sum of the distances from this node to all the other nodes. 
In other words, a node wants to have short distance to every node, but 
cannot add a link to every node, because it would be too expensive.&lt;/p&gt;

&lt;p&gt;For this game one can study the usual objects of 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithmic_game_theory&quot;&gt;algorithmic game theory&lt;/a&gt;:
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Nash_equilibrium&quot;&gt;Nash equilibrium&lt;/a&gt; 
and the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Price_of_anarchy&quot;&gt;price of anarchy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is conjectured that the price of anarchy is constant for 
any value $\alpha$, and &lt;a href=&quot;https://arxiv.org/abs/1909.09799&quot;&gt;this recent preprint&lt;/a&gt; 
makes progress on the conjecture.&lt;/p&gt;

&lt;h2 id=&quot;lempel-ziv-compression-algorithms&quot;&gt;Lempel-Ziv compression algorithms&lt;/h2&gt;

&lt;p&gt;Lempel-Ziv algorithm is a classic compression algorithm (or more 
precisely a classic family of algorithms, are there are several versions). 
A &lt;a href=&quot;https://semidoc.github.io/lagarde-catastrophe&quot;&gt;blog post on Semidoc&lt;/a&gt; 
describes the algorithm and gives an overview of 
&lt;a href=&quot;https://arxiv.org/abs/1707.04312&quot;&gt;this paper&lt;/a&gt; which studies how the compression
rate can change when the original text is changed by one bit.&lt;/p&gt;

&lt;p&gt;Two recent papers on arxiv deal with Lempel-Ziv:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1910.00941.pdf&quot;&gt;The first one&lt;/a&gt; gives a new analysis of 
the fact that Lempel-Ziv is optimal for some models of random text (hidden 
Markov sources)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.10347&quot;&gt;The second one&lt;/a&gt; improves the complexity of 
the algorithm decompressing the text.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-armed-bandit&quot;&gt;Multi-armed bandit&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Multi-armed bandit&lt;/em&gt; is an expression that appears here and there in 
TCS conference, and very often in theoretical machine learning. It is a type of 
problem where one has to make decisions one after the other, to 
maximize some pay-off. Basically, at each round, it has the choice 
between several options called the “arms” of the bandit (like the levers 
of different slot-machines).&lt;/p&gt;

&lt;p&gt;A basic version is the following framework:&lt;/p&gt;

&lt;p&gt;Given: $k$  arms, $T$ rounds.&lt;/p&gt;

&lt;p&gt;In each round $t\in[T]$:
1. Algorithm picks arm $a_t$.
2. Algorithm observes reward $r_t\in [0,1]$ for the chosen arm.&lt;/p&gt;

&lt;p&gt;Pay-off: the sum of the rewards&lt;/p&gt;

&lt;p&gt;The reward for an arm comes from an unknown distribution, but if the 
algorithm chooses an arm repeatedly, it somehow learns this distribution. 
There is already a lot to say on this simple case, and there are a flurry
of papers about this, these days.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.07272.pdf&quot;&gt;Here&lt;/a&gt; is a recent introduction 
to multi-armed bandit. Also if you are in Rennes, France, on Wednesday
there is a 
&lt;a href=&quot;https://perso.crans.org/besson/phd/defense/&quot;&gt;PhD defense on this topic&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;delaunay-triangulations-have-perfect-matchings&quot;&gt;Delaunay triangulations have perfect matchings&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Delaunay_triangulation&quot;&gt;Delaunay triangulations&lt;/a&gt; 
are triangulations of point sets in the plane. I recentely learnt that
the graphs that are Delaunay triangulations, always have a perfect 
matching (that is a matching of size $n/2$ if $n$ is even, and $(n-1)/2$
is $n$ is odd).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/delaunay.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
A point set, its Delaunay triangulation, and the associated graph with a perfect matching.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;A short proof of this appeared on arxiv recently, 
&lt;a href=&quot;https://arxiv.org/pdf/1907.01617.pdf&quot;&gt;here&lt;/a&gt;. (Actually it is a stronger
result that is proved, about the so-called “toughness” of Delaunay 
triangulations.)&lt;/p&gt;

&lt;h2 id=&quot;learning-augmented-algorithms&quot;&gt;Learning-augmented algorithms&lt;/h2&gt;

&lt;p&gt;Learning-augmented algorithms are algorithms that can use informtation
coming from some machine learning source. 
Here is an example.&lt;/p&gt;

&lt;p&gt;Binary search takes $O(\log n)$ in the worst-case. 
Now if you have some neural network (NN) telling you that the element you’re 
looking for is around position $i$, how do you modify your search? 
Well you can begin by testing position $i$. Then, if the NN is not perfect, 
this might not be the right value, but maybe it’s close. Say the value 
you’re looking for is larger. Then you can try to find a position 
that has larger value than your element, for example by doing exponential guesses. 
Once you have both upper and lower bound, you can run the usual binary 
search.&lt;/p&gt;

&lt;p&gt;If the error (that is, the number of positions between your element
and the prediction of the NN) is $\mu$, then your algorithm runs in 
$O(\log \mu)$. This is good: if the prediction is good then you speed up 
the search, and if it’s bad, then you do not loose much.&lt;/p&gt;

&lt;p&gt;In more general terms, one looks for two properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;consistency: the better the prediction, the better the algorithm&lt;/li&gt;
  &lt;li&gt;robustness: if the predition is bad, then the algorithm does not get 
much worse.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that for real application, one might also be interested in the running 
time of the NN, and a lot of other things.&lt;/p&gt;

&lt;p&gt;Some material on this topic:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a &lt;a href=&quot;https://www.mit.edu/~andoni/algoS19/scribes/scribe24.pdf&quot;&gt;lecture note&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theory.stanford.edu/~sergei/slides/HALG-slides.pdf&quot;&gt;the slides&lt;/a&gt; 
of a talk at &lt;a href=&quot;http://2019.highlightsofalgorithms.org/&quot;&gt;HALG 2019&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mit.edu/~vakilian/ttic-workshop.html&quot;&gt;a workshop&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 18 Nov 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///more-november-2019</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///more-november-2019</guid>
      </item>
    
      <item>
        <title>Mid-November (non-technical) notes</title>
        <description>&lt;p&gt;Some notes about a new conference, a new research organization etc.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/comida-india.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;conference-model-survey&quot;&gt;Conference model survey&lt;/h2&gt;

&lt;p&gt;An online survey has been created &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfOmbqTTfQfUYEXADCLLqat-OAl7XUh8gFceg27uDfpr_NaaQ/viewform&quot;&gt;here&lt;/a&gt; to gather the opinions of the distributed 
computing community about its model of conferences. Topics include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;changing from one deadline to three deadlines a year. Basically it’s like 
having the reviewing process made three times (with a smaller number of papers), 
and then having a conference with the papers accepted to any of the three phases.&lt;/li&gt;
  &lt;li&gt;collocating the two main conferences of the domain.&lt;/li&gt;
  &lt;li&gt;changing the format (more keynotes, or shorter talks etc.)&lt;/li&gt;
  &lt;li&gt;transitioning to a journal model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details can be found in the survey.&lt;/p&gt;

&lt;h2 id=&quot;paper-presentation-videos&quot;&gt;Paper presentation videos&lt;/h2&gt;

&lt;p&gt;One of the side topics of the survey is the possibility for the 
authors to upload a video presenting their papers. It’s something I have thought
of doing for my papers, but finally didn’t try.&lt;/p&gt;

&lt;p&gt;Till Miltzow is a researcher in graph theory and he records a talk for every 
paper he has, see &lt;a href=&quot;https://sites.google.com/view/miltzow/publications&quot;&gt;here&lt;/a&gt;. 
I think it’s good!&lt;/p&gt;

&lt;h2 id=&quot;the-polytcs-project&quot;&gt;The PolyTCS project&lt;/h2&gt;

&lt;p&gt;You may know the &lt;a href=&quot;https://en.wikipedia.org/wiki/Polymath_Project&quot;&gt;Polymath Project&lt;/a&gt;,
which is a project from the math community. It consists in choosing an open problem, 
and then working on this problem in a massively collaborative way: everything is
public, and everyone can help. This project has been quite successful, with great
collaborations, and great papers.&lt;/p&gt;

&lt;p&gt;A similar project has been launched in TCS: 
&lt;a href=&quot;https://polytcs.wordpress.com/&quot;&gt;The PolyTCS project&lt;/a&gt;.
The first problem is about boolean functions, see 
&lt;a href=&quot;https://polytcs.wordpress.com/2019/11/01/the-entropy-influence-conjecture/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;forc-conference&quot;&gt;FORC conference&lt;/h2&gt;

&lt;p&gt;A new conference in TCS: &lt;a href=&quot;https://responsiblecomputing.org/forc-2020-call-for-paper/&quot;&gt;Symposium on the Foundations of Responsible Computing (FORC)&lt;/a&gt;. Topics include privacy, fairness and electoral processes.&lt;/p&gt;

&lt;h2 id=&quot;symbolic-computations-in-python&quot;&gt;Symbolic computations in python&lt;/h2&gt;

&lt;p&gt;I recently had to do a lot of small computations on toy examples. To check the 
computations, I wanted to have some symbolic math software, but the usual
ones (e.g. Maple) are big machines, and often not open-source. If you are in the 
same situation, python with &lt;a href=&quot;https://www.sympy.org/en/index.html&quot;&gt;sympy&lt;/a&gt; is a 
good choice.&lt;/p&gt;

&lt;h2 id=&quot;pull-requests-for-this-blog&quot;&gt;Pull requests for this blog&lt;/h2&gt;

&lt;p&gt;This blog is a github page, that is, it works as a git repository. 
I recently got some pull requests for it, from my friend 
&lt;a href=&quot;https://perso.crans.org/besson/me/index.fr.html&quot;&gt;Lilian Besson&lt;/a&gt;. 
Pull requests is a way to suggest changes, e.g. correct a typo. It is very 
convenient (on my side): I just have to pull these commits. 
If you want to know more and correct one of the many typos of this blog, you can 
take a look at &lt;a href=&quot;https://www.freecodecamp.org/news/how-to-make-your-first-pull-request-on-github/&quot;&gt;this page&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 12 Nov 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///mid-november-2019-non-technical</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///mid-november-2019-non-technical</guid>
      </item>
    
  </channel>
</rss>
