<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Discrete notes</title>
    <description>Blog</description>
    <link>https://discrete-notes.github.io/</link>
    <atom:link href="https://discrete-notes.github.io//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>STOC and SOCG picks</title>
        <description>&lt;p&gt;A few weeks ago the accepted papers lists of 
&lt;a href=&quot;http://acm-stoc.org/stoc2019/&quot;&gt;STOC&lt;/a&gt; 
and 
&lt;a href=&quot;http://eecs.oregonstate.edu/socg19/&quot;&gt;SOCG&lt;/a&gt; 
were made public. 
A bunch of titles caught my attention, and here are a few bits of information on
some papers I could find online.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/saucisses.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;350px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;testing-graphs-in-vertex-distribution-free-models&quot;&gt;Testing graphs in vertex-distribution-free models&lt;/h2&gt;

&lt;p&gt;Graph &lt;a href=&quot;https://en.wikipedia.org/wiki/Property_testing&quot;&gt;property testing&lt;/a&gt; 
basically consists in deciding if a graph has a property 
or not, by looking only at some parts of it. 
More precisely one queries a few nodes, and ask for their neighbors for example, 
and then outputs whether the graph has the property or is far from having 
it. Note that such a statement can only be true with some probability.
An introduction to graph property testing is 
&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~/oded/COL/tgp-intro.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the classic model, one is allowed to query a random node, with “random” 
meaning “uniformly at random”. In a new 
&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~/oded/VO/vdf.pdf&quot;&gt;paper&lt;/a&gt;
the author considers the case where the random access is not uniform but depends 
on an arbitrary distribution. This in turn implies a change in the definition of 
being far from a property.&lt;/p&gt;

&lt;p&gt;I didn’t dive in the paper, but I imagine the following scenario. 
Some nodes are more “important” than others. This translates into two things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It’s not a very big deal if you don’t detect that something is wrong around
an unimportant node.&lt;/li&gt;
  &lt;li&gt;Your random queries have more chance to visit an important node than an 
unimportant node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, you have a dynamic graph, you may not know about newly arrived 
nodes, but these are less important, so it’s no big deal if you don’t query them
now.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~/oded/VO/vdf.pdf&quot;&gt;Testing Graphs in Vertex-Distribution-Free Models&lt;/a&gt;&lt;/em&gt;,
by &lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~oded/&quot;&gt;Oded Goldreich&lt;/a&gt;, 
and will appear at STOC 2019.
It was listed in 
&lt;a href=&quot;https://ptreview.sublinear.info/?p=1044&quot;&gt;October 2018 property testing review&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;transportation-problem&quot;&gt;Transportation problem&lt;/h2&gt;

&lt;p&gt;The transportation problem is the following. 
Given a graph, where each node $v$ is given a (positive or negative) supply 
$\mu(v)$, such that $\sum_v \mu(v)=0$, one has to find a flow to transport the 
supplies in order to reach the configuration where every node has supply 0. 
See the picture below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/transportation.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is very similar to the 
&lt;a href=&quot;Transportation theory (mathematics)&quot;&gt;optimal transport problem&lt;/a&gt; in non-discrete 
mathematics. The &lt;a href=&quot;https://arxiv.org/pdf/1902.08384.pdf&quot;&gt;paper&lt;/a&gt; presents an 
approximation algorithm for fixed dimensions via continuous optimization.&lt;/p&gt;

&lt;p&gt;I don’t know how much continuous optimization is used, but it reminds me of an 
excellent invited talk by 
&lt;a href=&quot;https://people.csail.mit.edu/madry/&quot;&gt;Aleksander Mądry&lt;/a&gt; at 
&lt;a href=&quot;http://2016.highlightsofalgorithms.org/&quot;&gt;HALG 2016&lt;/a&gt;, that convinced me that 
continuous optimization can be a good approach to solve combinatorial problems.
There is no video of the talk, but 
&lt;a href=&quot;https://www.youtube.com/watch?v=noRNcDbqtVY&quot;&gt;this one&lt;/a&gt;
seems pretty close.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1902.08384.pdf&quot;&gt;Preconditioning for the Geometric Transportation Problem&lt;/a&gt;&lt;/em&gt;
by &lt;a href=&quot;http://www.math.toronto.edu/khesin/&quot;&gt;Boris A. Khesin&lt;/a&gt;,
&lt;a href=&quot;http://www.cs.toronto.edu/~anikolov/&quot;&gt;Aleksandar Nikolov&lt;/a&gt;,
and Dmitry Paramonov, and will appear at SOCG.]&lt;/p&gt;

&lt;h2 id=&quot;lp-roundings-iterated-meets-randomized&quot;&gt; LP roundings: iterated meets randomized&lt;/h2&gt;

&lt;p&gt;A classic approach in combinatorial optimization is to express the problem as a 
linear program. 
Unfortunately, only the fractional case is known to be solvable 
efficiently, and only the integer solutions can be transfered back to solution 
for the original problem. 
A solution is to compute a fractional solution and then to round it to an integer 
solution. This approach is very powerful for approximation algorithms.&lt;/p&gt;

&lt;p&gt;There are two important types of rounding: randomized rounding and iterated 
rounding. 
In &lt;a href=&quot;https://en.wikipedia.org/wiki/Randomized_rounding&quot;&gt;randomized rounding&lt;/a&gt;, 
a fractional variable $x_i$ is simply rounded to 1 
with probability $x_i$. 
In iterated rounding, one iteratively modify the fractional solution until it 
gets to an integral solutions (for example adding some small amount to a set of 
variables, until they reach 1, etc.). 
Both roundings have their pros and cons.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.01597.pdf&quot;&gt;The paper&lt;/a&gt; presents a method that 
combines the two approaches into one common framework, and shows how to use it.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.01597.pdf&quot;&gt;On a generalization of iterated and randomizedrounding&lt;/a&gt;&lt;/em&gt;
by &lt;a href=&quot;https://www.win.tue.nl/~nikhil/&quot;&gt;Nikhil Bansal&lt;/a&gt;, and will appear at STOC.]&lt;/p&gt;

&lt;h2 id=&quot;queue-layout&quot;&gt; Queue layout&lt;/h2&gt;

&lt;p&gt;Given a graph, a $k$-queue layout is an ordering of the vertices and a partition 
of the
edges into $k$ sets, such that there are not two edges of the same partition, 
that are nested. 
Below is an example of a graph and a 2-queue layout of it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/queue-layout.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Queue_number&quot;&gt;queue number&lt;/a&gt; of a graph is 
the minimum $k$ such that there exists a $k$ layout of the graph. 
An important open question is whether planar graph have bounded queue number. 
&lt;a href=&quot;https://arxiv.org/pdf/1811.00816.pdf&quot;&gt;The paper&lt;/a&gt; makes a step towards a positive
answer, by proving that planar graphs with bounded degree have bounded queue 
number.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.00816.pdf&quot;&gt;Planar Graphs of Bounded Degree have Constant Queue Number&lt;/a&gt;&lt;/em&gt;
by 
&lt;a href=&quot;http://algo.inf.uni-tuebingen.de/?site=mitarbeiter/michaelbekos/index&quot;&gt;Michael A. Bekos&lt;/a&gt;, 
&lt;a href=&quot;http://www-pr.informatik.uni-tuebingen.de/?site=mitarbeiter/henryfoerster/index&quot;&gt;Henry Förster&lt;/a&gt;,
&lt;a href=&quot;https://informatik.uni-koeln.de/ls-juenger/people/gronemann/&quot;&gt;Martin Gronemann&lt;/a&gt;, 
&lt;a href=&quot;https://i11www.iti.kit.edu/en/members/tamara_mchedlidze/index&quot;&gt;Tamara Mchedlidze&lt;/a&gt;,
&lt;a href=&quot;http://mozart.diei.unipg.it/montecchiani/&quot;&gt;Fabrizio Montecchiani&lt;/a&gt;, 
Chrysanthi Raftopoulou, and
&lt;a href=&quot;https://i11www.iti.kit.edu/en/members/torsten_ueckerdt/index&quot;&gt;Torsten Ueckerdt&lt;/a&gt;.
It will appear at STOC (even though the topic is very SOCG-friendly).
I didn’t know about queue layouts, and it’s good to discover it, as it is yet 
another example of graph parameter that correspond to a pattern, in the sense of 
&lt;a href=&quot;https://arxiv.org/abs/1812.05913&quot;&gt;this paper&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;reachability-of-petri-nets&quot;&gt;Reachability of Petri nets&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Petri_net&quot;&gt;Petri nets&lt;/a&gt; are very common in 
theoretical computer science, for 
example in &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_checking&quot;&gt;model checking&lt;/a&gt;. 
Nevertheless, it is horribly hard to decide the reachability problem for this 
model: can you get from a configuration $A$ to a configuration $B$?
The decidability of the problem was proved only in the 80s, and the best 
upper bound is non-primitive recursive cubic-Ackermannian (don’t ask what it 
means, exponential space is already scaring me).&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/1809.07115.pdf&quot;&gt;paper&lt;/a&gt; proves that the problem is 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Nonelementary_problem&quot;&gt;non-elemetary&lt;/a&gt;, that is, 
its time complexity cannot be bounded by a power tower.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1809.07115.pdf&quot;&gt;The Reachability Problem for Petri Nets is Not Elementary&lt;/a&gt;&lt;/em&gt;
by &lt;a href=&quot;https://www.mimuw.edu.pl/~wczerwin/&quot;&gt;Wojciech Czerwiński&lt;/a&gt;,
&lt;a href=&quot;https://mimuw.edu.pl/~sl/&quot;&gt;Sławomir Lasota&lt;/a&gt;,
&lt;a href=&quot;https://warwick.ac.uk/fac/sci/dcs/people/ranko_lazic&quot;&gt;Ranko Lazić&lt;/a&gt;,
&lt;a href=&quot;https://www.labri.fr/perso/leroux/&quot;&gt;Jérôme Leroux&lt;/a&gt; and 
&lt;a href=&quot;https://www.labri.fr/perso/fmazowiecki/&quot;&gt;Filip Mazowiecki&lt;/a&gt;; and will appear at STOC.
In France, &lt;a href=&quot;http://www.lsv.fr/~schmitz/index.html.en&quot;&gt;Sylvain Schmitz&lt;/a&gt; is one 
of the researcher working on this kind of stratospherical complexities. 
&lt;a href=&quot;http://www.lsv.fr/~halfon/&quot;&gt;Simon Halfon&lt;/a&gt; one his PhD students at the time, 
gave an excellent talk on this topic at the 
&lt;a href=&quot;https://www.irif.fr/en/seminaires/doctorants/index&quot;&gt;IRIF PhD seminar&lt;/a&gt; in 2017.]&lt;/p&gt;

&lt;h2 id=&quot;quartet-distance-and-4-cycles&quot;&gt;Quartet distance and 4-cycles&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Quartet_distance&quot;&gt;quartet distance&lt;/a&gt; is a 
distance between phylogenetic trees. More precisely it is a distance between two 
unrooted trees with labeled leaves. It basically takes all the tuples of four 
leaves and count how many of them are in different topology in the two trees.&lt;/p&gt;

&lt;p&gt;For example in the picture below, for the two trees on the left, we want to 
decide whether the leaves $a$, $b$, $c$ and $d$ are in the same topology or not. 
We simplify the tree until we have only these leaves, and we see on the pictures
on the right&lt;br /&gt;
that it is not the case. Thus this quartet will add one to the distance. 
(Note that on the left, all leaves should be labeled, but I indicate only the 
ones we are interested in).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/quartet.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/1811.06244.pdf&quot;&gt;paper&lt;/a&gt; shows that computing the 
problem of computing 
distance between two trees is equivalent to the problem of computing the number 
of 4-cycles in a graph, up to 
polylogarithmic factors. This implies better algorithms and better insights
on the complexity.&lt;/p&gt;

&lt;p&gt;[The paper is
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.06244.pdf&quot;&gt;Computing Quartet Distance is Equivalent to Counting 4-Cycles&lt;/a&gt;&lt;/em&gt;
 by Bartłomiej Dudek (whom I have had the chance to meet in EPFL), 
and &lt;a href=&quot;https://sites.google.com/a/cs.uni.wroc.pl/gawry/&quot;&gt;Paweł Gawrychowski&lt;/a&gt;, and 
it
will appear at STOC.]&lt;/p&gt;

</description>
        <pubDate>Wed, 13 Mar 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///stoc-socg-picks</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///stoc-socg-picks</guid>
      </item>
    
      <item>
        <title>February notes</title>
        <description>&lt;p&gt;Notes for February 2019.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/lierre.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;minorities-in-network&quot;&gt;Minorities in network&lt;/h2&gt;

&lt;p&gt;I attended a talk about minorities in network,
by &lt;a href=&quot;http://claudiawagner.info/&quot;&gt;Claudia Wagner&lt;/a&gt; at the 
&lt;a href=&quot;http://www.complexnetworks.fr/events/&quot;&gt;Complex network seminar&lt;/a&gt; of Sorbonnes 
University. 
There was a lot of content, here are a few things I noted.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Contrary to what I thought, a &lt;a href=&quot;https://en.wikipedia.org/wiki/K-core&quot;&gt;$k$-core&lt;/a&gt; 
is not another name for a $k$-clique. 
A $k$-core of a graph is a maximal connected subgraph where all the nodes have 
degree at least $k$. It can have much more nodes than just $k$
(a $k$-regular graph is its own $k$-core). Such subgraphs can be considered as 
coherent communities in social networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A part of the talk was about minorities in wikipedia. 
One would like to consider statements such as “women are more 
linked to other women than to men”. 
This is not true in general because there are more men pages, thus 
links have more chance to point to men, but could still be true, proportionally.
But it’s a bit too rough to just look at proportions because the graph may be 
complicated and there might be many correlations going on.
One way to deal with this is following: consider the graph of wikipedia 
bibliographies, note the global gender proportions, then erase the gender of the 
nodes, and reassign them at random, keeping the right proportions. 
Now you can compare the neighbourhoods in this new graph and in the original 
graph, and try to understand what’s going on. One paper on the topic is the following: 
&lt;a href=&quot;https://arxiv.org/pdf/1501.06307.pdf&quot;&gt;It’s a Man’s Wikipedia? Assessing Gender Inequality in an Online Encyclopedia&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A notion known as &lt;em&gt;Burt efficiency&lt;/em&gt;, or &lt;em&gt;brokerage&lt;/em&gt;, or 
&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_holes&quot;&gt;structural hole&lt;/a&gt;&lt;/em&gt;, is more or 
less the following. 
A node that belongs to (or is close to) two clusters in a 
network, can have an advantage over the other nodes,
because it can enjoy the information gathered by both communities, and 
can choose to transfer or not such information. 
One can define coefficients to measure if a node is or not in such a 
position.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-stage-optimization&quot;&gt;Multi-stage optimization&lt;/h2&gt;

&lt;p&gt;For dynamic algorithms, one is usually concerned with having a good solution
 at any time, but these solutions do not need to be related.
Multi-stage optimization, introduced in 
&lt;a href=&quot;https://arxiv.org/abs/1404.3768&quot;&gt;this paper&lt;/a&gt;, considers the cases where one 
should not change the solution too much between the two steps. 
In other words, in this framework one maximizes the quality of the solution, 
while minimizing the churn.&lt;/p&gt;

&lt;p&gt;[I stumble on the notion in &lt;a href=&quot;https://arxiv.org/abs/1901.11260&quot;&gt;this preprint&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;conjecture&quot;&gt;1-2-3 Conjecture&lt;/h2&gt;

&lt;p&gt;Another edition of the Complex Network seminar by 
&lt;a href=&quot;http://www.labri.fr/index.php?n=Annuaires.Profile&amp;amp;id=Senhaji_ID1441185629&quot;&gt;Mohammed Senhaji&lt;/a&gt; 
(that I couldn’t attend) was about the 1-2-3 conjecture, which is the following.&lt;/p&gt;

&lt;p&gt;In any graph, one can label the edges with label 1, 2, or 3, such that, when each node 
computes the sum of the labels of its adjacent labels, not two neighbours have 
the same sum.&lt;/p&gt;

&lt;p&gt;A survey about the conjecture is &lt;a href=&quot;https://arxiv.org/pdf/1211.5122.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;lovszs-new-book&quot;&gt;Lovász’s new book&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://fr.wikipedia.org/wiki/L%C3%A1szl%C3%B3_Lov%C3%A1sz&quot;&gt;László Lovász&lt;/a&gt; 
wrote a new book: 
&lt;a href=&quot;http://web.cs.elte.hu/~lovasz/bookxx/geombook2019-01-20.pdf&quot;&gt;Graphs and geometry&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;vandermonde-identity&quot;&gt;Vandermonde identity&lt;/h2&gt;

&lt;p&gt;Vandermonde’s identity is the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{m+n}{r} = \sum_{k=0}^{r}\binom{m}{k} \binom{n}{r-k}.&lt;/script&gt;

&lt;p&gt;I thought it was only a bachelor exercise, until it naturally popped up in the 
calculation in &lt;a href=&quot;https://arxiv.org/pdf/1812.09120.pdf&quot;&gt;a recent paper&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 05 Mar 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///february-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///february-2019-notes</guid>
      </item>
    
      <item>
        <title>Simulation argument IV. Maximal matching</title>
        <description>&lt;p&gt;This is the fourth and last post of a series about the simulation argument, that 
started with &lt;a href=&quot;./simulation-1&quot;&gt;this post&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/puzzle-4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this post we tackle the maximal matching problem. 
Or actually we will show 
why establishing a lower bound for this problem is not as easy as for the 
sinkless orientation problem. 
The real lower bound is in &lt;a href=&quot;https://arxiv.org/abs/1901.02441&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;encoding&quot;&gt;Encoding&lt;/h3&gt;

&lt;p&gt;A maximal matching is a set of edges of the graph, such that not two edges are 
adjacent and not two unmatched node are linked by an edge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A natural encoding for this problem would be to label the edges of the matching 
with a first label, and edges not in the matching with second label. 
But this does not work for our setting. 
Instead, we will do the following. 
(As in the previous post we will deal with 2-colored trees.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The edges of the matching are labeled with the label $M$.&lt;/li&gt;
  &lt;li&gt;If a white node is not matched then its edges are labeled with $P$. 
These are pointers to matched black nodes.&lt;/li&gt;
  &lt;li&gt;The remaining edges are labeled with $O$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the following picture, red edges are for $M$, green edges are for $P$, and 
blue edges are for $O$ (the missing edge should be blue!).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The languages we use are then described by the following polynomials:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$L_W = MO^{\Delta-1}+P^{\Delta}$,&lt;/li&gt;
  &lt;li&gt;$L_B = M(P+O)^{\Delta-1} + O^{\Delta}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformation&quot;&gt;Transformation&lt;/h2&gt;

&lt;h3 id=&quot;simulation&quot;&gt;Simulation&lt;/h3&gt;

&lt;p&gt;We perform the simulation on a black node $u$, and get a polynomial $P_u$.&lt;br /&gt;
By definition $P_u\subseteq (M+O+P)^{\Delta}$, and is factorized.&lt;/p&gt;

&lt;h3 id=&quot;product-property&quot;&gt;Product property&lt;/h3&gt;

&lt;p&gt;As the product rule applies, we know that
$P_u \subseteq M(P+0)^{\Delta-1}+O^{\Delta}$.&lt;br /&gt;
Thus there is at most one factor of $P_u$ with an $M$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Claim 1&lt;/em&gt;: This factor is either $(M)$ or $(M+O)$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: Suppose the factor with $M$ has a $P$. 
Then it means that when we develop $P_u$,
there are two monomials $M\times K$ and $P\times K$, with $K$ a polynomial
of degree $\Delta-1$, that does not contain an $M$. 
But the monomial $P\times K$ is not included in 
$M(P+0)^{\Delta-1}+O^{\Delta}$. which contradicts the product property.&lt;/p&gt;

&lt;p&gt;Then we are left with 5 possible sums as factors in $P_u$: 
$(M), (O), (P), (M+O), (P+0)$.&lt;/p&gt;

&lt;p&gt;Now it seems that we cannot get much more out of our properties, so let’s try a 
simplification step.&lt;/p&gt;

&lt;h3 id=&quot;tentative-simplification-step&quot;&gt;(Tentative) Simplification step&lt;/h3&gt;

&lt;p&gt;If we can map $(M+O)$ and $P+0$ to a simple label, 
and still match the language, 
then we are done, and we can conclude like in the &lt;a href=&quot;./simulation-3&quot;&gt;previous post&lt;/a&gt;. 
But this is not going to happen.&lt;/p&gt;

&lt;p&gt;Suppose you are in following situation, that is easy because there is not even 
a $P+O$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, one thinks: let’s just take one of the two edges labeled with $M+O$
(let say the one with the smallest port-number on the white node), 
label it with $M$, label the other one with $O$, and we are done.&lt;/p&gt;

&lt;p&gt;The problem is the one we highlighted in the &lt;a href=&quot;./simulation-2&quot;&gt;second post&lt;/a&gt;: 
the edges are not uniformly set-labeled by all the nodes. 
Maybe the other black node that is dealing with these edges, labels it 
with only $O$.
And then you have to synchronize, and this is forbidden (it takes extra time).&lt;/p&gt;

&lt;h2 id=&quot;no-lower-bound-this-way&quot;&gt;No lower bound this way&lt;/h2&gt;

&lt;p&gt;So at the end, we cannot conclude like in the case of sinkless orientation.&lt;/p&gt;

&lt;p&gt;Comments on that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This make sense: actually there is a $O(\Delta)$ 
algorithm for this problem, thus no $\Omega(\log n)$ lower bound!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The above alone does not prove that there is no $\Omega(\log n)$ lower 
bound: we could use another definition of the problem, or we could have tried 
more exotic label replacement ($M$ tranformed into $P$, or whatever).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you want to prove a $\Omega(\Delta)$ lower bound then you need to be 
smarter. In particular you need that, after the transformation, the language is 
different. Basically there should be a parameter that starts with something like 
$\Delta$ and decreases at each transformation. 
For that, see &lt;a href=&quot;https://arxiv.org/abs/1901.02441&quot;&gt;the paper&lt;/a&gt;!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 15 Feb 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///simulation-4</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///simulation-4</guid>
      </item>
    
      <item>
        <title>Simulation argument III. Sinkless orientation</title>
        <description>&lt;p&gt;This is the third post of a series that starts &lt;a href=&quot;./simulation-1&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/puzzle-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this post, we show how to use simulation argument to prove a lower bound on 
the &lt;em&gt;sinkless orientation&lt;/em&gt; problem. 
This is done in the context described in the &lt;a href=&quot;./simulation-2&quot;&gt;previous post&lt;/a&gt;:
2-colored regular trees.&lt;/p&gt;

&lt;h2 id=&quot;sinkless-orientation-encoding&quot;&gt;Sinkless orientation encoding&lt;/h2&gt;

&lt;p&gt;The sinkless orientation problem consists in orienting the edges of the graph, 
such that no node is a sink, that is, no node has only edges pointing to it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/sinkless.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can be easily encoded in bipartite graphs: an edge is labeled $b$ if it is 
pointing to the black endpoint, and $w$ is it is pointing to the white endpoint.&lt;/p&gt;

&lt;p&gt;Now the languages, described as polynomials, are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$L_W=b(b+w)^{\Delta-1}$,&lt;/li&gt;
  &lt;li&gt;$L_B=w(b+w)^{\Delta-1}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformation&quot;&gt;Transformation&lt;/h2&gt;

&lt;p&gt;We will show a very strong property: from a $T$-round algorithm for sinkless 
orientation, we can get a $(T-1)$-round algorithm for… sinkless orientation!&lt;/p&gt;

&lt;h3 id=&quot;simulation&quot;&gt;Simulation&lt;/h3&gt;
&lt;p&gt;After one step of simulation centered at a black node $u$, the labeling of the 
edges adjacent to $u$ is described by a factorized polynomial $P_u$. 
By definition $P_u$ is included in $(w+b)^{\Delta}$.&lt;/p&gt;

&lt;h3 id=&quot;product-and-common-label-property&quot;&gt;Product and common label property&lt;/h3&gt;
&lt;p&gt;We now use the key properties to have a better upper bound on $P_u$.
Thanks to the product property, we know that $P_u\subseteq w(b+w)^{\Delta-1}$. 
And thanks to the common label property, we know that $P_v$ cannot be 
$b^{\Delta}$.
And well, that’s all we need!&lt;/p&gt;

&lt;h3 id=&quot;simplification-step&quot;&gt;Simplification step&lt;/h3&gt;
&lt;p&gt;Consider the simplification: $b+w \rightarrow b$.&lt;/p&gt;

&lt;p&gt;If we apply this simplification, we have all the edges labeled with unique 
labels (not set labels), and this labeling is correct with respect to the black 
language. In other words, after simplification, we have a polynomial $\hat{P_u}$, 
that is a monomial, and $\hat{P_u}\subseteq L_B$.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
But what about the white nodes?
We know from the previous section that $P_v$ was not $b^{\Delta}$, thus 
$\hat{P_v}$ is not $b^{\Delta}$ either, thus $\hat{P_v}\subseteq L_W$.&lt;/p&gt;

&lt;p&gt;Thus we have a new labeling, for the exact same language, in one less round!&lt;/p&gt;

&lt;h2 id=&quot;lower-bound&quot;&gt;Lower bound&lt;/h2&gt;

&lt;p&gt;The same reasoning would work for the white nodes as well. 
This means that from any $T$-round algorithm for sinkless orientation, we get 
down to a 0-round algorithm for sinkless orientation. 
And sinkless orientation is not a trivial language.&lt;/p&gt;

&lt;p&gt;So is sinkless orientation impossible to solve?&lt;/p&gt;

&lt;p&gt;No, remember that our lower bound technique works only up to $O(\log n)$ rounds.
Thus we have just proved an $\Omega(\log n)$ lower bound for this problem.&lt;/p&gt;

&lt;p&gt;Actually there is a $O(\log n)$ algorithm for this language, hence this bound is 
tight.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;We always have the property that the simplification step does not destroy the correctness of the labeling on the black node, as long as we do an “hereditary” simplification, that is we replace a set of labels by a label of the set. This is because of the product property.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 13 Feb 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///simulation-3</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///simulation-3</guid>
      </item>
    
      <item>
        <title>Simulation argument II. Edge-labelings on 2-colored trees, and polynomials</title>
        <description>&lt;p&gt;This is the second post of a series that starts 
&lt;a href=&quot;https://discrete-notes.github.io/simulation-1&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/puzzle-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In the previous post, we saw an overview of the simulation technique. 
In this post, we are going to see a more precise and usable approach, for the case edge 
labelings on 2-colored regular trees.&lt;/p&gt;

&lt;p&gt;The explanation differs a bit from the 
&lt;a href=&quot;https://arxiv.org/abs/1901.02441&quot;&gt;original paper&lt;/a&gt;, 
because we talk about polynomials, whereas they talk about regular expressions. 
This is mainly a vocabulary change, because there is not 
much algebra going on (but it is easier for me to see it this way).&lt;/p&gt;

&lt;h2 id=&quot;arestricted-setting&quot;&gt;A restricted setting&lt;/h2&gt;
&lt;p&gt;Let us describe the special case we are interested in, and why it is relevant.&lt;/p&gt;

&lt;h3 id=&quot;delta-regular-trees&quot;&gt;$\Delta$-regular trees&lt;/h3&gt;

&lt;p&gt;We consider that we are in the middle of a $\Delta$-regular tree. Here “middle”
means that we cannot see the leaves. 
As a consequence, the method works only for the regime below time 
$\Theta(\log n)$, because time $\Omega(\log n)$ you can always see a leaf in a 
$\Delta$-regular tree.
If we can prove a lower bound for this area of the tree, then we
have a lower bound for our problem.&lt;/p&gt;

&lt;h3 id=&quot;coloring&quot;&gt;2-coloring&lt;/h3&gt;
&lt;p&gt;We will consider graphs that are 2-colored (trees are bipartite), and we assume 
that every node knows on which side of the partition it is (black or white). So 
we work on this kind of graph (with omitted port-numbers):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/fig-delta-regulier.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This, in addition to port-numbers, is a powerful way to break symmetry between 
adjacent nodes.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;white algorithm&lt;/em&gt; for a problem $\Pi$ in $k$ rounds is an algorithm such that, 
after $k$ rounds the white nodes label their edges in a correct way, with 
respect to $\Pi$. 
In such an algorithm, the 
black nodes do not label the edges, they “trust” the white nodes. 
A &lt;em&gt;black algorithm&lt;/em&gt; is defined the same way, but for black nodes.&lt;/p&gt;

&lt;h2 id=&quot;encoding-of-edge-problems&quot;&gt;Encoding of edge problems&lt;/h2&gt;

&lt;h3 id=&quot;using-the-2-coloring&quot;&gt;Using the 2-coloring&lt;/h3&gt;
&lt;p&gt;We consider edge problems defined the following way. 
The multiset of edges adjacent to any vertex should follow some rule, that
depends on the color of the node. 
For example: there are three labels $a,b,c$, the white nodes should either 
have 2 edges labeled with $a$, and the rest with $c$, or 3 edges labeled with 
$a$ and the rest with $b$; and black nodes should have at least one label $a$, 
one label $b$ and one label $c$. (I just made up this example, it’s probably 
silly.)&lt;/p&gt;

&lt;p&gt;Most problems we are usually interested in are defined in general graphs, not in 
bipartite graphs, thus the problem does not refer to a coloring. 
Hence one expects the constraints on white nodes and black nodes to be the same, 
unlike in the example above. 
Actually, it is often useful to make them different (we’ll see that in the two 
next posts).&lt;/p&gt;

&lt;h3 id=&quot;polynomial-point-of-view&quot;&gt;Polynomial point of view&lt;/h3&gt;

&lt;p&gt;We now introduce the polynomial point of view. 
Consider a node $u$ in a graph where the edges are labeled.
We can define the product of the labels of $u$ as a polynomial $M_u$, whose 
variables are the labels of the problem. 
Note that $M_u$ is a monomial: it’s just a product of labels.&lt;/p&gt;

&lt;p&gt;One can now express the toy problem above in terms of two polynomials:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$L_W(a,b,c)=a^2c^{\Delta-2}+a^3b^{\Delta-3}$&lt;/li&gt;
  &lt;li&gt;$L_B(a,b,c)=abc(a+b+c)^{\Delta-3}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We denote $P \subseteq P’$ the fact that the monomials of P are all 
present in $P’$. Then the labeling of $u$ is correct for problem $\Pi$, if and only if, 
$M_u \subseteq L_{C(u)}$, where $C(u)$ is the color of $u$.&lt;/p&gt;

&lt;p&gt;Note that, in our example, all the polynomials we play with have three variables, 
because there are three labels, and that they are homogeneous of degree 
$\Delta$, because every node has $\Delta$ adjacent edges.&lt;/p&gt;

&lt;h2 id=&quot;simulation-step-and-independence&quot;&gt;Simulation step and independence&lt;/h2&gt;

&lt;h3 id=&quot;polynomial-point-of-view-1&quot;&gt;Polynomial point of view&lt;/h3&gt;
&lt;p&gt;In the previous post, we said that the simulation step is basically about having 
a view at distance $T-1$, imagining everything that could appear in the nodes 
at distance $T$, and labeling edges with all the labels that could be correct 
in one of these extensions.&lt;/p&gt;

&lt;p&gt;Now let us restate this in terms of polynomials. 
After the simulation, every edge is labeled with a set of labels, that can be 
transfered to a sum of labels for polynomials. 
Then the polynomial associated with a set labeling at a node $u$ could be for 
example $P_u=(a+b)^3(a+b+c)^4c^{\Delta-7}$ (this polynomial would not be 
possible for our toy language as we will see later).&lt;/p&gt;

&lt;p&gt;Note that the polynomial we get for a set labeling is not a monomial in general, 
but it is factorized.&lt;/p&gt;

&lt;h3 id=&quot;simulation-for-2-colored-trees&quot;&gt;Simulation for 2-colored trees&lt;/h3&gt;
&lt;p&gt;For 2-colored trees we can be more specific in the description of the simulation 
(and actually we will change the simulation outline a bit too).&lt;/p&gt;

&lt;p&gt;We have a white algorithm in time $T$, and we want a black algorithm 
in time $T-1$ (this will be our setting until the end of the post). 
Consider a black node $u$.
The black node $u$ will first imagine all the extensions of its $(T-1)$-view 
into a $(T+1)$-view. 
(Note that we simulate at distance $T+1$ and not just $T$, as stated in the 
previous post.) 
The black node will then simulate the run of its white neighbors with the 
algorithm in time $T$, and gather the set of labels possible for each edge.&lt;/p&gt;

&lt;p&gt;Note that we have fixed the topology to $\Delta$-regular trees, so the only 
thing to imagine for the extension is the port-number assignment.&lt;/p&gt;

&lt;h3 id=&quot;independence&quot;&gt;Independence&lt;/h3&gt;
&lt;p&gt;The key point here is the following: the $T$-view of a white 
neighbour $v$ of $u$ consists of: 
(1) the $(T-1)$-view of $u$, and 
(2) the extension of this view &lt;em&gt;in the direction of $v$&lt;/em&gt;. 
In the following picture, this means that this view includes only the 
black part and the red part.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/fig-simulation-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a consequence when simulating node $v$, the black node does not need to 
imagine something for the other parts of its imaginary $(T+1)$-view (e.g. the blue 
parts on the figure). 
In other words the labeling of the edge $(u,v)$ comes only from the different 
version of the red part, and is independent of what happens in the blue parts. 
Obviously this independence property is true for every white neighbor of $u$, 
with its “own” extension.&lt;/p&gt;

&lt;h2 id=&quot;two-key-properties&quot;&gt;Two key properties&lt;/h2&gt;

&lt;p&gt;There are two key properties about the set labelings given by the simulation.&lt;/p&gt;

&lt;h3 id=&quot;product-property&quot;&gt;Product property&lt;/h3&gt;
&lt;p&gt;This property is about the neighborhood of the black nodes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Claim:&lt;/em&gt;
Let $S_1, S_2, …, S_{\Delta}$ be the sets of labels on the edges after the 
simulation step (with an arbitrary order of the edges). Then any tuple of the form
$(s_1, s_2,…, s_{\Delta})$, with $s_i\in S_i$, has to be in the language (for 
the black side).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Suppose it is not the case, and let $(s_1, s_2,…, s_{\Delta})$ be a 
tuple of $S_1, S_2, …, S_{\Delta}$ that is not in the language. 
Then we build an extension such that the $T$-round 
algorithm would label the edges with $(s_1, s_2,…, s_{\Delta})$, which is a 
contradiction with the correctness of the algorithm.
Start with $s_1$. 
If $s_1$ is in $S_1$ it means that there is an extension, in 
the direction of the first edge (in the figure, suppose $(u,v)$ is the first 
edge, then we are talking about the red part), such that the first edge is 
labeled with $s_1$, and we take this extension. 
But, because of the independence property stated above, we can continue with 
$s_2$, and then $s_3$ etc., which leads to the desired structure.&lt;/p&gt;

&lt;p&gt;We can restate this in terms of polynomials.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Product property:&lt;/em&gt;
$P_u \subseteq L_B$.&lt;/p&gt;

&lt;p&gt;This means that every monomial of $P_u$ has to be a monomial of $L_B$, which is
just a reformulation of the claim above.&lt;/p&gt;

&lt;h3 id=&quot;common-label-property&quot;&gt;Common label property&lt;/h3&gt;

&lt;p&gt;And now a property for the neighborhoods of the white nodes. 
This will not give us a lot of information, because we don’t know how the other
black nodes around the node label the edges around this white node. 
But we still have the following:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Claim:&lt;/em&gt;
Take a white node $v$.
All the black nodes around $v$ agree on at least one label for each edge. 
That is, for each edge there is a label that is in the set of labels of the 
simulation of every black node.&lt;/p&gt;

&lt;p&gt;This label is simply the one that the white nodes would give with its $T$-round 
algorithm on the real graph (not a simulated one).&lt;/p&gt;

&lt;p&gt;This property is weaker than the first one, but is sometimes useful.&lt;/p&gt;

&lt;h2 id=&quot;typical-transformation-step&quot;&gt;Typical transformation step&lt;/h2&gt;

&lt;p&gt;The typical way to prove that one has a transformation from an algorithm in $T$ 
rounds for a problem $\Pi$, to an algorithm in $T-1$ rounds for a problem $\Pi’$,
is the following.&lt;/p&gt;

&lt;p&gt;First make the simulation. 
In our example, the black node $u$ has a polynomial included in 
$(a+b+c)^{\Delta}$. 
But this is a very rough over-approximation, and $P_u$ is probably much smaller. 
Then one can use the two key properties to rule out many labelings and have a 
more precise idea of what $P_u$ is. 
Once this is done, we can go for a simplification step: replace sets of labels by labels. 
If everything works fine you have a labeling that is correct for your problem 
$P’$.&lt;/p&gt;

&lt;h2 id=&quot;subtleties&quot;&gt;Subtleties&lt;/h2&gt;
&lt;p&gt;There are two subtleties that blocked me at some point.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, in this 2-colored framework, it is not enough to have a general way to 
go from a white T-round algorithm to a black (T-1)-round algorithm, you also 
need to go from a black algorithm to a white algorithm. In some cases, the two 
are very different.&lt;/li&gt;
  &lt;li&gt;Second, one should be careful when talking about the edges that are not 
adjacent to the node we consider. 
For example, in the simulation, the node $u$ computes what 
its white neighbor $v$ would put on an edge $(v,z)$. 
But the edge $(v,z)$ is probably labeled in different 
manner by $z$, and this has to be taken into account in the simplification step.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Indeed &lt;a href=&quot;https://users.ics.aalto.fi/suomela/mm-lb/&quot;&gt;previous lower bound techniques&lt;/a&gt; cannot work in the 2-colored model.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 12 Feb 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///simulation-2</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///simulation-2</guid>
      </item>
    
      <item>
        <title>Simulation argument I. General technique</title>
        <description>&lt;p&gt;As
&lt;a href=&quot;https://discrete-notes.github.io/january-2019-notes&quot;&gt;said earlier&lt;/a&gt; 
on this blog, a 
&lt;a href=&quot;https://arxiv.org/abs/1901.02441&quot;&gt;recent preprint&lt;/a&gt; 
proves a set of long-expected lower bounds for distributed graph algorithms. 
This post is the first of a (short) series of (short) posts about the basics of 
this paper.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/puzzle-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;a-series-of-posts&quot;&gt;A series of posts&lt;/h2&gt;

&lt;p&gt;In this first post we give an overview of the &lt;em&gt;simulation argument&lt;/em&gt;, which is 
the framework used for the lower bound.&lt;/p&gt;

&lt;p&gt;The next posts will be about:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;specific construction for edge labelings on bicolored trees&lt;/li&gt;
  &lt;li&gt;Example 1: sinkless orientation&lt;/li&gt;
  &lt;li&gt;Example 2: maximal matching&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the content of this series is probably better explained in the 
preprint cited above, so you may want to look at it instead of these posts! 
The main difference will appear in the second post where I propose a slightly 
different vocabulary, based on polynomials. The posts assume familiarity with 
the local model.&lt;/p&gt;

&lt;h3 id=&quot;a-few-bits-of-context&quot;&gt;A few bits of context&lt;/h3&gt;

&lt;p&gt;The simulation technique can be traced back to Linial, 
and his $\Omega(\log^*n)$ lower bound for coloring.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
However, it did a come-back with 
&lt;a href=&quot;https://arxiv.org/pdf/1511.00900.pdf&quot;&gt;a 2015 paper&lt;/a&gt; that had a different point 
of view. 
It was then simplified and improved.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 
The technique was an important part of 
&lt;a href=&quot;http://adga.hiit.fi/2017/hirvonen.pdf&quot;&gt;the talk of Juho Hirvonen&lt;/a&gt; 
at the 
&lt;a href=&quot;http://adga.hiit.fi/2017/&quot;&gt;2017 ADGA workshop&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-the-simulation-argument&quot;&gt;Overview of the simulation argument&lt;/h2&gt;

&lt;p&gt;The core question of the simulation argument is the following. 
Suppose that in time $T$ you can solve a problem. Then what can you solve 
if you have only $T-1$ rounds? 
For lower bounds, the basic line of reasoning is the following.&lt;/p&gt;

&lt;h3 id=&quot;base-step-0-rounds&quot;&gt;Base step: 0 rounds&lt;/h3&gt;
&lt;p&gt;Fix a problem $P$.
Suppose you have a way to do the following transformation.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Start with:&lt;/em&gt;&lt;br /&gt;
an algorithm $A$ in $T$ rounds for a problem $P$. &lt;br /&gt;
&lt;em&gt;Change it into:&lt;/em&gt; 
an algorithm $A’$ in $T-1$ rounds for another problem $P’$.&lt;/p&gt;

&lt;p&gt;Then, say you want to prove that solving $P$ requires more than one
round. 
Then you only need to prove that you have a transformation as above, with $T=1$, 
and that $P’$ is not trivial (that is, to prove that 
$P’$ cannot be solved in zero round).
Indeed if there is an algorithm in one round for $P$, then you can transform it 
into an algorithm in 0 round for $P’$, which would contradict the 
non-triviality of $P’$. 
At this point, you know that $P$ requires at least two rounds. 
This may sound silly as we have just moved the difficulty from one problem to
another: for this technique to work, we need to be able to prove that $P’$ is 
non-trivial. But this is usually much simpler.&lt;/p&gt;

&lt;h3 id=&quot;induction&quot;&gt;Induction&lt;/h3&gt;
&lt;p&gt;Lower bounds of one rounds are not super exciting, and one would like to prove
bounds of $k+1$ rounds, for some $k$. 
The argument goes the following way.
For the sake of contradiction, suppose there is an algorithm in $k$ rounds for 
your problem $P$.
Now you prove that there exists a family of problems $P_0, P_1, P_2, …,P_k$, with 
$P_0=P$, such that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;none of them is trivial&lt;/li&gt;
  &lt;li&gt;given an algorithm for $P_i$ in $k-i$ rounds, you can transform it into an 
algorithm in $k-i-1$ rounds for $P_{i+1}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The second bullet implies that $P_k$ can be solved in $0$ rounds, which impossible 
because the first bullet states that it is non-trivial. 
Thus $P$ needs at least $k+1$ rounds.&lt;/p&gt;

&lt;h3 id=&quot;simulation-argument&quot;&gt;Simulation argument&lt;/h3&gt;
&lt;p&gt;The question now: how do you define a transformation from one problem to another? &lt;br /&gt;
Suppose you know an algorithm in $T$ rounds, but you have only a view of 
$T-1$ rounds. Then you can do the following.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Imagine all the possible ways your $(T-1)$-neighborhood could be extended to a 
$T$-neighborhood.&lt;/li&gt;
  &lt;li&gt;Compute a solution for each of these extensions, using your $T$-round algorithm.&lt;/li&gt;
  &lt;li&gt;Label your node/edges with the set of all the labels that the T-round algorithm
would use in at least one of these extensions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obviously you are not solving the original problem because you are labeling your
node/edges with sets of labels, instead of labeling them with only one label. 
Also this labeling may be uninteresting: every node/edge could be labeled with 
all the possible labels. 
But maybe it &lt;em&gt;is&lt;/em&gt; interesting. 
And then you may be able to define a non-trivial problem $P’$ 
(probably a quite artificial problem but it’s ok) such that this set-labeling 
is a proper labeling for $P’$.&lt;/p&gt;

&lt;h3 id=&quot;simplification-step&quot;&gt;Simplification step&lt;/h3&gt;
&lt;p&gt;You may also want to have a simplification step. 
This is a step that you perform after you have labeled your nodes/edges with 
sets of labels.
The goal is to simplify the proof, by replacing the sets of labels by something 
simpler, typically simple labels. For example you decide that every set of labels 
{$a,b$} is replaced by $a$. For this step to be useful you need that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;one can compute the simplification without further communication (e.g. no 
synchronization with neighbours)&lt;/li&gt;
  &lt;li&gt;the new labels fit into a language that has good properties, in particular, it 
is not trivial.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;The simulation argument appears more clearly in the &lt;a href=&quot;https://users.ics.aalto.fi/suomela/doc/linial-easy.pdf&quot;&gt;modern version of the proof&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;There is an soon coming note by &lt;a href=&quot;https://disco.ethz.ch/alumni/brandts&quot;&gt;Sebastian Brandt&lt;/a&gt; that formalizes precisely the approach.&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 11 Feb 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///simulation-1</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///simulation-1</guid>
      </item>
    
      <item>
        <title>January notes</title>
        <description>&lt;p&gt;A few notes for January 2019.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/dino.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;http://www.jardindesplantesdeparis.fr/en/activities-events/galleries-gardens-zoo-libraries/illuminated-species-2931&quot;&gt;Illuminated species&lt;/a&gt; in Jardin des Plantes in Paris.&lt;/center&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;maximal-matching-lower-bound&quot;&gt;Maximal matching lower bound&lt;/h2&gt;

&lt;p&gt;They did it! A key problem of distributed computing on graphs has 
been solved by a team supervised by
&lt;a href=&quot;https://users.ics.aalto.fi/suomela/&quot;&gt;Jukka Suomela&lt;/a&gt;. 
&lt;a href=&quot;https://arxiv.org/abs/1901.02441&quot;&gt;The preprint&lt;/a&gt; has been 
uploaded to arxiv recently, and is not reviewed yet, so this should be taken 
with a grain of salt, but given the authors list, and what I have read of the 
paper, there is little doubt it is correct.&lt;/p&gt;

&lt;p&gt;The setting can be explained the following way. There are jobs available,
and people looking for jobs. Every job has at most $\Delta$ candidates, 
and each person has at most $\Delta$ jobs (s)he is interested in. 
There is no central entity, so the decisions of which job is matched
to which person have to be taken based on communication 
between the “jobs” and the people. 
At the end, we want that (1) every person
either has been assigned a job or all the jobs (s)he was interested in have been 
assigned to someone else, and (2) every job is either assigned to someone, or all 
the candidates for this job have been assigned another job. 
Note that there is no preference list, and that
this problem would be very easy if we had a central entity managing the job 
market.&lt;/p&gt;

&lt;p&gt;A strategy to solve the problem is the following: first every person chooses a 
job among the ones it has selected, and sends a proposal to this job, second the 
“jobs” choose one of the persons that have sent a proposal to it (if any). 
After this first round, some people and jobs are already matched, and they leave 
the game. 
The other jobs and persons continue, until we have reached a situation that 
fulfils the conditions (1) and (2) above.&lt;/p&gt;

&lt;p&gt;This strategy requires $O(\Delta)$ phases. The new result tells us
that this is basically optimal.&lt;/p&gt;

&lt;h2 id=&quot;search-vs-decision&quot;&gt;Search vs decision&lt;/h2&gt;

&lt;p&gt;My PhD was about distributed decision, and a sentence that often appears in 
papers on this topic is some variation of “unlike in the centralized setting,
search and decision are very different in the distributed setting”. This makes
me a bit uncomfortable because I don’t have a very strong statement to support 
the claim that in centralized computing search and decision are similar.&lt;/p&gt;

&lt;p&gt;Lance Fortnow 
&lt;a href=&quot;https://blog.computationalcomplexity.org/2019/01/search-versus-decision.html&quot;&gt;blogged about this&lt;/a&gt; 
early this month.&lt;/p&gt;

&lt;h2 id=&quot;fragile-complexity&quot;&gt;Fragile complexity&lt;/h2&gt;

&lt;p&gt;An interesting recent line of research is to go “beyond worst-case complexity”, 
that is to consider other measures of complexity/efficiency than the time on the
worst input, as the later can be inadequate for many purposes.
There are very nice concepts there, such as the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Smoothed_analysis&quot;&gt;smoothed complexity&lt;/a&gt;. 
See also the &lt;a href=&quot;http://timroughgarden.org/f14/l/top10.pdf&quot;&gt;top 10 ideas&lt;/a&gt; in this 
area, by &lt;a href=&quot;http://timroughgarden.org/&quot;&gt;Tim Roughgarden&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A new measure was introduced in 
&lt;a href=&quot;https://export.arxiv.org/abs/1901.02857&quot;&gt;an arxiv preprint&lt;/a&gt; this month: fragile 
complexity. I just read the abstract, and I don’t know why it is called ‘fragile’, 
but I understand the definition for sorting: the fragile complexity of a sorting
algorithm is the maximal number of comparisons that an element of the list can 
be part of.&lt;/p&gt;

&lt;h2 id=&quot;lipics-without-logos&quot;&gt;LIPIcs without logos&lt;/h2&gt;

&lt;p&gt;I like LIPIcs latex class, which is now used for many conferences (basically the 
ones that switched to open access). 
I would like to use 
it for my preprints (because it looks nice, and because if the conference is in 
lipics, it avoids extra-work). One problem is that preprints should not have the
official lipics logos, and other feature that are relevant only for conference 
publications. 
Following the example of a colleague, I used to use a modified version of the 
class file. This is not useful any more: the class now provides the command 
\hideLIPIcs that hides the conference-only features. (See the
&lt;a href=&quot;http://drops.dagstuhl.de/styles/lipics-v2019/lipics-v2019-authors/lipics-v2019-authors-guidelines.pdf&quot;&gt;authors guidelines&lt;/a&gt;, 
page 8.)&lt;/p&gt;

&lt;h2 id=&quot;pac-learning-and-deep-learning&quot;&gt;PAC learning and deep learning&lt;/h2&gt;

&lt;p&gt;It seems that deep learning is becoming popular among almost everybody (at least 
from a scientific point of view), except theoretical computer scientists, who 
criticize it for its lack of proven guarantees. As a reaction, 
there are several efforts to better understand ML from a theoretical point of 
view (for example in &lt;a href=&quot;http://people.csail.mit.edu/madry/lab/&quot;&gt;Alexander Mądry’s group&lt;/a&gt;
and in the &lt;a href=&quot;http://mltheory.cs.princeton.edu/&quot;&gt;ML theory group at Princeton&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The month in the Theory Dish blog, Amit Daniely and Roy Frostig 
&lt;a href=&quot;https://theorydish.blog/2019/01/04/on-pac-analysis-and-deep-neural-networks/&quot;&gt;blog about&lt;/a&gt;
the performances of deep learning on well-defined theoretical tasks in the 
context of &lt;a href=&quot;https://en.wikipedia.org/wiki/Probably_approximately_correct_learning&quot;&gt;PAC learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Incidentally, Leslie Valiant published a &lt;a href=&quot;http://www.probablyapproximatelycorrect.com/&quot;&gt;book&lt;/a&gt;
about PAC learning (that he defined in the eighties), and its relation with 
nature and evolution.&lt;/p&gt;

&lt;h2 id=&quot;wind-turbines-machine-learning-and-algorithms&quot;&gt;Wind turbines, machine learning and algorithms&lt;/h2&gt;

&lt;p&gt;Lance Fortnow also &lt;a href=&quot;https://blog.computationalcomplexity.org/2019/01/machine-learning-and-wind-turbines.html&quot;&gt;blogged about wind turbines and ML&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Basically, a problem for wind turbines is to predict sudden changes of the wind, 
that could cause damage. One could use complex fluid simulations, but these are 
slow therefore there is an increasing trend in using ML for these predictions.&lt;/p&gt;

&lt;p&gt;So there is a way to make things in a “deterministic” well-defined way, but one 
prefers the ML version that is quicker. 
This reminds me of a 
&lt;a href=&quot;https://cstheory.stackexchange.com/questions/38095/if-machine-learning-techniques-keep-improving-whats-the-role-of-algorithmics-i&quot;&gt;stack exchange topic&lt;/a&gt;
that was basically asking: will algorithms be really useful in the future, knowing
that most of the time we are interested in “good enough” solution, and that we 
don’t even want some approximation guarantees, or even worse, we want solutions 
that are good, with the definition of good given only by examples. Unfortunately 
there were not many answers…&lt;/p&gt;

&lt;h2 id=&quot;power-diagrams-to-represents-fluids&quot;&gt;Power diagrams to represents fluids&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://11011110.github.io/blog/2019/01/15/linkage.html&quot;&gt;11011110 blog&lt;/a&gt; links 
to
&lt;a href=&quot;https://twitter.com/BrunoLevy01/status/1080085027210309632&quot;&gt;nice fluid simulations&lt;/a&gt;
using &lt;a href=&quot;https://en.wikipedia.org/wiki/Power_diagram&quot;&gt;power diagram&lt;/a&gt;, 
to picture units of fluid. Power diagrams are generalizations of 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt; but special 
cases of &lt;a href=&quot;https://en.wikipedia.org/wiki/Weighted_Voronoi_diagram&quot;&gt;weighted Voronoi diagrams&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;order-types&quot;&gt;Order types&lt;/h2&gt;

&lt;p&gt;Suppose you have a configuration with four points in the plane, such that no 
three of them are aligned. Then, either all the points are extreme points, and 
they form a kind of quadrilateral, or there are three points forming a triangle, 
and the last point is in the triangle. These two cases are called the order types
(for four points in the plane). As you can expect, this gets more tricky when 
you look at more points.&lt;/p&gt;

&lt;p&gt;There was a &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/1.9781611975482.27&quot;&gt;paper at SODA 2019&lt;/a&gt;
about this topic.&lt;/p&gt;

</description>
        <pubDate>Wed, 30 Jan 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///january-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///january-2019-notes</guid>
      </item>
    
      <item>
        <title>October batch, forgotten notions</title>
        <description>&lt;p&gt;&lt;img src=&quot;assets/arbre-bw.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I saw many talks in October and I plan to blog a bit about those in some 
posts to come. This first post is about some notions a somehow knew but couldn’t 
really define.&lt;/p&gt;

&lt;h2 id=&quot;epsilon-nets&quot;&gt;$\epsilon$-nets&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/%CE%95-net_(computational_geometry)&quot;&gt;$\epsilon$-nets&lt;/a&gt; 
are often appear in computational geometry. Consider a set of points $X$ in 
a geometric space, and a collection $C$ of subsets of $X$ (for example you have 
a collection of balls, then it defines the collection of subsets of $X$: the 
points contained in each ball). 
Now you are given an $\epsilon\in [0,1]$. 
An $\epsilon$-net $E$ is a subset of $X$, such that for every element $c$ of $C$ 
that is large enough, $c$ contains an element of $X$, and large enough means: 
&lt;script type=&quot;math/tex&quot;&gt;\frac{|c|}{|X|}\geq \epsilon.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;As one can imagine this is a useful tool to build approximation algorithm, as 
one may be able to focus on the net, that is hopefully much smaller than $X$.&lt;/p&gt;

&lt;p&gt;The concept is related to the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension&quot;&gt;VC dimension&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A nice introduction with a chocolate-consumer example can be found 
&lt;a href=&quot;https://www.ti.inf.ethz.ch/ew/lehre/CG12/lecture/Chapter%2015.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ptas-qptas-etc&quot;&gt;PTAS, QPTAS etc.&lt;/h2&gt;

&lt;p&gt;A small list of the accronyms for approximation schemes (PTAS is fine for me, 
but after that…):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme&quot;&gt;PTAS&lt;/a&gt;, 
polynomial-time approximation scheme: for every $\epsilon$, you get 
approximation $(1+\epsilon)$, in polynomial time. That is when you fix 
$\epsilon$ you get a polynomial in $n$. Typically you have time complexity 
$f(\epsilon)\times n^{g(\epsilon)}$, 
where $f$ and $g$ can be arbitrarily bad.&lt;/li&gt;
  &lt;li&gt;EPTAS, for efficient-PTAS: here the exponent should not depend on $\epsilon$. 
With the example above, $g$ is a constant, and $f$ can still be arbitrary.&lt;/li&gt;
  &lt;li&gt;FPTAS, for fully-PTAS: the algorithm is polynomial in both $n$ and $1/\epsilon$.
Typically, $g$ is a constant, and $f$ a polynomial in $1/\epsilon$.&lt;/li&gt;
  &lt;li&gt;QPTAS, quasi-polynomial-time approximation scheme: this is worse than PTAS, 
because you allow that even for a fixed $\epsilon$ the complexity is only
quasi-polynomial, for example some $n^{\log n}$.&lt;/li&gt;
  &lt;li&gt;PRAS, EPRAS, FPRAS: the same as PTAS, EPTAS, FPTAS, but randomized (the result 
has to be a correct approximation with high probability).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Both this section and the previous one originate from the talk of 
&lt;a href=&quot;https://www.lamsade.dauphine.fr/~bonnet/&quot;&gt;Edouard Bonnet&lt;/a&gt; for the paper 
&lt;em&gt;&lt;a href=&quot;http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f568.pdf&quot;&gt;EPTAS for Max Clique on Disks and Unit Balls&lt;/a&gt;&lt;/em&gt; 
at FOCS 2018.)&lt;/p&gt;

&lt;h2 id=&quot;doubling-dimension&quot;&gt;Doubling dimension&lt;/h2&gt;

&lt;p&gt;A lot of recent papers prove nice results in doubling metrics. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Doubling_space&quot;&gt;doubling 
dimension&lt;/a&gt; 
of a metric space is the smallest positive $k$ such that every ball of 
the space can be covered by $2^k$ balls of half the radius. You can think of 
the plane, and show that you can cover a ball of radius 1, with 7 balls of 
radius 1/2, which gives doubling dimension 3. More generally for the euclidian 
space $R^d$, the doubling dimension is known to be in $O(d)$. Thus bounded 
doubling dimension is a natural generalization of bounded dimension.&lt;/p&gt;

&lt;p&gt;(The notion appears for example in &lt;em&gt;&lt;a href=&quot;http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f814.pdf&quot;&gt;$ε$-Coresets for Clustering (with Outliers) 
in Doubling Metrics&lt;/a&gt;&lt;/em&gt; also 
presented at FOCS.)&lt;/p&gt;

</description>
        <pubDate>Thu, 24 Jan 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///october-batch-forgotten</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///october-batch-forgotten</guid>
      </item>
    
      <item>
        <title>December notes</title>
        <description>&lt;p&gt;&lt;img src=&quot;assets/arbre.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;karchmer-wigderson-theorem&quot;&gt;Karchmer-Wigderson theorem&lt;/h2&gt;

&lt;p&gt;Karchmer-Wigderson theorem makes a precise link between circuit and 
communication complexity. More precisely it 
 relates the depth of a boolean
circuit computing some function $f$, to the communication complexity of a game 
based on $f$. The game is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inputs: Alice has an input $x$ such that $f(x)=1$, 
and Bob has another input $y$ such that $f(y)=0$.&lt;/li&gt;
  &lt;li&gt;Task: find $i$ such that $x_i \neq y_i$ (such an $i$ must exist).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The way to go from the circuit to a communication protocol is the following. 
Alice and Bob each run the circuit on their inputs, the usual bottom-up way. 
Thus each of them knows the 
evaluation after each gate (“after this gate, this wire holds a 0, after this 
one it’s a 1” etc.). Now the protocol will go top-down.
As after the final gate (that is, the top gate), 
Alice gets a 1 and Bob gets a 0, we know that the two wires 
entering this gate cannot both have the same value for Alice and Bob. 
Alice and Bob can communicate to agree on one wire that has different value. 
This takes constant number of bits. And then you just follow the wire to the 
next gate and continue. At the end Alice and Bob agree on an input wire that has different 
value and they win the game. The communication complexity is the circuit depth.&lt;/p&gt;

&lt;p&gt;For the other direction, see the Internet.&lt;/p&gt;

&lt;p&gt;(I learnt about Karchmer-Wigderson theorem recently by watching parts of an 
&lt;a href=&quot;https://www.youtube.com/watch?v=t1EsRm1dmw0&quot;&gt;online talk&lt;/a&gt; by 
&lt;a href=&quot;http://www.math.ias.edu/~mika/&quot;&gt;Mika Göös&lt;/a&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;counting-crows&quot;&gt;Counting crows&lt;/h2&gt;

&lt;p&gt;These days I often cross the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Jardin_des_plantes&quot;&gt;Jardin des Plantes&lt;/a&gt;
in Paris, and I saw a sign giving information about the counting of crows in 
Paris. Basically a large number of crows now have a ring with a specific number. 
The problem is that the rings tend to fall, disappear etc. To evaluate this 
loss rate, the birds are rung with two rings. Then 
by counting the number of crows with zero, one and two rings, one can evaluate 
the loss rate, and then the population.&lt;/p&gt;

&lt;p&gt;This sounded very much like CS to me, for example in networks, 
sending packets that may disappear, and trying to know the message loss for 
example.&lt;/p&gt;

&lt;h2 id=&quot;minimum-degree-spanning-tree&quot;&gt;Minimum degree spanning tree&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Degree-constrained_spanning_tree&quot;&gt;Minimum-degree spanning tree&lt;/a&gt;, 
consists in finding a spanning tree of a graph, 
with the constraint that the maximum degree of the tree should be as small as 
possible. This problem is NP-hard, because if the answer is 2 then it means that 
you have an Hamiltonian path. But one can get an approximation with additive 
constant 1.
The algorithm (from 
&lt;a href=&quot;https://doi.org/10.1006%2Fjagm.1994.1042&quot;&gt;this paper&lt;/a&gt;, explained in 
&lt;a href=&quot;http://pages.cs.wisc.edu/~shuchi/courses/880-S07/scribe-notes/lecture08.pdf&quot;&gt;these lecture notes&lt;/a&gt;)
consists in a local search, with swapping of edges to as local moves.&lt;/p&gt;

&lt;p&gt;This problems looks very natural and potentially very useful. I’d be curious 
to know if there are real-world applications.&lt;/p&gt;

&lt;p&gt;(I discovered this problem in
&lt;a href=&quot;https://doi.org/10.1109/ICDCS.2015.66&quot;&gt;this distributed computing paper&lt;/a&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;cycle-cover&quot;&gt;Cycle cover&lt;/h2&gt;
&lt;p&gt;An &lt;a href=&quot;https://en.wikipedia.org/wiki/Edge_cycle_cover&quot;&gt;(edge) cycle-cover&lt;/a&gt;
is a set of cycles of a graph, such that every edge of the graph 
is contained in at least one cycle. There are several problems associated with 
this object, for example related to the total length of the cycles in such a 
cover. I want to mention a super-cute-super-hard open problem: the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Cycle_double_cover&quot;&gt;cycle double cover conjecture&lt;/a&gt;. 
The conjecture is: in every graph with no 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Bridge_(graph_theory)&quot;&gt;bridge&lt;/a&gt;, 
there is a list of cycles so that every edge is 
contained in exactly two.&lt;/p&gt;

&lt;p&gt;If you think “How can this be open?!”, I’ll just add that it is listed in the 
&lt;a href=&quot;http://www.openproblemgarden.org/op/cycle_double_cover_conjecture&quot;&gt;open problem garden&lt;/a&gt; 
as of “outstanding importance”. See there, or on the wikipedia page linked above 
for the details.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;https://arxiv.org/abs/1812.04492&quot;&gt;A paper about cycle covers&lt;/a&gt; recently 
appeared on the arxiv, reminded me this problem.)&lt;/p&gt;

&lt;h2 id=&quot;mst-algorithms&quot;&gt;MST algorithms&lt;/h2&gt;
&lt;p&gt;Consider the following algorithm for finding a minimum weight spanning tree, similar to 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Bor%C5%AFvka%27s_algorithm&quot;&gt;Borůvka’s algorithm&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Every node begins has a so-called fragment.&lt;/li&gt;
  &lt;li&gt;Until there is only one fragment: choose an arbitrary fragment, find the 
lightest edge linking a node of 
this fragment to another fragment, merge the two fragments into a new one by 
adding this edge.&lt;/li&gt;
  &lt;li&gt;Output the final fragment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This algorithm is actually a generalization of Borůvka, Prim and Kruskal algorithm! 
For Borůvka’s algorithm, one would basically choose an 
outgoing edge for all fragments in parallel. For Prim, one would always choose 
the same fragment to enlarge. F	or Kruskal, one would choose the lightest 
outgoing edge of all the fragments.&lt;/p&gt;

&lt;p&gt;This can probably be explained with red-blue rules, but I like this point of
view with a kind of scheduler, with different strategies.&lt;/p&gt;

&lt;p&gt;(I’m working again on some minimum spanning tree distributed problems, and it 
reminded me about this fact, that I discovered a few years ago, but that I’ve 
never seen in undergraduate courses.)&lt;/p&gt;

&lt;h2 id=&quot;squashed-cube-conjecture-and-distance-labelling&quot;&gt;Squashed cube conjecture and distance labelling&lt;/h2&gt;
&lt;p&gt;Distance labelling are labels given to the nodes of a graph such that given the 
labels of two arbitrary nodes $u$ and $v$, and without seeing the graph, one can compute 
the distance between $u$ and $v$. One usually tries to minimize the size of 
the labels.
A strategy for this is to find some metric embedding of the graph, because then 
one can 
simply give the coordinates of the nodes as labels. 
In this direction, a natural thing to do is to try the embed the graph in an 
hypercube with the Hamming distance. This cannot be done in general, but the 
squashed cube conjecture (that is actually a theorem) provides a result close to 
this.&lt;/p&gt;

&lt;p&gt;Consider that instead of two symbols, there are three: 0, 1, and *, and 
that the “distance” between $x$ and $y$ is the number of indices such that $x=0$ 
and $y=1$ or $x=1$ and $y=0$. That is * is at distance zero from 0 and 1. 
(Note that this distance is not a proper distance.)
Now how many symbols do you need to have a distance preserving embedding? 
Exactly $n-1$, as proved in &lt;a href=&quot;https://doi.org/10.1007/BF02579350&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(I discovered this in the related works section of 
&lt;a href=&quot;https://doi.org/10.1007/3-540-44676-1_40&quot;&gt;this paper&lt;/a&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;jaccard-metric&quot;&gt;Jaccard metric&lt;/h2&gt;
&lt;p&gt;Lipton and Regan talk about the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard metric&lt;/a&gt; in 
&lt;a href=&quot;https://rjlipton.wordpress.com/2018/12/14/explaining-the-jaccard-metric/&quot;&gt;a post&lt;/a&gt;
of their blog, &lt;em&gt;Gödel’s lost letter&lt;/em&gt;, in particular why it is a metric. I didn’t 
know about this distance over sets, so here is the definition.&lt;/p&gt;

&lt;p&gt;Let $A$ and $B$ 
be two sets, the distance is:
&lt;script type=&quot;math/tex&quot;&gt;d(A,B)=1-\frac{|A \cap B|}{|A \cup B|}.&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;preprints&quot;&gt;Preprints&lt;/h2&gt;
&lt;p&gt;I have two new preprints this month:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.05913&quot;&gt;Graph classes and forbidden patterns on three vertices&lt;/a&gt;
with &lt;a href=&quot;https://www.irif.fr/~habib/&quot;&gt;Michel Habib&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;xxx&quot;&gt;Lower bounds for text indexing with mismatches and differences&lt;/a&gt; with
&lt;a href=&quot;https://www.di.ens.fr/~vcohen/&quot;&gt;Vincent Cohen-Addad&lt;/a&gt; and 
&lt;a href=&quot;https://starikovskaya.github.io/homepage/&quot;&gt;Tatiana Starikoskaya&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I really like both papers, I hope I’ll find some time to blog about it soon.&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Dec 2018 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///december-2018-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///december-2018-notes</guid>
      </item>
    
      <item>
        <title>November notes</title>
        <description>&lt;p&gt;A few notes for November 2018.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;a-new-paper-on-descriptive-complexity-of-distributed-computing&quot;&gt;A new paper on descriptive complexity of distributed computing&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Descriptive_complexity_theory&quot;&gt;Descriptive complexity&lt;/a&gt; 
basically aims at characterizing complexity classes through logic. A classic 
results is &lt;a href=&quot;https://en.wikipedia.org/wiki/Descriptive_complexity_theory&quot;&gt;Fagin’s Theorem&lt;/a&gt;
that characterizes NP. 
Descriptive complexity for distributed computing is a relatively new topic,&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
and a &lt;a href=&quot;https://arxiv.org/abs/1811.08197&quot;&gt;new paper&lt;/a&gt; just came out on arxiv.
I’m not a specialist, but from what I understood, the classic assumption of the
LOCAL model that there are unique identifiers, is pretty difficult to transfer 
into logic, and this paper seems to make a step in this direction.&lt;/p&gt;

&lt;h2 id=&quot;a-map-of-the-theory-of-distributed-computing-community&quot;&gt;A map of the theory of distributed computing community&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://users.ics.aalto.fi/suomela/&quot;&gt;Jukka Suomela&lt;/a&gt; published
&lt;a href=&quot;https://plus.google.com/+JukkaSuomela/posts/JgWYFk4XzWW&quot;&gt;a nice map&lt;/a&gt; of the 
PODC/DISC communities. (PODC and DISC are the two main conferences in theory 
of distributed computing.)&lt;br /&gt;
It is a graph where the nodes are the authors, and the edges between them 
have different thickness, 
depending on how much they have collaborated, or have had papers in the same 
sessions. There are strong thematic clusters, which is no surprise.&lt;/p&gt;

&lt;h2 id=&quot;symmetries-in-lps-and-sos&quot;&gt;Symmetries in LPs and SOS&lt;/h2&gt;
&lt;p&gt;I attended the PhD defense of 
&lt;a href=&quot;https://www.lip6.fr/actualite/personnes-fiche.php?ident=D1634&quot;&gt;Cécile Rottner&lt;/a&gt;
who was a student in the OR team of &lt;a href=&quot;https://www.lip6.fr/?LANG=en&quot;&gt;LIP6&lt;/a&gt;. 
Her thesis was about a problem that
any electric utility company faces: how to manage the different the power plants
to meet the demand while using the less energy possible, given that there are 
many constraints on these plants (a nuclear reactor cannot be switched on and 
off in a minute, some other stuff has to cool down, etc.). As often in OR (as far as
I know) this is done by having big LPs and playing with them, adding new 
inequalities, trying to use the structure to speed the computation, having 
branch and cut routines etc.&lt;/p&gt;

&lt;p&gt;One of the big challenges that one has to tackle when solving these big LPs 
in an industrial context is to break the symmetries.
Suppose you have two identical 
nuclear reactors in your system. Then if you use one or the other in your solution, 
you will have the same cost. This implies that you can have many many optimal 
solutions. This is bad for a branch and cut 
strategy, where the ideal case is to have only one optimal solution, and to be 
cutting all the other branches quickly. Cécile showed ways to solve this problem.&lt;/p&gt;

&lt;p&gt;This reminded me of another PhD defense with symmetries: the one of 
&lt;a href=&quot;https://sites.google.com/view/vverdugo/&quot;&gt;Victor Verdugo&lt;/a&gt;. Victor had a part of 
his PhD work on how to break symmetry for 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Sum-of-squares_optimization&quot;&gt;sum-of-squares&lt;/a&gt;. The
timing for this blog post is pretty good: the paper of Victor on this topic 
just appeared on arxiv, 
&lt;a href=&quot;https://arxiv.org/abs/1811.08539&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;double-blind-for-disc&quot;&gt;Double-blind for DISC&lt;/h2&gt;
&lt;p&gt;The conference &lt;a href=&quot;http://www.disc-conference.org/wp/&quot;&gt;DISC&lt;/a&gt; will go 
double-blind next year (that is the names of both the reviewers and the authors will
be anonymized).&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; At first I was sceptical about this idea, 
because of the usual reasons: extra-care in the process (e.g. when talking to 
people) with probably no big impact, etc. But recently I reviewed a paper by 
authors from a university I had never heard of, and I felt that before even 
starting I had a negative bias. I think double-blind is exactly about 
protecting authors from this bias.&lt;/p&gt;

&lt;p&gt;I heard many times that the 
problem is that well-known people get their papers accepted although 
they are not good enough, and I think this cannot really change (because 
of arxiv, favourite topics, writing style), but it’s the other side of the 
spectrum that can be made more fair. So let’s see how it goes.&lt;/p&gt;

&lt;h2 id=&quot;a-few-points-on-networks-and-games&quot;&gt;A few points on networks and games&lt;/h2&gt;
&lt;p&gt;I attended a talk at LIP6 by 
&lt;a href=&quot;http://webee.technion.ac.il/Sites/People/ArielOrda/&quot;&gt;Ariel Orda&lt;/a&gt; on game theory 
and networks. The talk described two very interesting works, but 
I just picked a few non-specific things that caught my interest.&lt;/p&gt;

&lt;h3 id=&quot;network-formation-games&quot;&gt;Network formation games&lt;/h3&gt;
&lt;p&gt;The first topic is about understanding the structure of real-world networks, by 
finding ways to build algorithmically networks that have similar properties. 
A well-known generation algorithm is the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;&gt;Barabási–Albert model&lt;/a&gt; 
where nodes basically arrive one by one and choose who to be linked to, 
based on the degree of the nodes that already arrived. There is a second 
type of network formation model that I didn’t know,&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; which is to start 
with a fixed set of nodes, and to make them play a game to decide 
which edges are in the graph. 
For example, the nodes want to maximize their pay-offs in a game where every 
link cost something, but having short paths to every node is rewarded. 
These are called network formation games.&lt;/p&gt;

&lt;h3 id=&quot;monetary-transfer&quot;&gt;Monetary transfer&lt;/h3&gt;
&lt;p&gt;The second idea is about introducing money in games.
Suppose you have a Nash equilibrium 
that is very bad (for some definition of bad) because each player, when 
maximizing its gain, is hurting the other players a lot. Then you can introduce
monetary transfer, that consists for two players A and B to agree that if the 
A does this thing that decreases its pay-off but increases the pay-off 
of B, then B will give part of its 
pay-off to reimburse A, and both will be happier. Natural idea to 
consider, but that I had never heard of.&lt;/p&gt;

&lt;h3 id=&quot;using-motifs-to-validate-a-model&quot;&gt;Using motifs to validate a model&lt;/h3&gt;
&lt;p&gt;I knew that people studying social 
networks are obsessed with finding motifs (small graphs that appear more 
often than others), but I was not sure why. It could be just to have 
more knowledge about the relationships, but in Orda’s talk, it was 
also a way to validate a model. Basically: in real-world graphs this and that 
motifs are very common, we don’t know why, and previous generative 
models did not have this property, but their model could capture this. 
As far as I know, parameters such as the diameter, or the clustering 
coefficient are more classic ways to validate such models.&lt;/p&gt;

&lt;h3 id=&quot;about-the-price-of-anarchy-selfishness-and-collaboration&quot;&gt;About the price of anarchy, selfishness and collaboration&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Price_of_anarchy&quot;&gt;price of anarchy&lt;/a&gt;, 
is the ratio between the social cost when each players tries to optimize 
its own pay-off, and when all the players play the strategy that minimize 
the social cost. It is often said that this price is the price to pay 
when players are selfish. But it is not completely true, it is also the 
price of not collaborating. You can image scenarios in which players have
the option of collaborating but each player will agree only if it ensures 
 a better pay-off. That is the players are still selfish but 
they can talk to each other and make agreements. 
This can change the outcome a lot. Then one 
will consider what is called a Nash bargaining solution.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;If you are interested, see &lt;a href=&quot;https://arxiv.org/abs/1805.06238&quot;&gt;Fabian Reiter’s very nice PhD thesis&lt;/a&gt;, and the &lt;a href=&quot;https://semidoc.github.io/reiter-dga&quot;&gt;gentle introduction&lt;/a&gt; to the topic I wrote on semidoc.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;See &lt;a href=&quot;https://twitter.com/JukkaSuomela/status/1065259077738082304&quot;&gt;this tweet&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;There seems that there is no third well-known way to generate networks, at least in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Network_formation&quot;&gt;wikipedia article about network formation&lt;/a&gt;.&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Actually I somehow knew because of &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=3178876.3186122&quot;&gt;this paper&lt;/a&gt; by my PhD advisor and colleges, but I had forgotten.&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 30 Nov 2018 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///november-2018-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///november-2018-notes</guid>
      </item>
    
  </channel>
</rss>
