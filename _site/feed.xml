<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Discrete notes</title>
    <description>Blog</description>
    <link>https://discrete-notes.github.io/</link>
    <atom:link href="https://discrete-notes.github.io//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>June notes</title>
        <description>&lt;p&gt;Some notes for June.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/saint-laurent.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;600px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
&lt;a href=&quot;http://www.st-laurent-de-la-cabrerisse.com/eng/&quot;&gt;Saint Laurent de la Cabrerisse&lt;/a&gt;
where the &lt;a href=&quot;https://www.irit.fr/algotel2019/index.html&quot;&gt;Algotel conference&lt;/a&gt; 
took place.&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;recorded-talks-and-collaborating-virtually&quot;&gt;Recorded talks and collaborating virtually&lt;/h2&gt;

&lt;p&gt;Continuing on the topic of video conferences (e.g. to avoid plane travel),
an important question is: can virtual conferences, and more generally 
communication via video-conference, be really efficient?&lt;/p&gt;

&lt;p&gt;I have to admit that, although I really like the idea of video conference, 
for talks or for work, I have 
difficulties following recorded talks, and I’m not very comfortable with 
video calls. 
Here are the reasons I could find for why video talks and calls are not as good 
as their physical analogues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;in a recorded talk, you can escape, take a coffee, answer an email, that is 
you can “escape the room”, thus you are less focused.&lt;/li&gt;
  &lt;li&gt;videos are less immersive: the sound is bad, what you watch 
occupies a very small part of your visual field, and in a talk you don’t feel 
that you are part of a crowd participating in something.&lt;/li&gt;
  &lt;li&gt;A consequence of the previous bullet is that the experience is socially less 
comfortable: in a real meeting, there can be a long silence and it’s fine, but 
during a video call, it’s often awkward.&lt;/li&gt;
  &lt;li&gt;Also video conferences are very static. This is especially true for talks: the 
main part of the screen is taken by the slide, which moves once every minute on 
average, and even if you see the speaker, you just see a face, no body movements.&lt;/li&gt;
  &lt;li&gt;Related to the previous point: I don’t know good ways to have a shared 
whiteboard when working through video, which reduces a lot the efficiency of the 
conversation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of these problems are kind of solved by some frameworks. For example 
&lt;a href=&quot;https://sites.google.com/site/plustcs/&quot;&gt;TCS+&lt;/a&gt; is an online seminar where the 
speaker does not record a video alone: it is more like a large video conference
with a dozen of labs connected, and one person giving the talk. (The 
talks are actually recorded so you can watch them offline, but it is not the 
main goal.) 
This means that there is a crowd, and that you cannot escape. Also if you have a 
good video/sound system, then it might be quite immersive. 
I never had the opportunity to test this so I can’t tell.&lt;/p&gt;

&lt;p&gt;Note that some people think the exact opposite of me: they actually prefer 
watching a recorded talk, because it’s offline: you can stop, rethink what was 
said, check a reference, jump to the next section etc. 
Also note that attending a live talk can also be boring, so it also depends a 
lot on the speaker.&lt;/p&gt;

&lt;p&gt;If you know good alternatives to skype (ideally open-source, with a possibility of 
drawing, and multi-users), send me an email. This will be useful not only to me 
but also to other people: I timidly signed to be part of a pool of people 
thinking about such technical topics, in the 
&lt;a href=&quot;https://labos1point5.org/en/home/&quot;&gt;Labo 1.5 collective&lt;/a&gt; I talked about last 
month.&lt;/p&gt;

&lt;h2 id=&quot;terence-tao-on-the-radius-of-the-earth&quot;&gt;Terence Tao on the radius of the Earth&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Terence_Tao&quot;&gt;Terence Tao&lt;/a&gt; has a blog, 
&lt;a href=&quot;https://terrytao.wordpress.com/&quot;&gt;What’s new&lt;/a&gt;. Most of the content is way too 
complicated for me, but once in a while he writes excellent posts about simpler
things. 
A very recent one is 
&lt;a href=&quot;https://terrytao.wordpress.com/2019/05/25/the-spherical-cayley-menger-determinant-and-the-radius-of-the-earth/&quot;&gt;about the radius of the Earth and determinants&lt;/a&gt;. An older one is about 
&lt;a href=&quot;https://terrytao.wordpress.com/2009/03/23/sailing-into-the-wind-or-faster-than-the-wind/&quot;&gt;sailing&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;basic-and-advanced-tools-in-combinatorics&quot;&gt;Basic and advanced tools in combinatorics&lt;/h2&gt;

&lt;p&gt;I have heard that some people tend to despise combinatorics as being “very basic”.
To some extent, my research is very basic: most of the time I consider very 
simple objects, a few edges and a few nodes in a graph. I almost never use 
complicated tools which have been improved over the centuries and which 
need time to master. Sometimes it is a bit unsettling: one can feel like playing
child games, compared to what one was taught at the university.&lt;/p&gt;

&lt;p&gt;On the other hand of course, it is very annoying when people boast about using
complicated stuff, and it is always very nice to find a basic proof of a theorem.&lt;/p&gt;

&lt;p&gt;Something I like is when you have a basic proof that is a bit of a mess, with
many cases for example, and using the point of view of a more general 
theory you suddenly understand things better. I had this experience with some 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Matroid&quot;&gt;matroid&lt;/a&gt; structure in
&lt;a href=&quot;https://pages.lip6.fr/Laurent.Feuilloley/publications/error_sensitive.html&quot;&gt;this paper&lt;/a&gt;
(although it does not appear at all in the final write-up).&lt;/p&gt;

&lt;p&gt;I was reminded of this by 
&lt;a href=&quot;https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html&quot;&gt;this post&lt;/a&gt;
on David Eppstein’s blog, where he describes a new point of view he has on a 
matching problem thanks to a more advanced notion of matroid (yes matroids, again).&lt;/p&gt;

&lt;h2 id=&quot;john-ellipsoid-and-the-s&quot;&gt; John ellipsoid and the “‘s”&lt;/h2&gt;

&lt;p&gt;This &lt;a href=&quot;https://arxiv.org/pdf/1905.11580.pdf&quot;&gt;arxiv preprint&lt;/a&gt; made me discover 
&lt;a href=&quot;https://en.wikipedia.org/wiki/John_ellipsoid&quot;&gt;John ellipsoid&lt;/a&gt;. Given a polytope
in $n$ dimensions, John ellipsoid is the largest-volume ellipsoid included in 
the polytope. On the picture below it is the red ellipse (more or less).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/john-ellipsoid.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An interesting property is that if you take the ellipsoid and dilate it by a 
factor $n$, then this new ellipsoid contains the polytope (more or less the blue 
ellipsoid on the picture). At first I thought that dimension 3 should not require 
more dilatation than dimension 2, but if you think about an equilateral triangle 
in the plane, and a tetrahedron in a 3D space, you see that the inner ball of the 
tetrahedron has to be smaller than the inner disk of the equilateral triangle, 
and that the outer ball has to be larger.&lt;/p&gt;

&lt;p&gt;The arxiv preprint presents an algorithm to approximate John ellipsoid.&lt;/p&gt;

&lt;p&gt;By the way, why is it “John ellipsoid” and, say, “Dijktra’s algorithm”, why is 
there a “‘s” sometimes but not always?&lt;/p&gt;

&lt;h2 id=&quot;eppsteins-report-on-socg-and-robust-statistics&quot;&gt; Eppstein’s report on SoCG and robust statistics&lt;/h2&gt;

&lt;p&gt;Eppstein wrote a nice
&lt;a href=&quot;https://11011110.github.io/blog/2019/06/21/report-from-socg.html&quot;&gt;report of SOCG&lt;/a&gt;,
with a selection of talks that caught his interest.&lt;/p&gt;

&lt;p&gt;He refers to &lt;a href=&quot;https://en.wikipedia.org/wiki/Centerpoint_(geometry)&quot;&gt;Tukey depth&lt;/a&gt;, 
which is related to Tukey median, which is a 
generalization of the median in higher dimension. I discovered this notion in a great 
talk by &lt;a href=&quot;https://people.csail.mit.edu/moitra/&quot;&gt;Ankur Moitra&lt;/a&gt; about the 
computational aspects of
&lt;a href=&quot;https://en.wikipedia.org/wiki/Robust_statistics&quot;&gt;robust statistics&lt;/a&gt;, at 
&lt;a href=&quot;http://2018.highlightsofalgorithms.org/&quot;&gt;HALG 2018&lt;/a&gt;. 
The basic story is the following. Suppose you have data, and you want to 
summarize it by one point. You want to use the average, but because your data 
has some adversarial noise, the average of the noisy data might be far from the 
average of the “real data”. If you are in 1D then you can take the median, which 
is robust to such noise. But if you are in $d$ dimension, then there are several 
generalizations of the median, 
and either they are not robust to noise or they are NP-hard to compute, and then 
you have to think.&lt;/p&gt;

&lt;p&gt;Two references on robust statistics : 
&lt;a href=&quot;http://people.csail.mit.edu/moitra/docs/robust2.pdf&quot;&gt;here&lt;/a&gt;
and 
&lt;a href=&quot;https://www.csun.edu/~ctoth/Handbook/chap58.pdf&quot;&gt;there&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper I talked about in a 
&lt;a href=&quot;https://discrete-notes.github.io/simulation-1&quot;&gt;series of posts in February&lt;/a&gt;, by 
the Aalto distributed computing people has been accepted to 
&lt;a href=&quot;http://focs2019.cs.jhu.edu/&quot;&gt;FOCS 2019&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;a href=&quot;http://eatcs.org/images/bulletin/beatcs128.pdf&quot;&gt;128th bulletin of the EATCS&lt;/a&gt; 
is out, with a chapter on matching with preference lists, and a chapter on 
leader election in directed dynamic graphs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 02 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///june-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///june-2019-notes</guid>
      </item>
    
      <item>
        <title>Material on distributed graph algorithms</title>
        <description>&lt;p&gt;This post is a list of pointers to books and surveys about distributed graph 
algorithms (LOCAL model, CONGEST model, and friends).
I probably missed some references, as typing “local model” in a search engine 
is not very helpful if you are not looking for top model agencies. 
Other references are most welcome!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/local.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;distributed-computing-a-locality-sensitive-approach-by-peleg-and-other-references&quot;&gt;Distributed computing: a locality-sensitive approach, by Peleg, and other references&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://epubs.siam.org/doi/book/10.1137/1.9780898719772&quot;&gt;Distributed computing: a locality-sensitive approach&lt;/a&gt;
by &lt;a href=&quot;http://www.weizmann.ac.il/math/peleg/&quot;&gt;David Peleg&lt;/a&gt; is the classic book 
about the local model. It’s from 2000, so it’s getting a bit outdated in terms 
of results.&lt;/p&gt;

&lt;h2 id=&quot;distributed-graph-coloring-by-barenboim-and-elkin&quot;&gt;Distributed graph coloring, by Barenboim and Elkin&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.bgu.ac.il/~elkinm/book.pdf&quot;&gt;Distributed graph coloring&lt;/a&gt;
by 
&lt;a href=&quot;https://www.openu.ac.il/personal_sites/leonid-barenboim/&quot;&gt;Leonid Bareboim&lt;/a&gt;
and &lt;a href=&quot;https://www.cs.bgu.ac.il/~elkinm/&quot;&gt;Michael Elkin&lt;/a&gt;, is a more recent book, 
with a focus on coloring. It explains quite a few techniques, and has a list of 
open problems (some of these problems have been solved already).&lt;/p&gt;

&lt;h2 id=&quot;survey-of-local-algorithms-by-suomela&quot;&gt;Survey of local algorithms, by Suomela&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://users.ics.aalto.fi/suomela/doc/local-survey.pdf&quot;&gt;survey of local algorithms&lt;/a&gt;
by &lt;a href=&quot;https://users.ics.aalto.fi/suomela/&quot;&gt;Jukka Suomela&lt;/a&gt; is the reference for 
results about constant-time computations in the local model.&lt;/p&gt;

&lt;h2 id=&quot;distributed-algorithms-by-suomela&quot;&gt;Distributed algorithms, by Suomela&lt;/h2&gt;

&lt;p&gt;A neat online textbook is
&lt;a href=&quot;https://users.ics.aalto.fi/suomela/da/da-print.pdf&quot;&gt;Distributed algorithm&lt;/a&gt;
by Jukka Suomela.
In addition to the classic topics such as coloring, that are contained in most 
references listed here, it has a focus on showing the similarities and differences 
between different models (such as port numbers and unique identifiers), and on the 
graph theory tools that can be used (such as covering maps and Ramsey theory).&lt;/p&gt;

&lt;h2 id=&quot;the-swiss-german-lecture-notes&quot;&gt;The Swiss-German lecture notes&lt;/h2&gt;

&lt;p&gt;There are several courses related to the local model that are taught in 
Switzerland and South Germany. They are rather close one from another, as they 
stem from the same Zurich source (but then they evolved on their own).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The current version of the original source is 
&lt;a href=&quot;https://disco.ethz.ch/courses/podc/&quot;&gt;Principles of Distributed Computing&lt;/a&gt;, 
by &lt;a href=&quot;https://disco.ethz.ch/members/wroger&quot;&gt;Roger Wattenhofer&lt;/a&gt; (and more recently
&lt;a href=&quot;https://people.csail.mit.edu/ghaffari/&quot;&gt;Mohsen Ghaffari&lt;/a&gt; but his part is 
covered below). In addition to the classic topics of synchronous computing, it 
covers some topics at the boundary with asynchronous computing (such as 
synchronizers), or fully asynchronous computing (such as shared objects).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A set of lecture notes by &lt;a href=&quot;http://ac.informatik.uni-freiburg.de/kuhn/&quot;&gt;Fabian Kuhn&lt;/a&gt;
is available chapter by chapter on
&lt;a href=&quot;http://ac.informatik.uni-freiburg.de/teaching/ss_18/network-algorithms.php&quot;&gt;the course webpage&lt;/a&gt;.
The topics are very close from the ones of the bullet above. Two topics that 
appear only here: dynamic networks and
&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_network_coding&quot;&gt;network coding&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another set of lecture notes is 
&lt;a href=&quot;https://www.mpi-inf.mpg.de/departments/algorithms-complexity/teaching/winter18/tods/&quot;&gt;Theory of Distributed Systems&lt;/a&gt;, 
by &lt;a href=&quot;http://people.mpi-inf.mpg.de/~clenzen/&quot;&gt;Christoph Lenzen&lt;/a&gt;. It covers 
(in addition to the classic material) self-stabilization and 
routing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Probably the most recent course is
&lt;a href=&quot;https://disco.ethz.ch/courses/podc/lecturenotes/LOCAL.pdf&quot;&gt;Distributed graph algorithms&lt;/a&gt;
by 
&lt;a href=&quot;https://people.csail.mit.edu/ghaffari/&quot;&gt;Mohsen Ghaffari&lt;/a&gt;. 
It is a shorter text, with a focus on algorithmic techniques for the local model, 
e.g. network decomposition.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;material-covering-only-the-basics-or-only-specialized-content&quot;&gt;Material covering only the basics, or only specialized content&lt;/h2&gt;

&lt;p&gt;A standard book is 
&lt;a href=&quot;https://www.elsevier.com/books/distributed-algorithms/lynch/978-1-55860-348-6&quot;&gt;Distributed Algorithms&lt;/a&gt;
by &lt;a href=&quot;http://people.csail.mit.edu/lynch/&quot;&gt;Nancy Lynch&lt;/a&gt;, but most of the book is 
off-topic for this post, because it deals with asynchronous systems.&lt;/p&gt;

&lt;p&gt;Yet another reference (with little material on the LOCAL model) is the online 
textbook 
&lt;a href=&quot;http://www.cs.yale.edu/homes/aspnes/classes/465/notes.pdf&quot;&gt;Notes on Theory of Distributed Systems&lt;/a&gt;
by &lt;a href=&quot;http://www.cs.yale.edu/homes/aspnes/&quot;&gt;James Aspnes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you look for &lt;a href=&quot;https://en.wikipedia.org/wiki/Self-stabilization&quot;&gt;self-stabilizing algorithms&lt;/a&gt;
then a reference from 2000 is 
&lt;a href=&quot;https://mitpress.mit.edu/books/self-stabilization&quot;&gt;Self-Stabilization &lt;/a&gt; 
by
&lt;a href=&quot;https://in.bgu.ac.il/en/natural_science/cs/dolev//Pages/default.aspx&quot;&gt;Shlomi Dolev&lt;/a&gt;, 
and a very recent one is 
&lt;a href=&quot;https://www.morganclaypool.com/doi/abs/10.2200/S00908ED1V01Y201903DCT015&quot;&gt;Introduction to Distributed Self-Stabilizing Algorithms&lt;/a&gt;
by
&lt;a href=&quot;http://www-verimag.imag.fr/Karine-Altisen,102.html?lang=en&quot;&gt;Karine Altisen&lt;/a&gt;,
&lt;a href=&quot;http://www-verimag.imag.fr/~devismes/WWW/introduction.html&quot;&gt;Stéphane Devismes&lt;/a&gt;,
&lt;a href=&quot;https://pages.lip6.fr/Swan.Dubois/&quot;&gt;Swan Dubois&lt;/a&gt;,
and 
&lt;a href=&quot;https://pages.lip6.fr/Franck.Petit/&quot;&gt;Franck Petit&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;in-french&quot;&gt;In French&lt;/h3&gt;
&lt;p&gt;Also, if you read French, you may be interested in 
&lt;a href=&quot;https://www.irif.fr/_media/users/pierref/notes_algo_distribue.pdf&quot;&gt;Algorithmique distribuée pour les réseaux&lt;/a&gt;
by &lt;a href=&quot;https://www.irif.fr/users/pierref/index&quot;&gt;Pierre Fraigniaud&lt;/a&gt;, and 
&lt;a href=&quot;http://dept-info.labri.fr/~gavoille/UE-AD/cours.pdf&quot;&gt;Algorithmes distribués&lt;/a&gt; 
by &lt;a href=&quot;http://dept-info.labri.fr/~gavoille/&quot;&gt;Cyril Gavoille&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Thanks to Jukka Suomela for pointing out some references.&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Jun 2019 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///local-model</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///local-model</guid>
      </item>
    
      <item>
        <title>April-May notes</title>
        <description>&lt;p&gt;Some deadlines disturbed the schedule of the monthly notes ; this post is for 
both April and May.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/table.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;cuckoo-cycles&quot;&gt;Cuckoo cycles&lt;/h2&gt;

&lt;p&gt;A 
&lt;a href=&quot;https://blog.computationalcomplexity.org/2019/04/cuckoo-cycles.html&quot;&gt;guest post&lt;/a&gt; 
by John Tromp, on the Computational Complexity blog, is about the cuckoo cycle 
problem.&lt;/p&gt;

&lt;p&gt;Basically there are some crypto-currencies, where the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Proof-of-work_system&quot;&gt;proof-of-work&lt;/a&gt; consists in 
finding cycles in a huge graph. 
This graph is the cuckoo graph, whose edges are defined by the hash function of 
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cuckoo_hashing&quot;&gt;cuckoo hashing&lt;/a&gt;. 
The post describes briefly the method used by the miners for this problem.&lt;/p&gt;

&lt;p&gt;Interesting to see a theoretical problem tackled in very practical way.&lt;/p&gt;

&lt;h2 id=&quot;pcp-videos&quot;&gt;PCP videos&lt;/h2&gt;

&lt;p&gt;At the end of 2018, a workshop took place in Tel Aviv, about the
&lt;a href=&quot;https://en.wikipedia.org/wiki/PCP_theorem&quot;&gt;PCP theorem&lt;/a&gt; and related topics.
Great news: the talks have been recorded, and the videos are available on a
&lt;a href=&quot;https://www.youtube.com/playlist?list=PLGRBwz8taWHiBHlgnX98zrbnWQCeDtFQ_&quot;&gt;youtube channel&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I watched &lt;a href=&quot;https://en.wikipedia.org/wiki/Ronitt_Rubinfeld&quot;&gt;Ronitt Rubinfield&lt;/a&gt;’s 
talk, and it’s very good. She tells the story of checkers and testers. 
Just a bit to arouse your interest: like many people, I know interactive proofs 
for difficult problems 
(isomorphism, satisfiability etc.), but actually testing and interactive proofs 
have started with things as simple as GCD.&lt;/p&gt;

&lt;h2 id=&quot;graphs-defined-bymatchsticks-pennies-and-hinges&quot;&gt;Graphs defined by matchsticks, pennies and hinges.&lt;/h2&gt;

&lt;p&gt;Here are three types of graphs that have a description in terms of real-world 
objects.&lt;/p&gt;

&lt;p&gt;First, &lt;a href=&quot;https://en.wikipedia.org/wiki/Matchstick_graph&quot;&gt;matchstick graphs&lt;/a&gt; are 
graphs that you can draw on a table with matchsticks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/alumettes.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In more precise terms, these are planar
&lt;a href=&quot;https://en.wikipedia.org/wiki/Unit_distance_graph&quot;&gt;unit-distance graphs&lt;/a&gt;, that 
is graphs that you can draw on the plane with only (straight) edges of length 1.&lt;/p&gt;

&lt;p&gt;Second, &lt;a href=&quot;https://en.wikipedia.org/wiki/Penny_graph&quot;&gt;penny graphs&lt;/a&gt; are graphs 
that can be represented by pennies on a table. More precisely, a graph is a 
penny graph, if it is possible to represent each vertex by a unit disk, and to 
place the disk in such a way that they touch only at the boundaries, and that 
each contact represents an edge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/pennies.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Penny graphs are matchsticks graphs but the reverse is not true.&lt;/p&gt;

&lt;p&gt;Finally consider the following experiment. 
Take a graph, replace edges by rods and nodes by hinges.
Now, if when you push on some side, the graph changes shape, then the graph is 
flexible, otherwise it is 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_rigidity&quot;&gt;rigid&lt;/a&gt;.
For example a cube is flexible, and a tetrahedron is rigid.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/rigide.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;breaking-symmetries-in-cells&quot;&gt;Breaking symmetries, in cells&lt;/h2&gt;

&lt;p&gt;A common challenge in distributed algorithms is breaking symmetry: 
if two processors have exactly the same view, they have to take the same step. 
For example, on a cycle, if the nodes do not have a way to break 
symmetry (for example unique identifiers), then they all have the same view, 
thus they take the same steps, and will output the same thing.&lt;/p&gt;

&lt;p&gt;Let us now take a look at biology. We know that an essential mechanism in living 
stuff is &lt;a href=&quot;https://en.wikipedia.org/wiki/Cell_division&quot;&gt;cell division&lt;/a&gt;, 
where a cell is basically replaced by two copies of itself. 
At the very beginning of a new organism, there is one cell, and then it divides 
again and again, and at the end you get a fully developed living being.&lt;/p&gt;

&lt;p&gt;From a distributed computing perspective, int his context,
it would be natural to have the exact same cell everywhere, with 
the same development. 
It is not the case: brain cells are not muscle cells. 
Ok, so a first reason it that there is the environment: two identical cells 
could evolve differently because the things around them are different. 
In some sense the environment breaks symmetry.
But if you think about an embryo, the environment is probably very similar on 
one side or the other, so at least at the beginning it should be just a heap of 
identical cells, and it is not the case. So, what’s going on?&lt;/p&gt;

&lt;p&gt;A quick look at wikipedia provides the answer: cell division does not produce 
identical cells. In particular, stem cells
&lt;a href=&quot;https://en.wikipedia.org/wiki/Asymmetric_cell_division&quot;&gt;divide asymmetrically&lt;/a&gt;.
The wikipedia article goes quickly into specific things, but from what I 
understand, that fact that the sperm cell fertilizes the egg cell “in one 
direction” gives a special 
direction, that influences the way the small stuff in the cell is distributed.
Thus what is inside the cell is not uniformly distributed. 
This asymmetry is then propagated at the level of the cells. 
This is because the division of a cell happens in such a way that the two cells 
have very different small stuff inside (because some proteins make this happen 
this way).&lt;/p&gt;

&lt;h2 id=&quot;discrete-analysis-and-light-bulbs&quot;&gt;Discrete analysis and light bulbs&lt;/h2&gt;

&lt;p&gt;Once in while I remember that the 
&lt;em&gt;&lt;a href=&quot;https://discreteanalysisjournal.com/&quot;&gt;Discrete Analysis&lt;/a&gt;&lt;/em&gt;
journal exists. 
It is one of these few journals that are really open access, 
in the sense of the &lt;a href=&quot;https://freejournals.org/&quot;&gt;Free journal network&lt;/a&gt;.
One can check the list of free journals 
&lt;a href=&quot;https://freejournals.org/current-member-journals/&quot;&gt;here&lt;/a&gt;.
The relevant journals for complexity/algorithms/graphs are
&lt;a href=&quot;https://dmtcs.episciences.org/&quot;&gt;DMTCS&lt;/a&gt;, 
&lt;a href=&quot;https://journals.carleton.ca/jocg/index.php/jocg&quot;&gt;JoCG&lt;/a&gt;, 
&lt;a href=&quot;http://jgaa.info/&quot;&gt;JGAA&lt;/a&gt;, 
&lt;a href=&quot;https://lmcs.episciences.org/&quot;&gt;LMCS&lt;/a&gt;, 
&lt;a href=&quot;https://digitalcommons.georgiasouthern.edu/tag/&quot;&gt;TAG&lt;/a&gt;, 
and 
&lt;a href=&quot;https://theoryofcomputing.org/introduction.html&quot;&gt;ToC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Discrete analysis&lt;/em&gt; is mainly about analytic methods in combinatorics, thus not 
exactly my cup of tea, but I know it from the blog of 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Timothy_Gowers&quot;&gt;Timothy Gowers&lt;/a&gt;. 
And once in a while, there is a paper that arouses my curiosity.&lt;/p&gt;

&lt;p&gt;The website is especially well-done for readers like me. 
Unlike usual journals with ugly websites, and just a list of papers, 
Discrete Analysis has a nice picture for each paper, a small page describing it,
with an introduction written by a member of the editorial board. All this is 
very nice, although at first sight it may look a bit too flashy.&lt;/p&gt;

&lt;p&gt;A paper that “makes the cover” of the website these days is 
&lt;a href=&quot;https://discreteanalysisjournal.com/article/2730-fixed-energy-harmonic-functions&quot;&gt;Fixed-energy harmonic functions&lt;/a&gt;.
In addition to the introduction, there is the video of a talk about the paper. 
It goes quickly into maths that I can’t understand without effort, but the first 
five minutes explain the problem in a fun fashion. 
Consider an electric circuit with light bulbs. 
The circuit is represented by a 
graph and every edge has a weight: the resistance of the light bulb of this 
section of the circuit. 
Now you can plug the wires of a battery at two arbitrary nodes of the graph and 
ask for the energy dissipated on each bulb, that is for the brightness of each 
bulb. This is a classic exercise in physics, and on reasonable circuits one 
just has to use simple laws of electricity and a lot of time. 
For example the video features this circuit, where all the resistances are equal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/ampoules.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The paper studies the reverse problem: given a graph, and two nodes to plug the 
battery, how to choose the resistance of the light bulbs such that they all 
shine with the same brightness?&lt;/p&gt;

&lt;h2 id=&quot;pie-rule&quot;&gt;Pie rule&lt;/h2&gt;

&lt;p&gt;In the &lt;a href=&quot;./march-2019-notes-1&quot;&gt;March notes&lt;/a&gt;, I mentioned some criteria for fair 
item assignment. One is well-known: the first player defines two bundles of items, 
the second player choose the bundle it prefers. This way the first player will 
design the bundles in the most balanced way. 
A more concrete setting is with a pie (hance the name of “pie rule”): 
the first player cuts the cake into two pieces, and 
the second player chooses the slice.&lt;/p&gt;

&lt;p&gt;I discovered that something similar exists for games such as chess, go, etc. 
In some of these two-players games, the first player has a great advantage. 
To mitigate this, one can use the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Pie_rule&quot;&gt;pie rule&lt;/a&gt;. Let say that we consider 
chess, and that A plays white, and B plays black. Then the pie rule 
says that after A has played the first move, B has the choice between:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;playing the normal way until the end&lt;/li&gt;
  &lt;li&gt;changing the color, that is taking the white side, and letting A do the first 
move for the black, and then playing the normal way until the end (with A playing 
black, and B playing white).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This way, A has an incentive to make a not-too-good move to start, which 
mitigates its first-player advantage.&lt;/p&gt;

&lt;h2 id=&quot;multiplicative-weight-update&quot;&gt;Multiplicative weight update&lt;/h2&gt;

&lt;p&gt;The 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Multiplicative_weight_update_method&quot;&gt;multiplicative weight update method&lt;/a&gt; 
is a general algorithmic method, that is not known enough, and that has been 
discovered several times in several contexts. 
The basic description is the following. Suppose you have several experts, that 
give you advice for a series of decisions that you take one by one. 
After each decision, you redesign the way you aggregate the advice to take a 
decision. Typically you want to give more weight to experts that have given good
advice.
The multiplicative weight update method basically states that updating the weight
in multiplicative manner is a good idea.
It seems pretty specific but it is actually very general.&lt;/p&gt;

&lt;p&gt;In addition to the wikipedia page and the references therein (including a survey 
by Arora, Hazan and Kale), you may want to take a look at a series of post that 
Luca Trevisan is currently writing 
on this topic. 
Actually he is mostly interested in the developments of this 
method that have been used in a variety of papers in complexity recently. 
The series of posts starts 
&lt;a href=&quot;https://lucatrevisan.wordpress.com/2019/04/17/online-optimization-for-complexity-theorists/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jeremy Kun also has a 
&lt;a href=&quot;https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/&quot;&gt;nice post&lt;/a&gt; 
about the method.&lt;/p&gt;

&lt;h2 id=&quot;maximum-matchings-that-are-not-perfect&quot;&gt;Maximum matchings that are not perfect&lt;/h2&gt;

&lt;p&gt;I recently had to think about graphs that do not have a perfect matching. 
The first example is of course graphs with an odd number of vertices. 
Next, you can find things like the one below (with a maximum non-perfect 
matching).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage-max.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Intuitively, in each triangle, one node must be matched with a node outside the 
triangle, but the three triangles are looking for the same node in the middle, 
and as only one can be served, the other triangles must have an unmatched 
node. 
Actually, even a star with three leaves is a good example.&lt;/p&gt;

&lt;p&gt;Remember &lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte_theorem&quot;&gt;Tutte theorem&lt;/a&gt;: 
A graph, $G = (V, E)$, has a perfect matching if and only if for every subset $U$ 
of $V$, the subgraph induced by $V − U$ has at most $|U|$ connected components with 
an odd number of vertices (definition from wikipedia). 
In the drawing above, take the middle node as $U$, you get three connected component 
with an odd number of vertices. As $3&amp;gt;1$, this graph doesn’t have perfect matching.&lt;/p&gt;

&lt;p&gt;Tutte theorem is actually a special case of the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte%E2%80%93Berge_formula&quot;&gt;Tutte–Berge formula&lt;/a&gt;, 
which states that the size of the maximum matching is: 
$\frac{1}{2} \min_{U\subseteq V}  \left(|U|-\operatorname{odd}(G-U)+|V|\right)$&lt;/p&gt;

&lt;p&gt;where $\operatorname{odd}$ counts the number of odd connected components.&lt;/p&gt;

&lt;h2 id=&quot;irreproducibility-and-unpublished-failures&quot;&gt;Irreproducibility and unpublished failures&lt;/h2&gt;

&lt;p&gt;A
&lt;a href=&quot;https://www.nature.com/articles/d41586-019-01307-2?utm_source=twt_nnc&amp;amp;utm_medium=social&amp;amp;utm_campaign=naturenews&amp;amp;sf211598052=1&quot;&gt;paper in Nature&lt;/a&gt; 
identifies the “four horsemen of irreproducibility” that harm science. 
One of them is the publication bias: studies that do not 
conclude that there is a relation between two things are not publishable. 
Two problems with this are: (1) researchers have an incentive to find
correlations, when they should have an incentive to find the truth, and (2) if 
because of statistics a false correlation can appear in 5% of the experiments, 
then only the wrong conclusion will be made public (after 19 teams on average 
have tried, and failed to find a correlation).&lt;/p&gt;

&lt;p&gt;There are efforts to fight this problem. One is the notion of
 &lt;a href=&quot;https://cos.io/rr/&quot;&gt;registered report&lt;/a&gt;. This consists in having the journal to
review an experiment (the topic, the hypothesis tested, the method), before the 
experiment is made. If this set-up is accepted, then the experiment is made, the 
paper written and it goes through a second review.
The paper is provisionally accepted after the first review.&lt;/p&gt;

&lt;p&gt;In TCS and maths in general, this publication bias is not a central problem, as 
one can check the proofs, and hopefully be convinced that the papers published 
are correct. Nevertheless, we probably loose a lot of time trying the same 
approaches on the same problems, without making our failures public.&lt;/p&gt;

&lt;h2 id=&quot;planes-etc&quot;&gt;Planes etc.&lt;/h2&gt;

&lt;p&gt;As a follow-up to the 
&lt;a href=&quot;https://discrete-notes.github.io/march-2019-notes-2&quot;&gt;note of last month&lt;/a&gt; about 
conference travel and its environmental impact, here are a few more elements.&lt;/p&gt;

&lt;p&gt;An &lt;a href=&quot;https://theconversation.com/chercheurs-donnez-lexemple-prenez-moins-lavion-110613&quot;&gt;article (in French)&lt;/a&gt; 
about flights of researchers, mentions several alternatives. One is to have 
purely virtual conferences (such as 
&lt;a href=&quot;https://ehc.english.ucsb.edu/?page_id=12687&quot;&gt;this one&lt;/a&gt;).
Another one is to have the conference distributed among multiple sites, for 
example one in each continent. Probably worth testing…
By the way, remember that we can already enjoy a virtual TCS seminar: 
&lt;a href=&quot;https://sites.google.com/site/plustcs/&quot;&gt;TCS+&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, it’s difficult to make efforts when you are almost alone to make them and 
that it even hurts you career. Maybe a charter or a code of conduct, for 
researchers of an institution or of a community would help.
For example an important Danish newpaper now has such a charter (no domestic 
flights (remember it’s Danemark), less flying in general, less advertising for 
far away destination etc.). I couldn’t find good press coverage of this in English, 
but &lt;a href=&quot;https://www.lemonde.fr/climat/article/2019/01/16/climat-plus-de-vols-interieurs-pour-les-journalistes-de-politiken_5409874_1652612.html?xtmc=politiken&amp;amp;xtcr=5&quot;&gt;here is an article&lt;/a&gt; in the main French newspaper.&lt;/p&gt;

</description>
        <pubDate>Thu, 23 May 2019 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///april-may-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///april-may-2019-notes</guid>
      </item>
    
      <item>
        <title>Algorithms and natural history</title>
        <description>&lt;p&gt;Until recently there was in Paris an exhibition about some techniques used for 
research in natural history.
It raises some algorithmic questions and comments about bones, diamonds and 
polytopes.
(More details about this great exhibition at the end of the post.)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/grenouille.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;Researchers measure the teeth of the tadpoles this frog 
species, and learn stuff.&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;computing-skull-volumes&quot;&gt;Computing (skull) volumes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/crane.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When you get a skull, something you want to measure is how much brain you can 
fit in, that is, what is its inside volume.
This used to be done by filling the skull with some kind of grains, and then 
measuring the volume of grain. 
It is now done by with sensor measurements and computations. 
But if I give you some triangulation of an object, how do you compute the volume?
If you assume the object to be convex, then one can surely do some kind of discrete 
integrals, suming some basic polytope volumes. 
&lt;a href=&quot;https://www.ams.org/journals/mcom/1991-57-195/S0025-5718-1991-1079024-2/S0025-5718-1991-1079024-2.pdf&quot;&gt;This paper&lt;/a&gt; 
(or more precisely its introduction) seems to validate this intuition. 
It also raises the question about how the object to measure is given.&lt;/p&gt;

&lt;p&gt;This made me remember that some price was awarded for something related to 
volume computation. 
I dug out the reference: 1991 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Fulkerson_Prize&quot;&gt;Fulkerson prize&lt;/a&gt; was awarded to 
Dyer, Frieze and Kannan for works of this flavour.
Their model is black-box: you ask for a point and you are answered whether it is 
inside or outside. 
They provide a randomized PTAS&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; for approximating
the volume of high-dimensional polytopes using 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&quot;&gt;Markov chain Monte Carlo&lt;/a&gt; 
methods. 
As many counting problems can be rephrased in terms of polytopes, this is much 
more useful than just computing high-dimensional skull volumes.
For a bit more on this, see 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_volume_approximation&quot;&gt;this wikipedia page&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;inclusions-ofdiamonds-and-polytopes&quot;&gt;Inclusions of diamonds and polytopes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/diamant-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The diamond above is the &lt;a href=&quot;Tavernier Blue&quot;&gt;Blue Diamond of the French Crown&lt;/a&gt; 
that Louis XIV bought at some point of the 17th century. 
Or actually, it’s what it used to look like, because this diamond disappeared. 
It was suspected that it has been recut and is now the diamond known as the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Hope_Diamond&quot;&gt;Hope diamond&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This was presented in the exhibition because the museum in Paris has a lead cast 
of the original diamond, and used it to validated the link between the two. 
Basically the two diamonds fit so well one in the other, that is very likely 
that they are the same with only a light recut.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/diamant-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But how can you decide if two polytopes could be included one in the other?
And what’s the complexity? (Note that contrary to skulls, diamonds are pretty 
much polytopes.)&lt;/p&gt;

&lt;p&gt;First for the decision problem: the two polytopes are somehow aligned and you 
just want to check the inclusion of one into the other.
If both polytopes are given by sets of vertices, then some methods similar to
&lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_hull_algorithms&quot;&gt;convex hull computations&lt;/a&gt; 
should do the job.&lt;/p&gt;

&lt;p&gt;Now how to find this alignment? For our diamonds, the spikes and planar 
symmetries surely help. But what if your polytopes are say, 
very close to spheres, but with random perturbations? Doesn’t seem very easy. 
This looks similar to some robotics problem, such as the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Motion_planning&quot;&gt;piano mover’s problem&lt;/a&gt;, but it’s 
not quite the same.&lt;/p&gt;

&lt;h2 id=&quot;symmetrizing-old-bones&quot;&gt;Symmetrizing old bones&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/dinosaures.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;350px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yet another problems that one can transfer to polytopes. 
Here the paleontologists find skulls like the one on the left (I didn’t take 
notes of the name of this fellow), that has been deformed by non-uniform forces 
in the ground.
Then they try to reconstruct the correct shape, like the one on the right.&lt;/p&gt;

&lt;p&gt;Again if you are a human, if you know a lot about such creatures, it is possible 
to find a plausible shape by adjusting the parameters of some transformation.
In particular, you try to have a left-right symmetry.
But what if I give you an arbitrary triangulation, and tell you: find the best 
way to modify it in order to have a symmetry?&lt;/p&gt;

&lt;p&gt;Could be related to 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;principal component analysis&lt;/a&gt;, 
that gives you some notion of “this object is somehow aligned with this 
direction”, but again if you have a random near-spherical objects, this does not 
help you (but in this case, you may be happy with any plane, as an approximation).&lt;/p&gt;

&lt;h3 id=&quot;about-the-exhibition&quot;&gt;About the exhibition&lt;/h3&gt;
&lt;p&gt;The name of the exhibition in French was 
&lt;em&gt;&lt;a href=&quot;https://www.mnhn.fr/en/node/5277&quot;&gt;Secrets dévoilés : voir l’imperceptible&lt;/a&gt;&lt;/em&gt;, 
which basically means &lt;em&gt;Unveiling secrets : seeing the imperciptible&lt;/em&gt;.
It was created by the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/National_Museum_of_Natural_History,_France&quot;&gt;National Museum of Natural History of Paris&lt;/a&gt;
and consisted in panels, hung in the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Jardin_des_plantes&quot;&gt;Jardin des Plantes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some of the pictures can be seen on the 
&lt;a href=&quot;https://marieducom.com/portfolio/exhibition-secrets-devoiles-voir-limperceptible/?lang=fr&quot;&gt;website of the illustrator&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;For PTAS and related acronyms, see &lt;a href=&quot;https://discrete-notes.github.io/october-batch-forgotten&quot;&gt;this post&lt;/a&gt;.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 09 May 2019 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///natural-history</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///natural-history</guid>
      </item>
    
      <item>
        <title>March notes II.</title>
        <description>&lt;p&gt;Second set of notes for March 2019.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/pivoines.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;a-pair-of-distributed-computing-talks&quot;&gt;A pair of distributed computing talks&lt;/h2&gt;

&lt;p&gt;We had a couple of distributed computing talks this month. Here are a few words 
about the concepts involved.&lt;/p&gt;

&lt;h3 id=&quot;beeping-model&quot;&gt;Beeping model&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;beeping model&lt;/em&gt; is a very weak model of distributed network computing.
In this model, the nodes of the network communicate by beeping. 
At each round, a node decides to beep or not. 
And a node can receive only one information: either someone in the neighborhood 
has beeped, or not. 
One cannot know how many nodes beeped. 
In particular, if the node chooses	 to beep, then it cannot know if another node 
has beeped.&lt;/p&gt;

&lt;p&gt;The beeping is used to model primitive communication, for example between cells.
The surprising point is that you can actually do things in this model !
For example compute a maximal independent set 
(see &lt;a href=&quot;http://www.cs.tau.ac.il/~afek/MISdisc.pdf&quot;&gt;this paper&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;superimposed-codes&quot;&gt;Superimposed codes&lt;/h3&gt;

&lt;p&gt;Beeps are binary information, thus to transmit more than one bit, nodes have to 
encode information into sequences of beeps. 
The problem is that a node may receive beeps from several neighbors, and a 
priori cannot know if beeps it heard form a single sequence, or if it is a 
mix of several sequences.&lt;/p&gt;

&lt;p&gt;A nice tool to solve that is &lt;em&gt;superimposed codes&lt;/em&gt;: these codes are such that if one superimpose 
them (that is, perform a bit-wise OR), one can still decode the different words. 
In other words any sequence can be decomposed uniquely into several code words.&lt;/p&gt;

&lt;p&gt;It seems that these codes were already used for punched-cards…&lt;/p&gt;

&lt;h3 id=&quot;fixing-blocking-edges-in-stable-marriage&quot;&gt;Fixing blocking edges in stable marriage&lt;/h3&gt;

&lt;p&gt;A marriage (or bipartite matching) is a matching in a balanced 
bipartite graph, where every node is has a preference list. 
It is &lt;em&gt;&lt;a href=&quot;Stable marriage problem&quot;&gt;stable&lt;/a&gt;&lt;/em&gt; if not two participants would prefer to 
be matched together instead of their current matching (such a pair is called a 
&lt;em&gt;blocking edge&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Gale and Shapley designed a well-known algorithm to compute such a marriage. 
But it starts from an empty solution. Suppose you want to start from a non-stable
solution and improve it step by step, for example because you are designing a
self-stabilizing algorithm. This is not easy.&lt;/p&gt;

&lt;p&gt;In the following example, 
attributed to &lt;a href=&quot;https://en.wikipedia.org/wiki/Donald_Knuth&quot;&gt;Knuth&lt;/a&gt;, 
we start on the left from 
a solution that is unstable (the blocking pair is in red), and solve this pair 
on the right picture, and then iterate the procedure. Unfortunately, at the end 
of the procedure, we are back to the first instance!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/mariages.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;privacy-in-stable-marriage&quot;&gt;Privacy in stable marriage&lt;/h3&gt;

&lt;p&gt;The classic centralized algorithms for stable marriage use the full list of 
preferences, that is there is no privacy for the participants. 
For distributed algorithms, it makes sense to try to minimize the amount of 
information the participants have to share to solve the task.
For example the algorithm presented in the talk uses little information 
(e.g. “this player is already
matched to someone higher in his/her list”).&lt;/p&gt;

&lt;p&gt;[The talks were by 
&lt;a href=&quot;https://sites.google.com/view/dufoulon/accueil&quot;&gt;Fabien Dufoulon&lt;/a&gt; and 
&lt;a href=&quot;https://www.lri.fr/~laveau/&quot;&gt;Marie Laveau&lt;/a&gt;, 
both PhD students in Paris-Sud University.
Fabien told us about his new (not yet public) result in the beeping model, 
using a variant of superimposed codes. 
Marie told us about her result on self-stabilizing 
stable matching. 
It is &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-01576055/document&quot;&gt;this paper&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;different-techniques-for-different-regimes-in-rockets-and-algorithms&quot;&gt;Different techniques for different regimes (in rockets and algorithms)&lt;/h2&gt;

&lt;p&gt;I discovered recently that the flames of different space rockets have different 
colors, partly because they use different mix of fuels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/rockets.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;550px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Space shuttle (yellowish flames) and Soyouz (pinkish flames).&lt;/p&gt;

&lt;p&gt;This was surprising to me, as I would naively think that there must be a solution 
that is considered “the best to push stuff upward”, and that everybody would use it.&lt;/p&gt;

&lt;p&gt;But I guess different regimes (how much you want to lift, at which speed etc.) 
ask for different solutions. 
In the world of algorithms, it is similar to the following scenario.
Say you want an algorithm for instances that depend on two parameters $n$ and 
$k$. Then you may use different technique for different regimes. 
For example, expressing $k$ as a function of $n$, you could have: for constant 
$k$ do brute-force, for logarithmic $k$ do some divide and conquer, for 
$k$ around $\sqrt{n}$ do dynamic programming, etc.&lt;/p&gt;

&lt;p&gt;I wonder if we have such examples with many different regimes asking for many 
different solutions. 
In distributed graph algorithms, an example is coloring 
(and similar problems) where for 
bounded degree and unbounded degree you have completely different techniques.&lt;/p&gt;

&lt;p&gt;For the related question of problems (with one parameter) having many different 
equally good solutions, I see $O(\log n)$-approximation of 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Set_cover_problem&quot;&gt;Set-Cover&lt;/a&gt;, for which you can 
basically use all the standard tools of approximation: greedy, LP-rounding, 
primal-dual etc.&lt;/p&gt;

&lt;p&gt;[I semi-randomly stumbled on 
&lt;a href=&quot;https://www.youtube.com/watch?v=EO_gwxon764&quot;&gt;this video&lt;/a&gt; (or more precisely 
the French-speaking version of it) that explains a lot of stuff about rockets 
flames. 
For example why one can see the kind of patterns below in rocket flames 
(it’s actually in the second part of the video).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/shock-diamonds.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;170px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Shock_diamond&quot;&gt;Mach disks&lt;/a&gt; or 
shock diamonds, and come from the fact that the flames expand when they get 
out of the engine, but because the outside pressure is higher, it is then 
recompressed, it heats up again, and then again 
expands etc. Maybe there is a discrete analogue for this too (convergence by 
oscillations?).]&lt;/p&gt;

&lt;h2 id=&quot;abel-prize-and-limitations-of-the-discrete-approach&quot;&gt;Abel prize and limitations of the discrete approach&lt;/h2&gt;

&lt;p&gt;Lipton and Regan 
&lt;a href=&quot;https://rjlipton.wordpress.com/2019/03/21/the-shortest-path-to-the-abel-prize/&quot;&gt;blog about&lt;/a&gt; 
the new Abel prize recipient, 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Karen_Uhlenbeck&quot;&gt;Karen Uhlenbeck&lt;/a&gt;, or actually about
her research. They take the interesting approach of first trying to solve a 
problem with a discrete natural approach, and then to show how it fails, and how 
analysis in the style of Uhlenbeck’s research solves the problem.&lt;/p&gt;

&lt;p&gt;The question is simply to prove that in the plane the shortest path between two 
points is a straight line. The natural discrete approach is to consider 
polygonal lines, to prove the statement by induction (on the picture below the 
blue line is better by triangle inequality), and then to say “every curve is 
polygonal line up to small stuff”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/uhlenbeck.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The reason why it fails is all the nasty functions (fractals and other monsters) 
that ruin the last step.&lt;/p&gt;

&lt;h2 id=&quot;sigact-news-year-and-conference-reviews&quot;&gt;SIGACT news, year and conference reviews&lt;/h2&gt;

&lt;p&gt;I was surprised to receive SIGACT news at home, and then remembered than I had 
registered to ACM for FOCS. It is first time I glance at SIGACT news, as I don’t
have access to it online.&lt;/p&gt;

&lt;p&gt;I was especially interested in the online algorithm review. 
For the first issue of the year, the author, 
&lt;a href=&quot;https://www.uni-siegen.de/fb6/aan/optimierung/mitarbeiter/vanstee/?lang=d&quot;&gt;Rob van Stee&lt;/a&gt;, 
does a review of the key results of 2018 in his area. 
This is very nice format, that gives a higher perspective on a field.&lt;/p&gt;

&lt;p&gt;A more classic format is the review of a conference. In my experience, this is 
less useful, because (1) one cannot be knowledgeable about every topic in a 
conference, (2) one tries to be quite exhaustive ; consequently 
the review often ends up being a list of small, not very informative
sentences about many many papers.&lt;/p&gt;

&lt;p&gt;[By the way, it’s sad that SIGACT News is not open-access.]&lt;/p&gt;

&lt;h2 id=&quot;point5-and-conference-travel&quot;&gt;1point5 and conference travel&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Le_Monde&quot;&gt;Le Monde&lt;/a&gt;, one of the major French 
newpapers, published 
&lt;a href=&quot;https://labos1point5.org/&quot;&gt;the letter of the “1point5” group&lt;/a&gt;. 
This name comes from the objective of keeping global warming
upper bounded by 1.5 degree Celsius. The collective is interested in the impact of 
research on climate. They basically argue that research should be made 
differently to reduce its impact on the planet.&lt;/p&gt;

&lt;p&gt;I have been thinking about this for a while. 
It happened to me several times to go 
to a conference, and think it was not worth the kerosene. 
Conferences are often nice moments, you indeed learn stuff and make new projects,
but it’s difficult to argue that this is as convincing as the following picture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/histogram-CO2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The precise numbers can be discussed, but I got them from very reliable sources,
and everywhere you will find numbers in the same ballpark.&lt;/p&gt;

&lt;p&gt;In the long term, we can hope to make conferences less numerous, more efficient, 
more local. But in the short, term it seems that the only solution is to avoid 
submitting to far away conferences. Obviously this is a very broad topic…&lt;/p&gt;

&lt;h2 id=&quot;place-in-a-queue&quot;&gt;Place in a queue&lt;/h2&gt;

&lt;p&gt;I recently went to a doctor who does not take appointments, thus people wait. 
The rule is “first-in first-cured” and people have to remember when it’s their 
turn.
It is usually fine, but sometimes, maybe by mistake, maybe by slyness, people 
disagree. 
I noticed that people in the room have somehow random sampling of the 
order, e.g. “I arrived before this person, that person was here before, these two 
I don’t know”.&lt;/p&gt;

&lt;p&gt;You get diagrams like this one (with some inconsistencies):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/queue-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Making this into a linear order has a feedback-arcset flavor.&lt;/p&gt;

&lt;p&gt;In other countries, I noticed that people, when they arrive say to a restaurant,
ask “who is the latest arrived?”, which gives a simpler picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/queue-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But maybe it’s less robust to slyness..?&lt;/p&gt;

</description>
        <pubDate>Tue, 09 Apr 2019 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///march-2019-notes-2</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///march-2019-notes-2</guid>
      </item>
    
      <item>
        <title>Lightning and search algorithms</title>
        <description>&lt;p&gt;I recently stumbled on &lt;a href=&quot;https://www.youtube.com/watch?v=nBYZpsbu9ds&quot;&gt;this video&lt;/a&gt;
of lightning. I knew that lightning kind of “search for the good way”, but 
I was stunned by how it looks like the run of a search algorithm: exploration 
phases, backtracking, compromise between depth and breadth etc.
For fun, I drew a lightning bolt, second by second. You may find it in the 
video around 0:25-0:35.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/eclair-1a.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-1b.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-2a.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-2b.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-3a.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-3b.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-4a.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-4b.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-5a.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-5b.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-6a.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;
&lt;img src=&quot;assets/eclair-6b.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 26 Mar 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///lightning</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///lightning</guid>
      </item>
    
      <item>
        <title>March notes I.</title>
        <description>&lt;p&gt;First half of the March 2019 notes.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;assets/mimosa-2.JPG&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;450px&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Mimosa in Jardin des Plantes.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;tverbergs-theorem&quot;&gt;Tverberg’s theorem&lt;/h2&gt;

&lt;p&gt;For geometric algorithms in the plane, one sometimes has go through case by case 
analysis along the lines of “if this point is below this line, then this and 
that intersect, if not then…”. 
Such proofs are a bit boring and it is easy to make mistakes. 
So when you have a general theorem that does the work for you, it’s good news. 
Tverberg’s theorem looks like a theorem that could be useful from this point of 
view.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tverberg%27s_theorem&quot;&gt;Tverberg’s theorem&lt;/a&gt; states 
a condition such that the convex hulls of several sets of points intersect. 
Precisely: in a $d$-dimensional euclidian space, for any integer $r$, for any 
set of $(d+1)(r-1)+1$ points, there exists a partition in $r$ subsets such that 
the convex hulls of the $r$ subsets intersect. 
The picture below present the case $d=2$ and $r=3$, for $(d+1)(r-1)+1=7$ points, 
and for one less point. 
In this last case the theorem does not hold (we just show one partition).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/tverberg.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;270px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[I saw the name of this theorem on the SOCG accepted papers list, unfortunately
the paper is not available online, and we don’t know what they do with this 
theorem.]&lt;/p&gt;

&lt;h2 id=&quot;criteria-for-fair-item-assignment&quot;&gt;Criteria for fair item assignment&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fair_item_assignment&quot;&gt;Fair item assignement&lt;/a&gt; 
consists, given items and participants, to assign the items to the participants 
in a fair manner, for some definition of fair. There are many variants of the 
problem and many ways to define fairness.&lt;/p&gt;

&lt;p&gt;Here are a few fairness criteria, to decide if the assignment is fair or not:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Max-min fair-share (“I cut, you choose”): every participant is least as happy 
as if she had cut the set of items first, and then let all the other 
participants choose before her.&lt;/li&gt;
  &lt;li&gt;Max-min fair-share (“You cut, I choose”): the same but reversed, that is every
participant is at least as happy as if someone else did the cut, and she could 
decide first.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Envy-freeness&quot;&gt;Envy-freeness&lt;/a&gt;: 
no participant would like to change her items for the items of 
another participant.&lt;/li&gt;
  &lt;li&gt;Competitive equilibrium from equal incomes: there exists a price for each item
such that, given the same budget, the participants would choose different items.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[I learnt about this at the CS PhD seminar of Sorbonne university, by Parham 
Shams.]&lt;/p&gt;

&lt;h2 id=&quot;tai-chi-problem&quot;&gt;Tai chi problem&lt;/h2&gt;

&lt;p&gt;When crossing the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Jardin_des_plantes&quot;&gt;Jardin des plantes&lt;/a&gt; 
I often witness the following kind of scene.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/taichi.JPG&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is people practising &lt;a href=&quot;https://en.wikipedia.org/wiki/Tai_chi&quot;&gt;taichi&lt;/a&gt;. 
There is a professor in front of the group (with a white cap on the picture)
who is showing the movements, and everybody is supposed to perform them in 
real-time (it’s pretty slow).&lt;/p&gt;

&lt;p&gt;The thing is: people on the first row can see the professor, but the ones on the 
back cannot, so they follow the moves of the people on the rows closer to the 
professor. Therefore there is a kind of wave from the front to the back. 
Maybe it can make an interesting problem: how fast does the wave propagate, is 
it close to a straight line, how does it depend on the size of the participants?&lt;/p&gt;

&lt;h2 id=&quot;core-algorithms-deployed&quot;&gt;Core algorithms deployed&lt;/h2&gt;

&lt;p&gt;It is common as a reseacher to have doubt about how useful your research is. 
In my case, I sometimes wonder whether the algorithmic problem we try to solve 
really have an impact. 
It could well be the case that the algorithmic problems that 
are really useful are the ones that have only basic answers.
That is, we focus on the sweet spot between trivially easy and trivially hard 
problems, and this set could have an empty intersection with real-world 
questions.&lt;/p&gt;

&lt;p&gt;I was really happy to discover a few years ago 
&lt;a href=&quot;https://cstheory.stackexchange.com/questions/19759/core-algorithms-deployed&quot;&gt;the topic “core algorithms deployed”&lt;/a&gt;
on stack exchange,
that removes any doubt on this regard.&lt;/p&gt;

&lt;h2 id=&quot;numerical-errors-in-missile-trajectories&quot;&gt;Numerical errors in missile trajectories&lt;/h2&gt;

&lt;p&gt;I discovered in the introduction of
&lt;a href=&quot;https://www.lri.fr/~melquion/doc/19-hdr.pdf&quot;&gt;this  PhD thesis&lt;/a&gt; the following 
story, illustrating why computer arithmetics is important and difficult.&lt;/p&gt;

&lt;p&gt;In 2011, a US “&lt;a href=&quot;https://en.wikipedia.org/wiki/Missile_defense&quot;&gt;defence missile&lt;/a&gt;” 
missed a “real missile”, with tragic consequences, 
because of computer arithmetic. 
The basic reason is the following. 
There used to be a software doing the trajectory computations, with rounding 
operations, including on the time variable. This  variable slowly shifted 
compared to the real time because of these rounding. 
It sounds dangerous, but it was actually ok: 
the trajectory computations would consider fast movements 
so between the start and end of the trajectory, the shift was negligible, and 
the absolute time did not matter. The problem is that part of the system was 
updated with more accurate time variable. Then the shift would not be the same 
in two different parts of the system. Then the shift would not cancel out and 
this was the bug.&lt;/p&gt;

&lt;p&gt;[A better explanation can be found in 
&lt;a href=&quot;http://www-users.math.umn.edu/~arnold/disasters/Patriot-dharan-skeel-siam.pdf&quot;&gt;this document&lt;/a&gt;.
The PhD thesis has a more light-hearted story about gamers trying to finish a 
video game level without jumping, and using a slow shift of a platform 
(also due to rounding errors) to do so.]&lt;/p&gt;

&lt;h2 id=&quot;max-cut-and-planar-graphs&quot;&gt;Max cut and planar graphs&lt;/h2&gt;

&lt;p&gt;Two results about &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_cut&quot;&gt;max-cut&lt;/a&gt; that I 
didn’t know:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a cut is maximum if and only if its complement is a minimum odd-circuit cover 
(this is easy to check, see for example 
&lt;a href=&quot;https://web.engr.oregonstate.edu/~glencora/wiki/uploads/planar-max-cut.pdf&quot;&gt;here&lt;/a&gt;,
or simply stare a bit at the picture below.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be used to obtain a polynomial-time algorithm for max-cut in planar 
graphs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/maxcut.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[I got aware of this by
&lt;a href=&quot;https://arxiv.org/abs/1903.06061&quot;&gt;this preprint&lt;/a&gt;
which presents an algorithm for max-cut whose complexity is parametrized by the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Crossing_number_(graph_theory)&quot;&gt;number of crossing&lt;/a&gt; 
in the drawing of the graph.]&lt;/p&gt;

</description>
        <pubDate>Mon, 25 Mar 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///march-2019-notes-1</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///march-2019-notes-1</guid>
      </item>
    
      <item>
        <title>STOC and SOCG picks</title>
        <description>&lt;p&gt;A few weeks ago the accepted papers lists of 
&lt;a href=&quot;http://acm-stoc.org/stoc2019/&quot;&gt;STOC&lt;/a&gt; 
and 
&lt;a href=&quot;http://eecs.oregonstate.edu/socg19/&quot;&gt;SOCG&lt;/a&gt; 
were made public. 
A bunch of titles caught my attention, and here are a few bits of information on
some papers I could find online.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/saucisses.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;350px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;testing-graphs-in-vertex-distribution-free-models&quot;&gt;Testing graphs in vertex-distribution-free models&lt;/h2&gt;

&lt;p&gt;Graph &lt;a href=&quot;https://en.wikipedia.org/wiki/Property_testing&quot;&gt;property testing&lt;/a&gt; 
basically consists in deciding if a graph has a property 
or not, by looking only at some parts of it. 
More precisely one queries a few nodes, and ask for their neighbors for example, 
and then outputs whether the graph has the property or is far from having 
it. Note that such a statement can only be true with some probability.
An introduction to graph property testing is 
&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~/oded/COL/tgp-intro.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the classic model, one is allowed to query a random node, with “random” 
meaning “uniformly at random”. In a new 
&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~/oded/VO/vdf.pdf&quot;&gt;paper&lt;/a&gt;
the author considers the case where the random access is not uniform but depends 
on an arbitrary distribution. This in turn implies a change in the definition of 
being far from a property.&lt;/p&gt;

&lt;p&gt;I didn’t dive in the paper, but I imagine the following scenario. 
Some nodes are more “important” than others. This translates into two things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It’s not a very big deal if you don’t detect that something is wrong around
an unimportant node.&lt;/li&gt;
  &lt;li&gt;Your random queries have more chance to visit an important node than an 
unimportant node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, you have a dynamic graph, you may not know about newly arrived 
nodes, but these are less important, so it’s no big deal if you don’t query them
now.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~/oded/VO/vdf.pdf&quot;&gt;Testing Graphs in Vertex-Distribution-Free Models&lt;/a&gt;&lt;/em&gt;,
by &lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~oded/&quot;&gt;Oded Goldreich&lt;/a&gt;, 
and will appear at STOC 2019.
It was listed in 
&lt;a href=&quot;https://ptreview.sublinear.info/?p=1044&quot;&gt;October 2018 property testing review&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;transportation-problem&quot;&gt;Transportation problem&lt;/h2&gt;

&lt;p&gt;The transportation problem is the following. 
Given a graph, where each node $v$ is given a (positive or negative) supply 
$\mu(v)$, such that $\sum_v \mu(v)=0$, one has to find a flow to transport the 
supplies in order to reach the configuration where every node has supply 0. 
See the picture below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/transportation.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is very similar to the 
&lt;a href=&quot;Transportation theory (mathematics)&quot;&gt;optimal transport problem&lt;/a&gt; in non-discrete 
mathematics. The &lt;a href=&quot;https://arxiv.org/pdf/1902.08384.pdf&quot;&gt;paper&lt;/a&gt; presents an 
approximation algorithm for fixed dimensions via continuous optimization.&lt;/p&gt;

&lt;p&gt;I don’t know how much continuous optimization is used, but it reminds me of an 
excellent invited talk by 
&lt;a href=&quot;https://people.csail.mit.edu/madry/&quot;&gt;Aleksander Mądry&lt;/a&gt; at 
&lt;a href=&quot;http://2016.highlightsofalgorithms.org/&quot;&gt;HALG 2016&lt;/a&gt;, that convinced me that 
continuous optimization can be a good approach to solve combinatorial problems.
There is no video of the talk, but 
&lt;a href=&quot;https://www.youtube.com/watch?v=noRNcDbqtVY&quot;&gt;this one&lt;/a&gt;
seems pretty close.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1902.08384.pdf&quot;&gt;Preconditioning for the Geometric Transportation Problem&lt;/a&gt;&lt;/em&gt;
by &lt;a href=&quot;http://www.math.toronto.edu/khesin/&quot;&gt;Boris A. Khesin&lt;/a&gt;,
&lt;a href=&quot;http://www.cs.toronto.edu/~anikolov/&quot;&gt;Aleksandar Nikolov&lt;/a&gt;,
and Dmitry Paramonov, and will appear at SOCG.]&lt;/p&gt;

&lt;h2 id=&quot;lp-roundings-iterated-meets-randomized&quot;&gt;LP roundings: iterated meets randomized&lt;/h2&gt;

&lt;p&gt;A classic approach in combinatorial optimization is to express the problem as a 
linear program. 
Unfortunately, only the fractional case is known to be solvable 
efficiently, and only the integer solutions can be transfered back to solution 
for the original problem. 
A solution is to compute a fractional solution and then to round it to an integer 
solution. This approach is very powerful for approximation algorithms.&lt;/p&gt;

&lt;p&gt;There are two important types of rounding: randomized rounding and iterated 
rounding. 
In &lt;a href=&quot;https://en.wikipedia.org/wiki/Randomized_rounding&quot;&gt;randomized rounding&lt;/a&gt;, 
a fractional variable $x_i$ is simply rounded to 1 
with probability $x_i$. 
In iterated rounding, one iteratively modify the fractional solution until it 
gets to an integral solutions (for example adding some small amount to a set of 
variables, until they reach 1, etc.). 
Both roundings have their pros and cons.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.01597.pdf&quot;&gt;The paper&lt;/a&gt; presents a method that 
combines the two approaches into one common framework, and shows how to use it.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.01597.pdf&quot;&gt;On a generalization of iterated and randomizedrounding&lt;/a&gt;&lt;/em&gt;
by &lt;a href=&quot;https://www.win.tue.nl/~nikhil/&quot;&gt;Nikhil Bansal&lt;/a&gt;, and will appear at STOC.]&lt;/p&gt;

&lt;h2 id=&quot;queue-layout&quot;&gt;Queue layout&lt;/h2&gt;

&lt;p&gt;Given a graph, a $k$-queue layout is an ordering of the vertices and a partition 
of the
edges into $k$ sets, such that there are not two edges of the same partition, 
that are nested. 
Below is an example of a graph and a 2-queue layout of it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/queue-layout.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Queue_number&quot;&gt;queue number&lt;/a&gt; of a graph is 
the minimum $k$ such that there exists a $k$ layout of the graph. 
An important open question is whether planar graph have bounded queue number. 
&lt;a href=&quot;https://arxiv.org/pdf/1811.00816.pdf&quot;&gt;The paper&lt;/a&gt; makes a step towards a positive
answer, by proving that planar graphs with bounded degree have bounded queue 
number.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.00816.pdf&quot;&gt;Planar Graphs of Bounded Degree have Constant Queue Number&lt;/a&gt;&lt;/em&gt;
by 
&lt;a href=&quot;http://algo.inf.uni-tuebingen.de/?site=mitarbeiter/michaelbekos/index&quot;&gt;Michael A. Bekos&lt;/a&gt;, 
&lt;a href=&quot;http://www-pr.informatik.uni-tuebingen.de/?site=mitarbeiter/henryfoerster/index&quot;&gt;Henry Förster&lt;/a&gt;,
&lt;a href=&quot;https://informatik.uni-koeln.de/ls-juenger/people/gronemann/&quot;&gt;Martin Gronemann&lt;/a&gt;, 
&lt;a href=&quot;https://i11www.iti.kit.edu/en/members/tamara_mchedlidze/index&quot;&gt;Tamara Mchedlidze&lt;/a&gt;,
&lt;a href=&quot;http://mozart.diei.unipg.it/montecchiani/&quot;&gt;Fabrizio Montecchiani&lt;/a&gt;, 
Chrysanthi Raftopoulou, and
&lt;a href=&quot;https://i11www.iti.kit.edu/en/members/torsten_ueckerdt/index&quot;&gt;Torsten Ueckerdt&lt;/a&gt;.
It will appear at STOC (even though the topic is very SOCG-friendly).
I didn’t know about queue layouts, and it’s good to discover it, as it is yet 
another example of graph parameter that correspond to a pattern, in the sense of 
&lt;a href=&quot;https://arxiv.org/abs/1812.05913&quot;&gt;this paper&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;reachability-of-petri-nets&quot;&gt;Reachability of Petri nets&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Petri_net&quot;&gt;Petri nets&lt;/a&gt; are very common in 
theoretical computer science, for 
example in &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_checking&quot;&gt;model checking&lt;/a&gt;. 
Nevertheless, it is horribly hard to decide the reachability problem for this 
model: can you get from a configuration $A$ to a configuration $B$?
The decidability of the problem was proved only in the 80s, and the best 
upper bound is non-primitive recursive cubic-Ackermannian (don’t ask what it 
means, exponential space is already scaring me).&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/1809.07115.pdf&quot;&gt;paper&lt;/a&gt; proves that the problem is 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Nonelementary_problem&quot;&gt;non-elemetary&lt;/a&gt;, that is, 
its time complexity cannot be bounded by a power tower.&lt;/p&gt;

&lt;p&gt;[The paper is 
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1809.07115.pdf&quot;&gt;The Reachability Problem for Petri Nets is Not Elementary&lt;/a&gt;&lt;/em&gt;
by &lt;a href=&quot;https://www.mimuw.edu.pl/~wczerwin/&quot;&gt;Wojciech Czerwiński&lt;/a&gt;,
&lt;a href=&quot;https://mimuw.edu.pl/~sl/&quot;&gt;Sławomir Lasota&lt;/a&gt;,
&lt;a href=&quot;https://warwick.ac.uk/fac/sci/dcs/people/ranko_lazic&quot;&gt;Ranko Lazić&lt;/a&gt;,
&lt;a href=&quot;https://www.labri.fr/perso/leroux/&quot;&gt;Jérôme Leroux&lt;/a&gt; and 
&lt;a href=&quot;https://www.labri.fr/perso/fmazowiecki/&quot;&gt;Filip Mazowiecki&lt;/a&gt;; and will appear at STOC.
In France, &lt;a href=&quot;http://www.lsv.fr/~schmitz/index.html.en&quot;&gt;Sylvain Schmitz&lt;/a&gt; is one 
of the researcher working on this kind of stratospherical complexities. 
&lt;a href=&quot;http://www.lsv.fr/~halfon/&quot;&gt;Simon Halfon&lt;/a&gt; one his PhD students at the time, 
gave an excellent talk on this topic at the 
&lt;a href=&quot;https://www.irif.fr/en/seminaires/doctorants/index&quot;&gt;IRIF PhD seminar&lt;/a&gt; in 2017.]&lt;/p&gt;

&lt;h2 id=&quot;quartet-distance-and-4-cycles&quot;&gt;Quartet distance and 4-cycles&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Quartet_distance&quot;&gt;quartet distance&lt;/a&gt; is a 
distance between phylogenetic trees. More precisely it is a distance between two 
unrooted trees with labeled leaves. It basically takes all the tuples of four 
leaves and count how many of them are in different topology in the two trees.&lt;/p&gt;

&lt;p&gt;For example in the picture below, for the two trees on the left, we want to 
decide whether the leaves $a$, $b$, $c$ and $d$ are in the same topology or not. 
We simplify the tree until we have only these leaves, and we see on the pictures
on the right&lt;br /&gt;
that it is not the case. Thus this quartet will add one to the distance. 
(Note that on the left, all leaves should be labeled, but I indicate only the 
ones we are interested in).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/quartet.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/1811.06244.pdf&quot;&gt;paper&lt;/a&gt; shows that computing the 
problem of computing 
distance between two trees is equivalent to the problem of computing the number 
of 4-cycles in a graph, up to 
polylogarithmic factors. This implies better algorithms and better insights
on the complexity.&lt;/p&gt;

&lt;p&gt;[The paper is
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.06244.pdf&quot;&gt;Computing Quartet Distance is Equivalent to Counting 4-Cycles&lt;/a&gt;&lt;/em&gt;
 by Bartłomiej Dudek (whom I have had the chance to meet in EPFL), 
and &lt;a href=&quot;https://sites.google.com/a/cs.uni.wroc.pl/gawry/&quot;&gt;Paweł Gawrychowski&lt;/a&gt;, and 
it
will appear at STOC.]&lt;/p&gt;

</description>
        <pubDate>Wed, 13 Mar 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///stoc-socg-picks</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///stoc-socg-picks</guid>
      </item>
    
      <item>
        <title>February notes</title>
        <description>&lt;p&gt;Notes for February 2019.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/lierre.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;minorities-in-network&quot;&gt;Minorities in network&lt;/h2&gt;

&lt;p&gt;I attended a talk about minorities in network,
by &lt;a href=&quot;http://claudiawagner.info/&quot;&gt;Claudia Wagner&lt;/a&gt; at the 
&lt;a href=&quot;http://www.complexnetworks.fr/events/&quot;&gt;Complex network seminar&lt;/a&gt; of Sorbonnes 
University. 
There was a lot of content, here are a few things I noted.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Contrary to what I thought, a &lt;a href=&quot;https://en.wikipedia.org/wiki/K-core&quot;&gt;$k$-core&lt;/a&gt; 
is not another name for a $k$-clique. 
A $k$-core of a graph is a maximal connected subgraph where all the nodes have 
degree at least $k$. It can have much more nodes than just $k$
(a $k$-regular graph is its own $k$-core). Such subgraphs can be considered as 
coherent communities in social networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A part of the talk was about minorities in wikipedia. 
One would like to consider statements such as “women are more 
linked to other women than to men”. 
This is not true in general because there are more men pages, thus 
links have more chance to point to men, but could still be true, proportionally.
But it’s a bit too rough to just look at proportions because the graph may be 
complicated and there might be many correlations going on.
One way to deal with this is following: consider the graph of wikipedia 
bibliographies, note the global gender proportions, then erase the gender of the 
nodes, and reassign them at random, keeping the right proportions. 
Now you can compare the neighbourhoods in this new graph and in the original 
graph, and try to understand what’s going on. One paper on the topic is the following: 
&lt;a href=&quot;https://arxiv.org/pdf/1501.06307.pdf&quot;&gt;It’s a Man’s Wikipedia? Assessing Gender Inequality in an Online Encyclopedia&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A notion known as &lt;em&gt;Burt efficiency&lt;/em&gt;, or &lt;em&gt;brokerage&lt;/em&gt;, or 
&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_holes&quot;&gt;structural hole&lt;/a&gt;&lt;/em&gt;, is more or 
less the following. 
A node that belongs to (or is close to) two clusters in a 
network, can have an advantage over the other nodes,
because it can enjoy the information gathered by both communities, and 
can choose to transfer or not such information. 
One can define coefficients to measure if a node is or not in such a 
position.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-stage-optimization&quot;&gt;Multi-stage optimization&lt;/h2&gt;

&lt;p&gt;For dynamic algorithms, one is usually concerned with having a good solution
 at any time, but these solutions do not need to be related.
Multi-stage optimization, introduced in 
&lt;a href=&quot;https://arxiv.org/abs/1404.3768&quot;&gt;this paper&lt;/a&gt;, considers the cases where one 
should not change the solution too much between the two steps. 
In other words, in this framework one maximizes the quality of the solution, 
while minimizing the churn.&lt;/p&gt;

&lt;p&gt;[I stumble on the notion in &lt;a href=&quot;https://arxiv.org/abs/1901.11260&quot;&gt;this preprint&lt;/a&gt;.]&lt;/p&gt;

&lt;h2 id=&quot;conjecture&quot;&gt;1-2-3 Conjecture&lt;/h2&gt;

&lt;p&gt;Another edition of the Complex Network seminar by 
&lt;a href=&quot;http://www.labri.fr/index.php?n=Annuaires.Profile&amp;amp;id=Senhaji_ID1441185629&quot;&gt;Mohammed Senhaji&lt;/a&gt; 
(that I couldn’t attend) was about the 1-2-3 conjecture, which is the following.&lt;/p&gt;

&lt;p&gt;In any graph, one can label the edges with label 1, 2, or 3, such that, when each node 
computes the sum of the labels of its adjacent labels, not two neighbours have 
the same sum.&lt;/p&gt;

&lt;p&gt;A survey about the conjecture is &lt;a href=&quot;https://arxiv.org/pdf/1211.5122.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;lovszs-new-book&quot;&gt;Lovász’s new book&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://fr.wikipedia.org/wiki/L%C3%A1szl%C3%B3_Lov%C3%A1sz&quot;&gt;László Lovász&lt;/a&gt; 
wrote a new book: 
&lt;a href=&quot;http://web.cs.elte.hu/~lovasz/bookxx/geombook2019-01-20.pdf&quot;&gt;Graphs and geometry&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;vandermonde-identity&quot;&gt;Vandermonde identity&lt;/h2&gt;

&lt;p&gt;Vandermonde’s identity is the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{m+n}{r} = \sum_{k=0}^{r}\binom{m}{k} \binom{n}{r-k}.&lt;/script&gt;

&lt;p&gt;I thought it was only a bachelor exercise, until it naturally popped up in the 
calculation in &lt;a href=&quot;https://arxiv.org/pdf/1812.09120.pdf&quot;&gt;a recent paper&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 05 Mar 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///february-2019-notes</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///february-2019-notes</guid>
      </item>
    
      <item>
        <title>Simulation argument IV. Maximal matching</title>
        <description>&lt;p&gt;This is the fourth and last post of a series about the simulation argument, that 
started with &lt;a href=&quot;./simulation-1&quot;&gt;this post&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;assets/puzzle-4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this post we tackle the maximal matching problem. 
Or actually we will show 
why establishing a lower bound for this problem is not as easy as for the 
sinkless orientation problem. 
The real lower bound is in &lt;a href=&quot;https://arxiv.org/abs/1901.02441&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;encoding&quot;&gt;Encoding&lt;/h3&gt;

&lt;p&gt;A maximal matching is a set of edges of the graph, such that no two of these
edges are adjacent, and no two unmatched node are linked by an edge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A natural encoding for this problem would be to label the edges of the matching 
with a label $A$, and edges not in the matching with a label $B$. 
But this does not work for our setting. 
Instead, we will do the following. 
(As in the previous post we will deal with 2-colored trees.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The edges of the matching are labeled with the label $M$.&lt;/li&gt;
  &lt;li&gt;If a white node is not matched then its edges are labeled with $P$. 
These are pointers to matched black nodes.&lt;/li&gt;
  &lt;li&gt;The remaining edges are labeled with $O$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the following picture, red edges are for $M$, green edges are for $P$, and 
blue edges are for $O$ (the missing edge should be blue!).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The languages we use are then described by the following polynomials:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$L_W = MO^{\Delta-1}+P^{\Delta}$,&lt;/li&gt;
  &lt;li&gt;$L_B = M(P+O)^{\Delta-1} + O^{\Delta}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformation&quot;&gt;Transformation&lt;/h2&gt;

&lt;h3 id=&quot;simulation&quot;&gt;Simulation&lt;/h3&gt;

&lt;p&gt;We perform the simulation on a black node $u$, and get a polynomial $P_u$.&lt;br /&gt;
By definition $P_u\subseteq (M+O+P)^{\Delta}$, and is factorized.&lt;/p&gt;

&lt;h3 id=&quot;product-property&quot;&gt;Product property&lt;/h3&gt;

&lt;p&gt;As the product rule applies, we know that
$P_u \subseteq M(P+0)^{\Delta-1}+O^{\Delta}$.&lt;br /&gt;
Thus there is at most one factor of $P_u$ with an $M$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Claim 1&lt;/em&gt;: This factor is either $(M)$ or $(M+O)$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: Suppose the factor with $M$ has a $P$. 
Then it means that when we develop $P_u$,
there are two monomials $M\times K$ and $P\times K$, with $K$ a polynomial
of degree $\Delta-1$, that does not contain an $M$. 
But the monomial $P\times K$ is not included in 
$M(P+0)^{\Delta-1}+O^{\Delta}$. which contradicts the product property.&lt;/p&gt;

&lt;p&gt;Then we are left with 5 possible sums as factors in $P_u$: 
$(M), (O), (P), (M+O), (P+0)$.&lt;/p&gt;

&lt;p&gt;Now it seems that we cannot get much more out of our properties, so let’s try a 
simplification step.&lt;/p&gt;

&lt;h3 id=&quot;tentative-simplification-step&quot;&gt;(Tentative) Simplification step&lt;/h3&gt;

&lt;p&gt;If we can map $(M+O)$ and $(P+O)$ to a simple label, 
and still match the language, 
then we are done, and we can conclude like in the &lt;a href=&quot;./simulation-3&quot;&gt;previous post&lt;/a&gt;. 
But this is not going to happen.&lt;/p&gt;

&lt;p&gt;Suppose you are in following situation, which is supposed to be easy because 
there is not even a $(P+O)$. (On the picture, every black node writes on its 
adjacent edges.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/couplage-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ok, let’s just take one of the two edges labeled with $M+O$
(let say the one with the smallest port-number on the white node), 
label it with $M$, label the other one with $O$, and we are done.&lt;/p&gt;

&lt;p&gt;This does notwork.
The problem is the one we highlighted in the &lt;a href=&quot;./simulation-2&quot;&gt;second post&lt;/a&gt;: 
the edges are not uniformly set-labeled by all the nodes. 
A concrete bad case:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;let $(u,v)$ and $(v,w)$ be the two edges labeled by $(M+0)$&lt;/li&gt;
  &lt;li&gt;in its simulation, $u$ has (M+O) for both edges, and based on port-numbers, 
decides that $(u,v)$ gets $M$.&lt;/li&gt;
  &lt;li&gt;in its simulation $w$ has $(M+O)$ for $(v,w)$, but only $(O)$ for $(u,v)$. Thus 
it labels $(v,w)$ with $M$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To solve this problem you have to synchronize, and this is forbidden as it takes 
extra time.&lt;/p&gt;

&lt;h2 id=&quot;no-lower-bound-this-way&quot;&gt;No lower bound this way&lt;/h2&gt;

&lt;p&gt;So at the end, we cannot conclude like in the case of sinkless orientation.&lt;/p&gt;

&lt;p&gt;Comments on that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This makes sense: actually there is a $O(\Delta)$ 
algorithm for this problem, thus no $\Omega(\log n)$ lower bound exists.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The above alone does not prove that there is no $\Omega(\log n)$ lower 
bound: we could use another definition of the problem, or we could have tried 
more exotic label replacement ($M$ transformed into $P$, or whatever).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you want to prove a $\Omega(\Delta)$ lower bound then you need to be 
smarter. In particular you need that, after the transformation, the language is 
different. Basically there should be a parameter that starts with something like 
$\Delta$ and decreases at each transformation. 
For that, see &lt;a href=&quot;https://arxiv.org/abs/1901.02441&quot;&gt;the paper&lt;/a&gt;!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Thanks to 
&lt;a href=&quot;https://www.lix.polytechnique.fr/~mrabie/&quot;&gt;Mikaêl Rabie&lt;/a&gt; 
for reading and improving this series of posts!&lt;/p&gt;

</description>
        <pubDate>Fri, 15 Feb 2019 00:00:00 +0100</pubDate>
        <link>https://discrete-notes.github.io///simulation-4</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///simulation-4</guid>
      </item>
    
  </channel>
</rss>
