<p><img src="assets/nueva-constitucion.jpg" alt="" class="center-image" height="600px" /></p>
<p align="center"><small><i>
</i></small></p>
<hr />

<h2 id="network-creation-game">Network creation game</h2>

<p>Network generation models are mechanisms to create networks. 
In a classic setting<br />
the nodes arrive one after the other and are linked to nodes already in 
the network following some rules. 
In another setting, called <em>network creation game</em> the nodes are players, 
and they play a game whose choices are to link or not to other nodes. 
The outcome of  the game is a network. 
The cost that a player pays is $\alpha$ for every node it decides to be 
linked to, plus 
the sum of the distances from this node to all the other nodes. 
In other words, a node wants to have short distance to every node, but 
cannot add a link to every node, because it would be too expensive.</p>

<p>For this game one can study the usual objects of 
<a href="https://en.wikipedia.org/wiki/Algorithmic_game_theory">algorithmic game theory</a>:
the <a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a> 
and the
<a href="https://en.wikipedia.org/wiki/Price_of_anarchy">price of anarchy</a>.</p>

<p>It is an open problem to show that the price of anarchy is constant for 
any value $\alpha$.</p>

<p><a href="https://arxiv.org/abs/1909.09799">This recent preprint</a> makes progress 
on the conjecture.</p>

<h2 id="lempel-ziv-compression-algorithms">Lempel-Ziv compression algorithms</h2>

<p>Lempel Ziv algorithm is a classic (theoretical) compression algorithm. 
A <a href="https://semidoc.github.io/lagarde-catastrophe">blog post on Semidoc</a> 
describes the algorithm and give an overview of 
<a href="https://arxiv.org/abs/1707.04312">this paper</a> which studies how the compression
rate can change when the original text is changed by one bit.</p>

<p>Two recent papers on arxiv deal with Lempel-Ziv:</p>

<ul>
  <li>
    <p><a href="The first one">https://arxiv.org/pdf/1910.00941.pdf</a> gives a new analysis of 
the fact that Lempel Ziv is optimal for some models of random text (hidden 
Markov sources)</p>
  </li>
  <li>
    <p><a href="The second one">https://arxiv.org/abs/1802.10347</a> improves the complexity of 
the algorithm decompressing the text.</p>
  </li>
</ul>

<h2 id="multi-armed-bandit">Multi-armed bandit</h2>

<p><em>Multi-armed bandit</em> is an expression that appears here and there in 
TCS conference, and very often in theoretical learning. It is a type of 
problems where one has to make decisions one after the other, to 
maximize some pay-off. Basically, at each round, it has the choice 
between several option called the “arms” of the bandit (like the levers 
of different slot-machines).</p>

<p>A basic version is the following framework:</p>

<p>Given: $k$  arms, $T$ rounds.
In each round $t\in[T]$:</p>
<ol>
  <li>Algorithm picks arm $a_t$.</li>
  <li>Algorithm observes reward $r_t\in [0,1]$ for the chosen arm.</li>
</ol>

<p>The reward comes from an unknown distributionn that the algorithm, 
somehow learns. 
There is already a lot to say on this simple case, and there are a flud
of papers on this these days.</p>

<p><a href="https://arxiv.org/pdf/1904.07272.pdf">Here</a> is a recent introduction 
to multi-armed bandit. Also if you are in Rennes, France, next week, 
there is a 
<a href="https://perso.crans.org/besson/phd/defense/">PhD defense on this topic</a>.</p>

<p>## Delaunay triangulation have perfect matching</p>

<p><img src="assets/delaunay.png" alt="" class="center-image" height="600px" /></p>

<p><a href="https://en.wikipedia.org/wiki/Delaunay_triangulation">Delaunay triangulations</a> 
are triangulations of point sets in the plane. I recentely learnt that
the graphs that are Delaunay triangulations, always have a perfect 
matching (that is a matching of size $n/2$ if $n$ is even, and $(n-1)/2$
is $n$ is odd).</p>

<p>A short proof of this appeared on arxiv recently, 
<a href="https://arxiv.org/pdf/1907.01617.pdf">here</a>. (Actually it is a stronger
result that is proved, about the so-called “toughness” of Delaunay 
triangulations.)</p>

<h2 id="learning-augmented-algorithms">Learning-augmented algorithms</h2>

<p>Learning-augmented algorithms are algorithms that can use informtation
coming form some machine learning. 
Here is an example.</p>

<p>Binary search takes $O(\log n)$ in the worst-case. 
Now if you have some neural network (NN) telling you that the element you’re 
looking for is around position $i$, how do you modify your search? 
Well you can begin by testing position $i$. Then if the NN is not perfect, 
this might not be the right value, but maybe it’s close. Say the value 
you’re looking for is larger. Then you can try to get a larger position 
that is larger than your value, for example by doing exponential guesses. 
Once you have both upper and lower bound, you can run the usual binary 
search.</p>

<p>If the error (that is the number of positions between the right element
and the prediction of the NN) is $\mu$, then your algorithm runs in 
$O(\log \mu)$. This is good: if the prediction is good then you speed up 
the search, and if it’s bad, then you do not loose.</p>

<p>In more general terms, one looks for two properties:</p>

<ul>
  <li>consistency: the better the prediction, the better the algorithm</li>
  <li>robustness: if the predition is bad, then the algorithm does not get 
much worse.</li>
</ul>

<p>Note that for real application, one is also interested in the running 
time for the prediction.</p>

<p>Material on this topic:</p>

<ul>
  <li>a <a href="https://www.mit.edu/~andoni/algoS19/scribes/scribe24.pdf">lecture note</a></li>
  <li><a href="http://theory.stanford.edu/~sergei/slides/HALG-slides.pdf">the slides</a> 
of a talk at <a href="http://2019.highlightsofalgorithms.org/">HALG 2019</a></li>
  <li><a href="https://www.mit.edu/~vakilian/ttic-workshop.html">a workshop</a>.</li>
</ul>

